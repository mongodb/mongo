#
# This file defines the tasks and platforms for WiredTiger in the
# MongoDB continuous integration system (https://evergreen.mongodb.com).
#

#######################################
#            Project Settings         #
#######################################
stepback: true
pre:
  - func: "cleanup"
  - func: "setup environment"
post:
  - func: "dump stacktraces"
  - func: "upload stacktraces"
  - func: "upload test/format config"
  - func: "upload test/model workloads"
  - func: "dump stderr/stdout"
  - func: "upload artifact"
    vars:
      postfix: -${execution}
  - func: "save wt hang analyzer core/debugger files"
  - func: "cleanup"
timeout:
  - func: "run wt hang analyzer"
exec_timeout_secs: 21600 # 6 hrs

#######################################
#            Functions                #
#######################################

functions:
  "setup environment":
    - command: expansions.update
      type: setup
      params:
        updates:
        # The expansion is used for each task that runs a WiredTiger test. The expansions are
        # created before each task and is meant to be used at the start each task. All of these
        # variables are common among the build variants, if there are any specific variables that
        # needs to be set, users can add onto the additional_env_vars in the variant.
        - key: PREPARE_TEST_ENV
          value: |
            export WT_TOPDIR=$(git rev-parse --show-toplevel)
            export WT_BUILDDIR=$WT_TOPDIR/cmake_build

            if [ "$OS" = "Windows_NT" ]; then
              export PATH=/cygdrive/c/python/Python311:/cygdrive/c/python/Python311/Scripts:$PATH
            else
              export PATH=/opt/mongodbtoolchain/v5/bin:$PATH
              export LD_LIBRARY_PATH=$WT_BUILDDIR
            fi

            # Create the common sanitizer options and export the specific sanitizer environment
            # variables.
            COMMON_SAN_OPTIONS="abort_on_error=1:disable_coredump=0"
            if [[ "${CMAKE_BUILD_TYPE|}" =~ ASan ]]; then
              export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
              export TESTUTIL_BYPASS_ASAN=1
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ MSan ]]; then
              export MSAN_OPTIONS="$COMMON_SAN_OPTIONS:verbosity=3"
              export TESTUTIL_MSAN=1
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ TSan ]]; then
              export TSAN_OPTIONS="$COMMON_SAN_OPTIONS:verbosity=3"
              export TESTUTIL_TSAN=1
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ UBSan ]]; then
              export UBSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_stacktrace=1"
              export TESTUTIL_UBSAN=1
            fi

            if [[ ${ENABLE_TCMALLOC|0} -eq 1 ]]; then
              # The preloaded library should be downloaded/built during "configure wiredtiger" step.
              # DO NOT MOVE:This variable must be set in the same function(shell) that runs a test,
              # and as close as possible to the test execution, since it affects all the binaries.
              export LD_PRELOAD=$WT_TOPDIR/TCMALLOC_LIB/libtcmalloc.so

              if [ ! -f "$LD_PRELOAD" ]; then
                echo "Error: TCMalloc support was requested, but the library doesn't exist: $LD_PRELOAD"
                exit 1
              fi
            fi

            ${additional_env_vars}

            echo "Dump Environment"
            printenv
        # The expansion is used for any task that requires the mongodbtoolchain binaries.
        - key: PREPARE_PATH
          value: |
            if [ "$OS" = "Windows_NT" ]; then
              export PATH=/cygdrive/c/python/Python311:/cygdrive/c/python/Python311/Scripts:$PATH
            else
              export PATH=/opt/mongodbtoolchain/v5/bin:$PATH
            fi
  # Since Bazel (currently used in SCons) uses EngFlow's remote execution system instead of icecream,
  # additional credentials need to be setup to maintain efficient compilation speed.
  "get engflow creds": &get_engflow_creds
  - command: s3.get
    display_name: "get engflow key"
    params:
      aws_key: ${engflow_key}
      aws_secret: ${engflow_secret}
      remote_file: engflow/engflow.key
      bucket: serverengflow
      local_file: "mongo/engflow.key"
  - command: s3.get
    display_name: "get engflow cert"
    params:
      aws_key: ${engflow_key}
      aws_secret: ${engflow_secret}
      remote_file: engflow/engflow.cert
      bucket: serverengflow
      local_file: "mongo/engflow.cert"
  - command: shell.exec
    params:
      display_name: "generate evergreen engflow bazelrc"
      shell: bash
      working_dir: mongo
      script: |
        set -o errexit
        set -o verbose

        # Pulled from evergreen/generate_evergreen_engflow_bazelrc.sh
        # FIXME-SERVER-86966: consider consolidating once prelude.sh is runnable in the perf project.

        source ./evergreen/bazel_RBE_supported.sh

        if bazel_rbe_supported; then

          uri="https://spruce.mongodb.com/task/${task_id}?execution=${execution}"

          echo "build --tls_client_certificate=./engflow.cert" > .bazelrc.evergreen_engflow_creds
          echo "build --tls_client_key=./engflow.key" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdPipelineName=${build_variant}" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdJobName=${task_name}" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdUri=$uri" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=evg:project=${project}" >> .bazelrc.evergreen_engflow_creds
          echo "build --workspace_status_command=./evergreen/engflow_workspace_status.sh" >> .bazelrc.evergreen_engflow_creds
        fi
  "get project":
    command: git.get_project
    type: setup
    params:
      directory: wiredtiger
  "generate github token": &gen_github_token
    command: github.generate_token
    params:
      owner: wiredtiger
      repo: automation-scripts
      expansion_name: generated_token
      permissions:
          metadata: read
          contents: read
  "get automation-scripts": &get_automation_scripts
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        if ! [ -d "./automation-scripts" ]; then
          git clone https://x-access-token:${generated_token}@github.com/wiredtiger/automation-scripts.git
        fi
  "fetch artifacts": &fetch_artifacts
    command: s3.get
    type: setup
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${dependent_task|compile}_${build_id}${postfix|}.tgz
      bucket: build_external
      extract_to: ${destination|wiredtiger}
  "fetch endian format artifacts" :
    - command: s3.get
      type: setup
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: wiredtiger/${endian_format}/${revision}/artifacts/${remote_file}.tgz
        bucket: build_external
        extract_to: wiredtiger/cmake_build/test/format
  "fetch mongo-tests repo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        git clone https://github.com/wiredtiger/mongo-tests
  "fetch mongo repo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        pushd wiredtiger
        # Find the equivalent mongodb branch that matches the same branch version to wiredtiger. In
        # the case of patch builds, it is possible that the developer has a branch name that doesn't
        # adhere to naming rules, therefore derive the base branch name from git.
        branch=$(git branch --contains | grep -e "mongodb" -e "develop" -e "evg-pr-test-")
        if [[ $branch =~ "mongodb-" ]]; then
          mongo_repo=https://github.com/mongodb/mongo
          mongo_branch=v$(echo $branch | cut -d'-' -f 2)
        else
          mongo_repo=https://github.com/mongodb/mongo
          mongo_branch=master
        fi
        popd
        git clone $mongo_repo -b $mongo_branch
  "import wiredtiger into mongo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        cp -a wiredtiger mongo/src/third_party/
  "compile mongodb" :
    command: shell.exec
    params:
      shell: bash
      working_dir: "mongo"
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_PATH}
        virtualenv -p python3 venv
        source venv/bin/activate
        buildscripts/poetry_sync.sh

        # The bazel cache uses a substantial (>30GB) amount of data which causes our evergreen hosts
        # to run out of disk space. By default it's saved into ~/.cache, but we want it saved into
        # our evergreen workspace which has more backing storage
        echo "startup --output_user_root=${workdir}/bazel-output-root" >> ~/.bazelrc
        echo "BAZELISK_HOME=${workdir}/bazelisk_home" >> ~/.bazeliskrc

        bazel build --mongo_toolchain_version=v5 --config=opt --build_enterprise=False install-mongod
  "configure wiredtiger": &configure_wiredtiger
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      include_expansions_in_env:
       - "s3_bucket_tcmalloc"
       - "s3_access_key"
       - "s3_secret_key"
      shell: bash
      script: |
        set -o errexit
        ${PREPARE_PATH}
        # Not setting verbose mode as we have sensitive keys that could be logged.

        # Define common config flags for the tasks to make it cleaner when configuring the tasks.
        # Note that the config flags are resolved prior to changing to cmake_build directory.
        DEFINED_EVERGREEN_CONFIG_FLAGS="${CMAKE_BUILD_TYPE|} \
          ${CMAKE_INSTALL_PREFIX|-DCMAKE_INSTALL_PREFIX=$(pwd)/cmake_build/LOCAL_INSTALL} \
          ${CMAKE_TOOLCHAIN_FILE|-DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_gcc.cmake} \
          ${ENABLE_LAZYFS|} \
          ${NONSTANDALONE|} \
          ${ENABLE_SHARED|} \
          ${ENABLE_STATIC|} \
          ${HAVE_BUILTIN_EXTENSION_LZ4|} \
          ${HAVE_BUILTIN_EXTENSION_SNAPPY|} \
          ${HAVE_BUILTIN_EXTENSION_ZLIB|} \
          ${HAVE_BUILTIN_EXTENSION_ZSTD|} \
          ${HAVE_UNITTEST} \
          ${CODE_COVERAGE_FLAGS} \
          ${NON_BARRIER_DIAGNOSTIC_YIELDS|} \
          ${HAVE_DIAGNOSTIC|} \
          ${WORDS_BIGENDIAN|} \
          ${GNU_C_VERSION|} \
          ${GNU_CXX_VERSION|} \
          ${CLANG_C_VERSION|} \
          ${CLANG_CXX_VERSION|} \
          ${ENABLE_AZURE|} \
          ${ENABLE_CPPSUITE|} \
          ${ENABLE_GCP|} \
          ${ENABLE_S3|} \
          ${IMPORT_AZURE_SDK|} \
          ${IMPORT_GCP_SDK|} \
          ${IMPORT_S3_SDK|} \
          ${SPINLOCK_TYPE|} \
          ${ENABLE_COLORIZE_OUTPUT|-DENABLE_COLORIZE_OUTPUT=0} \
          ${CC_OPTIMIZE_LEVEL|}"

        # The RHEL PPC platform does not have ZSTD. Strip it out.
        if [ "${build_variant|}" = "rhel8-ppc" ] && [[ "$DEFINED_EVERGREEN_CONFIG_FLAGS" =~ (\-DHAVE_BUILTIN_EXTENSION_ZSTD=1) ]]; then
          DEFINED_EVERGREEN_CONFIG_FLAGS=${DEFINED_EVERGREEN_CONFIG_FLAGS/\-DHAVE_BUILTIN_EXTENSION_ZSTD=1/}
        fi

        if [[ "${build_variant|}" =~ "macos-14" ]]; then
          # For mac builds, we want explicitly tell cmake which python to use, as
          # well as the matching library directory and header files. The find_libpython
          # module gives us the library.
          SYSPY=${python_binary}
          $SYSPY -mvenv venv
          source venv/bin/activate
          pip3 install find_libpython==0.4.0
          SYSPYLIB=`find_libpython`
          SYSPYINCDEF=

          # We have the shared library to link to, it may be named simply 'Python3' or 'Python'.
          # If that's the case, use the associated dylib symlink found in an expected relative
          # location. Also get the location of the header files. We'll give this all to cmake.
          base=$(basename $SYSPYLIB)
          if [ "$base" = 'Python3' -o "$base" = 'Python' ]; then
             SYSPYDIR=$(dirname $SYSPYLIB)
             NSYSPYLIB=$(ls $SYSPYDIR/lib/libpython*.dylib 2>/dev/null | head -1)
             if [ -f "$NSYSPYLIB" ]; then
               SYSPYLIB=$NSYSPYLIB
             fi
             if [ -d "$SYSPYDIR/Headers" ]; then
               SYSPYINCDEF="$SYSPYDIR/Headers"
             fi
          fi

          if [ "${build_variant|}" = "macos-14-arm64" ]; then
            DEFINED_EVERGREEN_CONFIG_FLAGS="$DEFINED_EVERGREEN_CONFIG_FLAGS -DPython3_EXECUTABLE=$SYSPY -DPython3_LIBRARY=$SYSPYLIB -DPython3_INCLUDE_DIR=$SYSPYINCDEF"
          else
            DEFINED_EVERGREEN_CONFIG_FLAGS="$DEFINED_EVERGREEN_CONFIG_FLAGS -DPYTHON_EXECUTABLE:FILEPATH=$SYSPY -DPYTHON_LIBRARY=$SYSPYLIB -DPYTHON_INCLUDE_DIR=$SYSPYINCDEF"
          fi
        fi

        if [ "$OS" = "Windows_NT" ]; then
          # Use the Windows powershell script to configure the CMake build.
          # We execute it in a powershell environment as its easier to detect and source the Visual Studio
          # toolchain in a native Windows environment. We can't easily execute the build in a cygwin environment.
          echo "Using config flags $DEFINED_EVERGREEN_CONFIG_FLAGS ${windows_configure_flags|}"
          powershell.exe  -NonInteractive '.\test\evergreen\build_windows.ps1' -configure 1 $DEFINED_EVERGREEN_CONFIG_FLAGS ${windows_configure_flags|}
        else
          echo "Using config flags $DEFINED_EVERGREEN_CONFIG_FLAGS ${posix_configure_flags|}"

          # Make sure that SWIG v4 is available.
          source test/evergreen/ensure_swig_version.sh

          if [[ ${ENABLE_TCMALLOC|0} -eq 1 ]]; then
            # Preclude use of tcmalloc with sanitizer builds.
            if [[ "$DEFINED_EVERGREEN_CONFIG_FLAGS" =~ (CMAKE_BUILD_TYPE=([AMT]|UB)San) ]]; then
              echo "tcmalloc incompatible with build variant: ${build_variant} and cmake build type ${CMAKE_BUILD_TYPE}"
              exit 1
            fi
            # Run this script in its own process as it depends on managing
            # shell options.
            /bin/bash test/evergreen/tcmalloc_install_or_build.sh ${build_variant}
          fi

          # Compiling with CMake.
          echo "Find CMake"
          . test/evergreen/find_cmake.sh
          # If we've fetched the wiredtiger artifact from a previous compilation/build, it's best to remove
          # the previous build directory so we can create a fresh configuration. We can't use the the previous
          # CMake Cache configuration as its likely it will have absolute paths related to the previous build machine.
          echo "Remove the cmake_build directory, if it already exists"
          if [ -d cmake_build ]; then rm -r cmake_build; fi
          echo "Create a new cmake_build directory"
          mkdir -p cmake_build
          cd cmake_build

          echo "Call CMake"
          $CMAKE $DEFINED_EVERGREEN_CONFIG_FLAGS ${posix_configure_flags|} -G "${cmake_generator|Ninja}" ./..
          echo "Completed CMake"
        fi
  "python config check":
    command: shell.exec
    type: setup
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        # Confirm that the Python binary matches the version of that configured by CMake.
        ${python_binary|python3} ../test/evergreen/python_version_check.py -v -c ./CMakeCache.txt -s ${python_config_search_string}
  "make wiredtiger": &make_wiredtiger
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        echo "Starting 'make wiredtiger' step"
        ${PREPARE_PATH}

        if ! command -v swig >/dev/null 2>&1; then
            echo "SWIG is not installed. Skipping check ..."
        else
            # Make sure that SWIG v4 is available.
            source test/evergreen/ensure_swig_version.sh
        fi

        if [ "$OS" = "Windows_NT" ]; then
          # Use the Windows powershell script to execute Ninja build (can't execute directly in a cygwin environment).
          powershell.exe '.\test\evergreen\build_windows.ps1 -build 1'
        else
          # Compiling with CMake generated Ninja file.
          cd cmake_build
          if [ "${cmake_generator|Ninja}" = "Ninja" ]; then
            ninja -j ${num_jobs} 2>&1
          else
            make -j ${num_jobs} 2>&1
          fi
        fi
        echo "Ending 'make wiredtiger' step"
  "compile wiredtiger":
    - *gen_github_token
    - *get_automation_scripts
    - *configure_wiredtiger
    - *make_wiredtiger
  "dump stacktraces": &dump_stacktraces
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${python_binary|python3} ../test/evergreen/print_stack_trace.py
  "upload stacktraces": &upload_stacktraces
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/*stacktrace.txt
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
  "upload test/format config":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/test/format/RUNDIR*/CONFIG
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
      preserve_path: true
  "upload test/model workloads":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/test/model/tools/WT_TEST*/*.workload
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
      preserve_path: true
  "run data validation stress test checkpoint":
    - *fetch_artifacts
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/test/checkpoint"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${PREPARE_TEST_ENV}

          # Check if we running in disagg mode.
          disagg_args=""
          if [[ "${disagg_run|false}" == "true" ]]; then
              disagg_args="-d leader -t row"
          fi
          ../../../tools/run_parallel.sh "nice ./recovery-test.sh \"${data_validation_stress_test_args} ${run_test_checkpoint_args} $disagg_args\" WT_TEST.\$t test_checkpoint" ${validation_stress_num_runs|120}
  "run tiered storage test":
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build"
        shell: bash
        include_expansions_in_env:
          - aws_sdk_s3_ext_access_key
          - aws_sdk_s3_ext_secret_key
        script: |
          set -o errexit
          ${PREPARE_TEST_ENV}

          # Set the Azure credentials using config variable.
          export AZURE_STORAGE_CONNECTION_STRING="${azure_sdk_ext_access_key}"

          # GCP requires a path to a credentials file for authorization. To not expose the private
          # information within the file, we use a placeholder private variable which are replaced
          # in the command line with the evergreen expansion variables and stored in a temporary
          # file.
          file=$(mktemp --suffix ".json")

          # Use '|' as the delimiter instead of default behaviour because the private key contains
          # slash characters.
          sed -e 's|gcp_project_id|${gcp_sdk_ext_project_id}|'                      \
              -e 's|gcp_private_key|'"${gcp_sdk_ext_private_key}"'|'                \
              -e 's|gcp_private_id|${gcp_sdk_ext_private_key_id}|'                  \
              -e 's|gcp_client_email|${gcp_sdk_ext_client_email}|'                  \
              -e 's|gcp_client_id|${gcp_sdk_ext_client_id}|'                        \
              -e 's|gcp_client_x509_cert_url|${gcp_sdk_ext_client_x509_cert_url}|'  ../test/evergreen/gcp_auth.json > $file
          export GOOGLE_APPLICATION_CREDENTIALS="$file"

          virtualenv -p python3 venv
          source venv/bin/activate
          pip3 install boto3==1.34.144
          pip3 install azure-storage-blob==12.20.0
          pip3 install google-cloud-storage==2.17.0

          # Run Python testing for all tiered tests.
          python3 ../test/suite/run.py ${ignore_stdout} -j $(nproc) ${tiered_storage_test_name}
  "compile wiredtiger docs":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          set -o verbose

          # Check if specific branches are provided to the function through the expansion variable
          # defined in the documentation-update build variant. If none are specified, use the
          # current branch.
          if [ -z ${doc_update_branches} ]; then
            branches=$(git rev-parse --abbrev-ref HEAD)
          else
            branches=${doc_update_branches}
          fi

          # Because of Evergreen's expansion syntax, this is used to process each branch separately.
          IFS=,
          for branch in $branches; do

            echo "Checking out branch $branch ..."
            git checkout $branch

            # Java API is removed in newer branches via WT-6675.
            if [ $branch == "mongodb-4.2" ]; then
              pushd build_posix
              bash reconf
              ../configure CFLAGS="-DMIGHT_NOT_RUN -Wno-error" --enable-java --enable-python --enable-strict
              (cd lang/python && make ../../../lang/python/wiredtiger_wrap.c)
              (cd lang/java && make ../../../lang/java/wiredtiger_wrap.c)
            elif [ $branch == "mongodb-5.0" ] || [ $branch == "mongodb-4.4" ]; then
              pushd build_posix
              bash reconf
              ../configure CFLAGS="-DMIGHT_NOT_RUN -Wno-error" --enable-python --enable-strict
              (cd lang/python && make ../../../lang/python/wiredtiger_wrap.c)
            else
              . test/evergreen/find_cmake.sh
              if [ -d cmake_build ]; then rm -r cmake_build; fi
              mkdir -p cmake_build
              pushd cmake_build
              # Adding -DENABLE_PYTHON=1 -DENABLE_STRICT=1 as 6.0 does not default these like develop.
              $CMAKE -DCMAKE_C_FLAGS="-DMIGHT_NOT_RUN -Wno-error" -DENABLE_PYTHON=1 -DENABLE_STRICT=1 ../.
              make -C lang/python -j ${num_jobs}
            fi
            # Pop to root project directory.
            popd
            # Generate WiredTiger documentation.
            (cd dist && bash s_docs && echo "The documentation for $branch was successfully generated.")
            # Save generated documentation
            mv docs docs-$branch
          done

          # Checkout the default ("develop") branch again to leave wiredtiger/ in the same state we started with
          git checkout develop

  "update wiredtiger docs":
    - command: shell.exec
      type: setup
      params:
        shell: bash
        script: |
          # Use a single function to update the documentation of each supported WiredTiger branch.
          # This is useful as not all branches have a dedicated Evergreen project. Furthermore, the
          # documentation-update task is not triggered by every commit. We rely on the activity of
          # the develop branch to update the documentation of all supported branches.
          set -o errexit
          set -o verbose

          if [[ "${branch_name}" != "develop" ]]; then
            echo "We only run the documentation update task on the WiredTiger (develop) Evergreen project."
            exit 0
          fi

          git clone https://github.com/wiredtiger/wiredtiger.github.com.git
          cd wiredtiger.github.com

          # Branches to update are defined through an expansion variable.
          branches=${doc_update_branches}

          # Go through each branch to stage the doc changes.
          IFS=,
          for branch in $branches; do

            # Synchronize the generated documentation with the current one.
            echo "Synchronizing documentation for branch $branch ..."
            rsync -aq ../wiredtiger/docs-$branch/ $branch/ --delete

            # Commit and push the changes if any.
            if [[ $(git status "$branch" --porcelain) ]]; then
              git add $branch
              git commit -m "Update auto-generated docs for $branch" \
                        --author="doc-build-bot <svc-wiredtiger-doc-build@10gen.com>"
            else
              echo "No documentation changes for $branch."
            fi

          done
    - command: shell.exec
      type: setup
      params:
        shell: bash
        silent: true
        script: |
          set -o errexit

          # We could have exited the previous command for the same reason.
          if [[ "${branch_name}" != "develop" ]]; then
            echo "We only run the documentation update task on the WiredTiger (develop) Evergreen project."
            exit 0
          fi

          # Push the above-generated commit
          ${PREPARE_PATH}
          virtualenv -p python3 venv
          source venv/bin/activate
          python -m pip install PyGithub==2.3.0
          export GITHUB_OWNER="wiredtiger"
          export GITHUB_REPO="wiredtiger.github.com"
          export GITHUB_APP_ID="${doc_update_github_app_id}"
          export GITHUB_APP_PRIVATE_KEY="${doc_update_github_app_private_key}"
          # Make sure the below script is called under the default ("develop") branch.
          (cd wiredtiger && git checkout develop)
          python wiredtiger/test/evergreen/doc_update.py

  "make check directory":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        . test/evergreen/find_cmake.sh
        cd cmake_build/${directory}

        # Determine if we want to exclude or include disagg labels.
        disagg_filter=""
        if [[ "${disagg_run|false}" == "true" ]]; then
          disagg_filter='-L'  # Include labels
        else
          disagg_filter='-LE'  # Exclude labels
        fi
        # Create new disagg label filter or append to the existing one.
        ctest_args='${ctest_extra_args}'
        if $(echo "$ctest_args" | grep -q -w -- "$disagg_filter"); then
          ctest_args=$(echo "$ctest_args" | sed -E "s/$disagg_filter \"([^\"]*)\"/$disagg_filter \"\1|disagg\"/")
        else
          ctest_args+=" $disagg_filter \"disagg\""
        fi
        eval $CTEST $ctest_args --output-on-failure 2>&1

  "make check all":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        . test/evergreen/find_cmake.sh
        cd cmake_build
        echo "Using number of parallel processes '-j ${num_jobs}' for 'make check all'"
        # Create new disagg label filter or append to the existing one.
        ctest_args='${ctest_extra_args}'
        disagg_filter='-LE'  # Exclude labels
        if $(echo "$ctest_args" | grep -q -w -- $disagg_filter); then
          ctest_args=$(echo "$ctest_args" | sed -E "s/$disagg_filter \"([^\"]*)\"/$disagg_filter \"\1|disagg\"/")
        else
          ctest_args+=" $disagg_filter \"disagg\""
        fi
        eval $CTEST -L "^check$" $ctest_args -j ${num_jobs} --output-on-failure  2>&1

  # The following cppsuite tasks define a greater overall task.
  "cppsuite test run": &cppsuite_test_run
    command: shell.exec
    params:
      # The tests need to be executed in the cppsuite directory as some required libraries have
      # their paths defined relative to this directory.
      # The below script saves the exit code from the test to use it later in this function. By
      # doing this we can define our own custom artifact upload task without it being cancelled by
      # the test failing.
      # Additionally if the test fails perf statistics won't be uploaded as they may be invalid
      # due to the test failure.
      working_dir: "wiredtiger/cmake_build/test/cppsuite"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_TEST_ENV}
        ../../../test/evergreen/cppsuite_test_run.sh ${test_name} ${test_config_filename} "${test_config}"

  # The following cppsuite tasks define a greater overall task.
  "cppsuite test run all": &cppsuite_test_run_all
    command: shell.exec
    params:
      # The tests need to be executed in the cppsuite directory as some required libraries have
      # their paths defined relative to this directory.
      working_dir: "wiredtiger/cmake_build/test/cppsuite"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./run -C '${test_config}' -l 2

  # Delete unnecessary data from the upload.
  "cppsuite test remove files": &cppsuite_remove_files
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        rm -rf wiredtiger/cmake_build/examples
        rm -rf wiredtiger/cmake_build/bench
        mv wiredtiger/cmake_build/test/cppsuite wiredtiger/cmake_build/
        rm -rf wiredtiger/cmake_build/test/
        mkdir wiredtiger/cmake_build/test/
        mv wiredtiger/cmake_build/cppsuite wiredtiger/cmake_build/test/cppsuite

  # Custom cppsuite archive tasks.
  "cppsuite archive": &cppsuite_archive
    command: archive.targz_pack
    type: setup
    params:
      target: archive.tgz
      source_dir: wiredtiger/cmake_build/
      include:
        - "./**"

  # Custom cppsuite s3 artifact upload task.
  "cppsuite s3 put": &cppsuite_s3_put
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_file: archive.tgz
      bucket: build_external
      permissions: public-read
      content_type: application/tar
      display_name: cppsuite-test
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}${postfix|}.tgz

  # FIXME-WT-8538 This task prevents us from saving the same artifacts to evergreen twice. It can be
  # removed when we implement a generalised approach in WT-8538
  "cppsuite remove dir": &cppsuite_remove_dir
    command: shell.exec
    params:
      shell: bash
      script: |
        set -o verbose
        if [ -f wiredtiger/cmake_build/test/cppsuite/cppsuite_exit_code ]; then
          exit_code=`cat wiredtiger/cmake_build/test/cppsuite/cppsuite_exit_code`
        else
          exit_code=0
        fi
        rm -rf wiredtiger
        exit "$exit_code"

  # The cppsuite test per task function. Doesn't upload perf statistics to evergreen.
  "cppsuite test":
    - *cppsuite_test_run
    # Since we later remove the WiredTiger folder, we need to check for core dumps now.
    - *dump_stacktraces
    - *upload_stacktraces
    # Cleanup tasks.
    - *cppsuite_remove_files
    - *cppsuite_archive
    - *cppsuite_s3_put
    - *cppsuite_remove_dir

  # This cppsuite test function uploads perf statistics and should only be used on perf variants.
  "cppsuite perf test":
    - *cppsuite_test_run
    # Since we later remove the WiredTiger folder, we need to check for core dumps now.
    - *dump_stacktraces
    - *upload_stacktraces
    - command: subprocess.exec
      params:
        binary: bash
        args:
          - ./wiredtiger/test/evergreen/perf_submission.sh
        env:
          perf_file_path: ./wiredtiger/cmake_build/test/cppsuite/${test_name}.json
        include_expansions_in_env:
          - requester
          - revision_order_id
          - project_id
          - version_id
          - build_variant
          - task_name
          - task_id
          - execution
    # Cleanup tasks.
    - *cppsuite_remove_files
    - *cppsuite_archive
    - *cppsuite_s3_put
    - *cppsuite_remove_dir

  "wt2853_perf test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/bench/wt2853_perf"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./test_wt2853_perf ${wt2853_perf_args}

  "csuite test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        $(pwd)/test/csuite/${test_name}/test_${test_name} ${test_args|} 2>&1

  "unit test":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # no_run will contain a list of files we don't want to run,
        # do_run will contain a list of files we want to run.
        no_run=$(mktemp)
        do_run=$(mktemp)

        # We need a local variable for this. If unit_test_ignore isn't provided by the caller, it
        # ends up substituting a blank value. This breaks the -n check below, since there will be a
        # bare -n with no arguments (which returns true, for some reason).
        ignore="${unit_test_ignore}"

        if [ -n "$ignore" ]; then
            sed -e 's/ *#.*//' -e '/^$/d' < "$ignore" > $no_run
            (cd ./test/suite; ls test_*.py) | fgrep -vf $no_run > $do_run
        else
            # Some hooks prune out specific tests if we don't give them a list, e.g. hook_tiered.
            # But if there's anything in the list, they'll run that directly instead of pruning, so
            # generate an empty list.
            echo > $do_run
        fi

        cd cmake_build
        threads_command=""
        if [[ -n "${num_jobs}" ]]; then
          echo "Using num_jobs '-j ${num_jobs}' for 'unit test'"
          threads_command="-j ${num_jobs}"
        fi
        if [ ${check_coverage|false} = true ]; then
            ${python_binary|python3} ../test/suite/run.py ${ignore_stdout} ${unit_test_args|-v 2} ${unit_test_variant_args} $threads_command $(cat $do_run) 2>&1 || echo "Ignoring failed test as we are checking test coverage"
        else
            ${python_binary|python3} ../test/suite/run.py ${ignore_stdout} ${unit_test_args|-v 2} ${unit_test_variant_args} $threads_command $(cat $do_run) 2>&1
        fi

  "unit test tsan parallel":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # no_run will contain a list of files we don't want to run,
        # do_run will contain a list of files we want to run.
        no_run=$(mktemp)
        do_run=$(mktemp)
        sed -e 's/ *#.*//' -e '/^$/d' ${unit_test_parallel_ignore} > $no_run
        (cd ./test/suite; ls test_*.py) | fgrep -vf $no_run > $do_run
        cd cmake_build
        threads_command="-j ${nproc}"
        if [[ -n "${num_jobs}" ]]; then
          threads_command="-j ${num_jobs}"
        fi
        py=${python_binary|python3}
        echo "Using $threads_command for 'unit test tsan parallel'"
        ../tools/pytest_parallel --python $py $threads_command ${unit_test_parallel_args|-v 2} ${unit_test_variant_args} -- $(cat $do_run) 2>&1 || echo "Ignoring failed test as we are checking for tsan warnings"

  "tsan warning metric":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_PATH}
        python3 test/evergreen/tsan_warnings_analysis.py --timestamp ${timestamp}

  "code coverage analysis":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        echo "Performing code coverage analysis in ${working_dir}"
        ${PREPARE_PATH}
        ./test/evergreen/code_coverage_analysis.sh ${coverage_filter|src} ${num_jobs|4} ${python_binary|python3} ${generate_atlas_format} ${combine_coverage_report} ${first_coverage_file_path} ${second_coverage_file_path}
  "run code coverage tests":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        test/evergreen/find_cmake.sh

        # Record the start time, in seconds
        date +%s > time.txt

        ${python_binary|python3} test/evergreen/code_coverage/parallel_code_coverage.py -c ${code_coverage_test_file|test/evergreen/code_coverage/code_coverage_config.json} -b $(pwd)/build_ -j ${num_jobs} -s ${code_cov_extra_args} -v

        # Record the end time, in seconds
        date +%s >> time.txt

  "code coverage publish report":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter: wiredtiger/coverage_report/*
      bucket: build_external
      permissions: public-read
      content_type: text/html
      remote_file: wiredtiger/${build_variant}/${revision}/${task_name}_${build_id}-${execution}/

  "code coverage publish main page":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_file: wiredtiger/coverage_report/2_coverage_report.html
      bucket: build_external
      permissions: public-read
      content_type: text/html
      display_name: "Coverage report main page"
      remote_file: wiredtiger/${build_variant}/${revision}/${task_name}_${build_id}-${execution}/1_coverage_report_main.html

  "format test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # Fail, show the configuration file.
        fail() {
          echo "======= FAILURE =========="
          [ -f RUNDIR/CONFIG ] && cat RUNDIR/CONFIG
          exit 1
        }

        for i in $(seq ${times|1}); do
          ./t -c ${config|../../../test/format/CONFIG.stress} ${trace_args|-T bulk,txn,retain=50} ${test_format_extra_args|} || fail
        done
  "format test predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        # To test predictable replay, we run test/format three times with the same data seed
        # each time, and compare the keys and values found in the WT home directories.
        # The first run is a timed one. When it's completed, we get the run's stable timestamp,
        # and do the subsequent runs up to that stable timestamp.  This, along with predictable
        # replay using the same data seed, should guarantee we have equivalent data created.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}

        ../../../test/evergreen/format_test_predictable.sh ${times} ${test_format_extra_args}
  "format test script":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      add_expansions_to_env: true
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ${additional_san_vars}
        ${format_test_setting}
        # Use only 80% of the machines total memory and make sure that memory is in MB.
        # Note: Currently format sh script is only run on ubuntu variants therefore we get the
        # /proc/meminfo here.
        memory_max=$(cat /proc/meminfo | grep "MemTotal: " | grep -o "[[:digit:]]*")
        memory_max=$(( ($memory_max * 8) / (${num_jobs} * 1024 * 10) ))
        for i in $(seq ${times|1}); do
          ./format.sh -j ${num_jobs} ${format_test_script_args|} cache.maximum=$memory_max 2>&1
        done
  "format test disagg":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ${additional_san_vars}
        # Fail, show the configuration file.
        fail() {
          echo "======= FAILURE =========="
          [ -f RUNDIR/CONFIG ] && cat RUNDIR/CONFIG
          exit 1
        }

        for i in $(seq ${times}); do
          echo Iteration $i/${times}
          rm -rf RUNDIR
          ./t -c ../../../test/format/CONFIG.disagg ${format_args|} ${extra_arg|} || fail
          ./t -R $format_args ${extra_args|} || fail
        done
  "many dbs test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/manydbs"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./test_manydbs ${many_db_args|} 2>&1
  "thread test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/thread"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./t ${thread_test_args|} 2>&1
  "recovery stress test script":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/csuite"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}

        ../../../test/evergreen/recovery_stress_test_script.sh ${times|1} ${truncated_log_args|}
  "schema abort predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/csuite/schema_abort"
      shell: bash
      script: |
        # Run schema_abort in a way that can test predictable replay.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}

        ../../../../test/evergreen/schema_abort_predictable.sh ${times} ${truncated_log_args|}
  "upload artifact":
    - command: archive.targz_pack
      type: setup
      params:
        target: ${upload_filename|wiredtiger.tgz}
        source_dir: ${upload_source_dir|wiredtiger}
        include:
          - "./**"
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${upload_filename|wiredtiger.tgz}
        bucket: build_external
        permissions: public-read
        content_type: application/tar
        display_name: ${artifacts_name|Artifacts}
        remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}${postfix|}.tgz
  "upload endian format artifacts":
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${local_file}
        bucket: build_external
        permissions: public-read
        content_type: application/tar
        display_name: WT_TEST
        remote_file: wiredtiger/${endian_format}/${revision}/artifacts/${remote_file}
  "cleanup":
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        rm -rf "wiredtiger"
        rm -rf "wiredtiger.tgz"

  "run wt hang analyzer":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_PATH}

        # Dump core (-c) and debugger outputs (-o)
        wt_hang_analyzer_option="-c -o file -o stdout"

        echo "Calling the wt hang analyzer ..."
        ${python_binary|python3} ../test/wt_hang_analyzer/wt_hang_analyzer.py $wt_hang_analyzer_option

  "save wt hang analyzer core/debugger files":
    - command: archive.targz_pack
      type: setup
      params:
        target: "wt-hang-analyzer.tgz"
        source_dir: "wiredtiger/cmake_build"
        include:
          - "./*core*"
          - "./debugger*.*"
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: wt-hang-analyzer.tgz
        bucket: build_external
        optional: true
        permissions: public-read
        content_type: application/tar
        display_name: WT Hang Analyzer Output - Execution ${execution}
        remote_file: wiredtiger/${build_variant}/${revision}/wt_hang_analyzer/wt-hang-analyzer_${task_name}_${build_id}${postfix|}.tgz

  "dump stderr/stdout":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose

        if [ -d "WT_TEST" ]; then
          # Dump stderr/stdout contents generated by the C libraries onto console for Python tests
          find "WT_TEST" -name "std*.txt" ! -empty -exec bash -c "echo 'Contents from {}:'; cat '{}'" \;
        fi

  "checkpoint test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # Check if we running in disagg mode.
        disagg_args=""
        if [[ "${disagg_run|false}" == "true" ]]; then
            disagg_args="-d leader -t row -x"
        fi
        ./test_checkpoint ${checkpoint_args} $disagg_args  2>&1

  "checkpoint test predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        # Run test/checkpoint in a way that can test predictable replay.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        disagg_args=""
        if [[ "${disagg_run|false}" == "true" ]]; then
            disagg_args="-d leader -t row -x"
        fi
        ../../../test/evergreen/checkpoint_test_predictable.sh ${times} "${checkpoint_args} $disagg_args"

  "checkpoint stress test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ../../../test/evergreen/checkpoint_stress_test.sh ${tiered|0} ${times|1} ${no_of_procs|1} "${wt_config|cache_size=100M}" "${timestamp_config|}"

  "compatibility test":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        script: |
          set -o errexit
          set -o verbose
          test/compatibility/compatibility_test_for_releases.sh ${compat_test_args}

  "run-perf-test":
    # Run a performance test
    # Parameterised using the 'perf-test-name' and 'maxruns' variables
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/bench/wtperf"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${PREPARE_TEST_ENV}

          if [ ${no_create|false} = false ]; then
            rm -rf WT_TEST*
          fi
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          pip3 install psutil==5.9.4
          ${python_binary|python3} ../../../bench/perf_run_py/perf_run.py --${test_type|wtperf} -e ${exec_path|./wtperf} -t ${perf-test-path|../../../bench/wtperf/runners}/${perf-test-name} -ho WT_TEST -m ${maxruns} -v -b -o test_stats/evergreen_out_${perf-test-name}.json ${wtarg}
          ${python_binary|python3} ../../../bench/perf_run_py/perf_run.py --${test_type|wtperf} -e ${exec_path|./wtperf} -t ${perf-test-path|../../../bench/wtperf/runners}/${perf-test-name} -ho WT_TEST -m ${maxruns} -v -re -o test_stats/atlas_out_${perf-test-name}.json ${wtarg}

  "upload test stats":
    - command: subprocess.exec
      params:
        binary: bash
        args:
          - ./wiredtiger/test/evergreen/perf_submission.sh
        env:
          perf_file_path: ./wiredtiger/cmake_build/${test_path}.json
        include_expansions_in_env:
          - requester
          - revision_order_id
          - project_id
          - version_id
          - build_variant
          - task_name
          - task_id
          - execution

  "convert-to-atlas-evergreen-format":
    - command: shell.exec
      params:
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${python_binary|python3} wiredtiger/bench/perf_run_py/perf_json_converter_for_atlas_evergreen.py -i ${input_file} -n ${test_name} -o ${output_path}

  "upload stats to atlas":
    - *gen_github_token
    - *get_automation_scripts
    - command: shell.exec
      params:
        shell: bash
        silent: true
        script: |
          set -o errexit
          ${PREPARE_PATH}
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          pip3 install pymongo[srv]==3.12.2 pygit2==1.10.1
          EVERGREEN_TASK_INFO='{ "evergreen_task_info": { "is_patch": "'${is_patch}'", "task_id": "'${task_id}'", "distro_id": "'${distro_id}'", "execution": "'${execution}'", "task_name": "'${task_name}'", "version_id": "'${version_id}'", "branch_name": "'${branch_name}'" } }'
          echo "EVERGREEN_TASK_INFO: $EVERGREEN_TASK_INFO"
          ${python_binary|python3} automation-scripts/evergreen/upload_stats_atlas.py -u ${atlas_perf_test_username} -p ${atlas_perf_test_password} -c ${collection|} -d ${database|} -f ${stats_dir|./wiredtiger/cmake_build/bench/wtperf/test_stats}/atlas_out_${test-name}.json -t ${created_at} -i "$EVERGREEN_TASK_INFO" -g "./wiredtiger"

  "upload stats to evergreen":
    - command: subprocess.exec
      params:
        binary: bash
        args:
          - ./wiredtiger/test/evergreen/perf_submission.sh
        env:
          perf_file_path: ${stats_dir|./wiredtiger/cmake_build/bench/wtperf/test_stats}/evergreen_out_${test-name}.json
        include_expansions_in_env:
          - requester
          - revision_order_id
          - project_id
          - version_id
          - build_variant
          - task_name
          - task_id
          - execution
    # Push the json results to the 'Files' tab of the task in Evergreen
    # Parameterised using the 'test-name' variable
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${stats_dir|wiredtiger/cmake_build/bench/wtperf/test_stats}/atlas_out_${test-name}.json
        bucket: build_external
        permissions: public-read
        content_type: text/html
        remote_file: wiredtiger/${build_variant}/${revision}/${task_name}-${build_id}-${execution}/

  "validate-expected-stats":
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/bench/wtperf"
        shell: bash
        script: |
          set -o errexit
          ${PREPARE_PATH}
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          ${python_binary|python3} ../../../bench/perf_run_py/validate_expected_stats.py '${stat_file}' ${comparison_op} '${expected-stats}'

  "verify wt datafiles":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ./test/evergreen/verify_wt_datafiles.sh 2>&1

  "install gcp dependencies":
    - command: shell.exec
      type: setup
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          . test/evergreen/find_cmake.sh
          . test/evergreen/install_gcp_dependencies.sh $CMAKE

  "split stress test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/bench/workgen/runner"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        for i in $(seq ${times|15}); do
            ${python_binary|python3} split_stress.py
        done

  "build and push antithesis container":
    command: subprocess.exec
    type: setup
    params:
      working_dir: wiredtiger/tools/antithesis
      binary: bash
      add_expansions_to_env: true
      args:
      - "./build_and_push_containers.sh"

  "run workgen test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/bench/workgen/runner"
      include_expansions_in_env:
        - task_name
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # The task name gives us the test name to run (workgen-test-<filename>)
        FILENAME=`echo ${task_name}| cut -d'-' -f 3-`
        echo "Running $FILENAME.py"
        ${python_binary|python3} $FILENAME.py

#######################################
#               Variables             #
#######################################

variables:
  # Configure flags for builtins.
  - &configure_flags_with_builtins
    HAVE_BUILTIN_EXTENSION_LZ4: -DHAVE_BUILTIN_EXTENSION_LZ4=1
    HAVE_BUILTIN_EXTENSION_SNAPPY: -DHAVE_BUILTIN_EXTENSION_SNAPPY=1
    HAVE_BUILTIN_EXTENSION_ZLIB: -DHAVE_BUILTIN_EXTENSION_ZLIB=1
    HAVE_BUILTIN_EXTENSION_ZSTD: -DHAVE_BUILTIN_EXTENSION_ZSTD=1

  # Configure flags static library (default in cmake is dynamic).
  - &configure_flags_static_lib
    ENABLE_SHARED: -DENABLE_SHARED=0
    ENABLE_STATIC: -DENABLE_STATIC=1

  # Configure flags static library (default in cmake is dynamic) with builtins.
  - &configure_flags_static_lib_with_builtins
    <<: *configure_flags_with_builtins
    ENABLE_SHARED: -DENABLE_SHARED=0
    ENABLE_STATIC: -DENABLE_STATIC=1

  # Configure flags for tiered storage Azure extension.
  - &configure_flags_tiered_storage_azure
    ENABLE_AZURE: -DENABLE_AZURE=1
    IMPORT_AZURE_SDK: -DIMPORT_AZURE_SDK=external

  # Configure flags for tiered storage GCP extension.
  - &configure_flags_tiered_storage_gcp
    ENABLE_GCP: -DENABLE_GCP=1
    IMPORT_GCP_SDK: -DIMPORT_GCP_SDK=external

  # Configure flags for tiered storage S3 extension.
  - &configure_flags_tiered_storage_s3
    ENABLE_S3: -DENABLE_S3=1
    IMPORT_S3_SDK: -DIMPORT_S3_SDK=external

  # Configure flags for address sanitizer for stable mongodb toolchain clang (include builtins).
  - &configure_flags_asan_mongodb_stable_clang_with_builtins
    <<: *configure_flags_with_builtins
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0
    ENABLE_TCMALLOC: 0

  # Template for Mac tests
  - &mac_test_template
    expansions: &mac_test_template_expansions
      # The cmake toolchain file is set to the mongodb toolchain gcc by default.
      # Remove that configuration here and let MacOS use the default Xcode toolchain instead.
      # We'll explicitly use the python3 in /usr/bin, we use the same in configuring cmake.
      CMAKE_TOOLCHAIN_FILE:
      CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      num_jobs: $(echo $(sysctl -n hw.logicalcpu) / 2 | bc)
      cmake_generator: "Unix Makefiles"
      additional_env_vars: |
        export LD_LIBRARY_PATH=""
        export DYLD_LIBRARY_PATH=$WT_BUILDDIR
      # Must disable TCMALLOC as it may be picked up locally and its not on all hosts.
      ENABLE_TCMALLOC: 0
    tasks:
      - name: compile
        # This is a special case and should *only* be used for the macOS buildvariant.
        # The macOS variant can take up to an hour to schedule tasks, but we want PR testing to complete
        # in under 30 minutes. The compile task also only takes 2 minutes to run so we can bump up priority
        # with minimal impact on other projects.
        priority: 5
      - name: make-check-test
      # Use a special version of unit-test for macOS that checks for Python version consistency.
      - name: unit-test-macos
      - name: fops
      - name: memory-model-test-mac
        batchtime: 40320 # 28 days

#########################################################################################
# The following stress tests are configured to run for six hours via the "-t 360"
# argument to format.sh: format-stress-test, format-stress-asan-test, and
# race-condition-stress-asan-test. The recovery tests run in a loop, with
# the number of runs adjusted to provide aproximately six hours of testing.
#########################################################################################

  - &format-stress-test-disagg-leader
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "format test disagg"
        vars:
          # FIXME-WT-14566 Limit the runs for now.
          format_args: disagg.mode=leader runs.rows=100000:300000 runs.tables=1:3 runs.ops=300000
          times: 5

  - &format-stress-data-validation-test-disagg-leader
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "format test disagg"
        vars:
          # Validate data with a non-layered table.
          # FIXME-WT-14566 Limit the runs for now.
          format_args: disagg.mode=leader ops.verify=1 runs.mirror=1 table1.runs.source=table table1.disagg.enabled=0 runs.tables=2 runs.rows=100000 runs.ops=300000
          times: 5

  - &format-stress-test-disagg-switch
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "format test disagg"
        vars:
          # FIXME-WT-14566 Limit the runs for now.
          format_args: disagg.mode=switch runs.rows=100000:300000 runs.tables=1:3 runs.ops=300000
          times: 5

  - &format-stress-test-disagg-follower
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "format test disagg"
        vars:
          format_args: disagg.mode=follower
          times: 5

  - &format-stress-data-validation-test-disagg-follower
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "format test disagg"
        vars:
          format_args: disagg.mode=follower ops.verify=1 runs.mirror=1
          times: 5

  - &format-stress-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 360

  - &format-stress-asan-ppc-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          # Always disable mmap for PPC due to issues on variant setup.
          # See https://bugzilla.redhat.com/show_bug.cgi?id=1686261#c10 for the potential cause.
          format_test_script_args: -t 360 -- -C "mmap=false,mmap_all=false"

  - &format-stress-asan-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -t 360

  - &race-condition-stress-asan-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -R -t 360

  - &recovery-stress-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      - func: "recovery stress test script"
        vars:
          times: 25

  - &workgen-test
    tags: ["workgen-test"]
    exec_timeout_secs: 21600
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "run workgen test"

#######################################
#               Tasks                 #
#######################################

tasks:
  # Check the python configuration
  # Base compile task on posix flavours
  - name: compile
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "upload artifact"
      - func: "cleanup"

  # production build with --disable-shared
  - name: compile-production-disable-shared
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib
      - func: "upload artifact"
      - func: "cleanup"

  # production build with --disable-static
  - name: compile-production-disable-static
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "upload artifact"
      - func: "cleanup"

  - name: compile-gcc
    tags: ["pull_request", "pull_request_compilers"]
    commands:
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Release
      - func: "compile wiredtiger"
        vars:
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Debug
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=7
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=8
          GNU_CXX_VERSION: -DGNU_CXX_VERSION=8
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=9
          GNU_CXX_VERSION: -DGNU_CXX_VERSION=9
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=11
          GNU_CXX_VERSION: -DGNU_CXX_VERSION=11

  - name: compile-clang
    tags: ["pull_request", "pull_request_compilers"]
    commands:
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Release
      - func: "compile wiredtiger"
        vars:
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Debug
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=7
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=7
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=8
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=8
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=9
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=9
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=10
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=10
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=11
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=12
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=13
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=13
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=14
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=14

  # Compile WiredTiger with uncommon build flags to make sure we compile code that
  # is often pre-processed out.
  - name: compile-uncommon-build-flags
    tags: ["pull_request", "pull_request_compilers"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"

      # Check that WiredTiger will compile for code coverage, as this may switch to alternative
      # implementations of some code which may generate different compiler errors to a 'normal' build
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Coverage
          CODE_COVERAGE_FLAGS: -DCODE_COVERAGE_MEASUREMENT=1 -DINLINE_FUNCTIONS_INSTEAD_OF_MACROS=1

      # Check that WiredTiger compiles when building for big endian systems.
      # This doesn't need to run on a big endian machine, we're just checking
      # that the code inside #ifdef WORDS_BIGENDIAN paths will compile.
      - func: "compile wiredtiger"
        vars:
          WORDS_BIGENDIAN: -DWORDS_BIGENDIAN=1
          # The catch2 unit tests make use of WORDS_BIGENDIAN. Make sure they're compiled
          HAVE_UNITTEST: -DHAVE_UNITTEST=1

  - name: make-check-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check all"

  # This task is the same as make-check-test but fits for lower specs by allowing a bigger timeout.
  - name: make-check-test-low-perf
    commands:
      - command: timeout.update
        params:
          # Allow the overall task to run for a maximum time of 4h.
          exec_timeout_secs: 14400
          # Slow scenarios will not output anything and Evergreen kills them after 2h. Allow 4h of silence.
          timeout_secs: 14400
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check all"

  # Start of normal make check test tasks

  - name: lang-python-test
    tags: ["pull_request", "python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: lang/python

  - name: examples-c-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: examples-c-production-disable-shared-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: examples-c-production-disable-static-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: bloom-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/bloom

  - name: checkpoint-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          ctest_extra_args: -LE "long_running" ${ctest_extra_args}
          directory: test/checkpoint

  - name: checkpoint-test-long-running
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          ctest_extra_args: -L "long_running" ${ctest_extra_args}
          directory: test/checkpoint

  - name: cursor-order-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/cursor_order

  - name: fops-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/fops

  - name: format-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/format

  - name: huge-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/huge

  - name: manydbs-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/manydbs

  - name: packing-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/packing

  - name: readonly-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/readonly

  - name: salvage-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/salvage

  - name: thread-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/thread

  - name: bench-wtperf-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: bench/wtperf

  - name: catch2-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/catch2/catch2-unittests

  - name: catch2-assertions
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          HAVE_UNITTEST: -DHAVE_UNITTEST=1 -DHAVE_UNITTEST_ASSERTS=1 -DHAVE_DIAGNOSTIC=0
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/catch2/catch2-unittests

  # End of normal make check test tasks

  # Start of cppsuite test tasks.
  # All cppsuite pull request tasks must supply the relative path to the config file as we are in
  # the cmake build working directory and the LD_LIBRARY_PATH is .libs.

  - name: cppsuite-background-compact-default
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config: debug_mode=(background_compact,cursor_copy=true)
          test_config_filename: configs/background_compact_default.txt
          test_name: background_compact

  - name: cppsuite-default-all
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test run all"
        vars:
          test_config: debug_mode=(cursor_copy=true)

  - name: cppsuite-background-compact-long
    tags: ["cppsuite-stress-test"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/background_compact_long.txt
          test_name: background_compact

  - name: cppsuite-operations-test-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/operations_test_stress.txt
          test_name: operations_test

  - name: cppsuite-hs-cleanup-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/hs_cleanup_stress.txt
          test_name: hs_cleanup

  - name: cppsuite-burst-inserts-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/burst_inserts_stress.txt
          test_name: burst_inserts

  - name: cppsuite-bounded-cursor-stress-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_stress_stress.txt
          test_name: bounded_cursor_stress

  - name: cppsuite-bounded-cursor-stress-reverse-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_stress_reverse_stress.txt
          test_name: bounded_cursor_stress

  - name: cppsuite-bounded-cursor-prefix-stat-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_stat_stress.txt
          test_name: bounded_cursor_prefix_stat

  - name: cppsuite-bounded-cursor-prefix-search-near-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_search_near_stress.txt
          test_name: bounded_cursor_prefix_search_near

  - name: cppsuite-bounded-cursor-prefix-indices-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_indices_stress.txt
          test_name: bounded_cursor_prefix_indices

  - name: cppsuite-reverse-split-stress
    tags: ["cppsuite-stress-test"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/reverse_split_stress.txt
          test_name: reverse_split

# Cppsuite perf tests
  - name: cppsuite-hs-cleanup-default-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          # Configure checkpoint cleanup to run more often as the test only runs for 90 seconds
          # and checkpoint cleanup is triggered every 300 seconds by default.
          test_config: checkpoint_cleanup=(wait=60)
          test_config_filename: configs/hs_cleanup_default.txt
          test_name: hs_cleanup

  - name: cppsuite-hs-cleanup-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/hs_cleanup_stress.txt
          test_name: hs_cleanup

  - name: cppsuite-operations-test-default-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/operations_test_default.txt
          test_name: operations_test

  - name: cppsuite-operations-test-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/operations_test_stress.txt
          test_name: operations_test
  # This is a perf test and as such doesn't run under the stress test tag. This name seems excessive
  # but in order to have a consistent naming system is required, the issue here is that the test
  # itself has the word "perf" in it.
  - name: cppsuite-bounded-cursor-perf-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/bounded_cursor_perf_stress.txt
          test_name: bounded_cursor_perf

  - name: cppsuite-api-instruction-count-benchmarks-default-perf
    depends_on:
      - name: compile
    # This and the later test only run on ARM as the required harsware counter is missing on x86.
    tags: ["cppsuite-perf-test-arm"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/api_instruction_count_benchmarks_default.txt
          test_name: api_instruction_count_benchmarks

  - name: cppsuite-api-timing-benchmarks-default-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test-arm"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/api_timing_benchmarks_default.txt
          test_name: api_timing_benchmarks

  # End of cppsuite test tasks.
  # Start of csuite test tasks
  - name: csuite-tests-fast
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/csuite
          ctest_extra_args: -LE "long_running|disagg" ${ctest_extra_args} -j ${num_jobs}

  - name: csuite-timestamp-abort-test-s3
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/timestamp_abort"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}

            ./test_timestamp_abort -PT -Po s3_store

  - name: csuite-random-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/random_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-schema-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/schema_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-timestamp-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/timestamp_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-wt12015-backup-corruption-test
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "csuite test"
        vars:
          test_name: wt12015_backup_corruption

  - name: csuite-wt11126-compile-config-test
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "csuite test"
        vars:
          test_name: wt11126_compile_config

  # End of csuite test tasks

  # Start of Python unit test tasks

  - name: unit-test
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"

  - name: unit-test-macos
    tags: ["python"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "python config check"
      - func: "unit test"

  - name: unit-test-zstd
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --zstd

  - name: unit-test-extra-long
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --extra-long

  # Run the tests that uses suite_random with a random starting seed
  - name: unit-test-random-seed
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -R cursor13 schema03 timestamp22

  - name: unit-test-tsan
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test tsan parallel"
        vars:
          # Run the set of tests that are not in the fail list.
          unit_test_parallel_ignore: test/suite/tsan.fail
          unit_test_parallel_args: -v 2 --timeout 300
      - func: "upload artifact"
        vars:
          upload_source_dir: "wiredtiger"
          upload_filename: unit-test-tsan.tgz

  - name: unit-test-disagg-leader-tsan
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test tsan parallel"
        vars:
          # Run the set of tests that are not in the fail list.
          unit_test_parallel_ignore: test/suite/tsan.fail test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_parallel_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)"
      - func: "upload artifact"
        vars:
          upload_source_dir: "wiredtiger"
          upload_filename: unit-test-disagg-leader-tsan.tgz

  - name: unit-test-disagg-follower-tsan
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As follower, run a minimal functionality test
          unit_test_args: -v 2 --timeout 60 --hook "disagg=(role=follower)" base01
      - func: "upload artifact"
        vars:
          upload_source_dir: "wiredtiger"
          upload_filename: unit-test-disagg-follower-tsan.tgz

  - name: unit-test-hook-disagg-leader
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)" --batch 0/5

  - name: unit-test-hook-disagg-leader-bucket01
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)" --batch 1/5

  - name: unit-test-hook-disagg-leader-bucket02
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)" --batch 2/5

  - name: unit-test-hook-disagg-leader-bucket03
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)" --batch 3/5

  - name: unit-test-hook-disagg-leader-bucket04
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader)" --batch 4/5

  - name: unit-test-hook-disagg-leader-table-bucket00
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader,table_prefix=table)" --batch 0/5

  - name: unit-test-hook-disagg-leader-table-bucket01
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader,table_prefix=table)" --batch 1/5

  - name: unit-test-hook-disagg-leader-table-bucket02
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader,table_prefix=table)" --batch 2/5

  - name: unit-test-hook-disagg-leader-table-bucket03
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader,table_prefix=table)" --batch 3/5

  - name: unit-test-hook-disagg-leader-table-bucket04
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As leader, run the set of tests that are not in the fail list.
          unit_test_ignore: test/suite/hook_disagg.fail
          # The timeout for this command is on a test file basis. Make the timeouts long,
          # as there are a few files that contain a large number of slower tests.
          unit_test_args: -v 2 --timeout 1800 --hook "disagg=(role=leader,table_prefix=table)" --batch 4/5

  - name: unit-test-hook-disagg-follower
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As follower, run a minimal functionality test
          unit_test_args: -v 2 --timeout 60 --hook "disagg=(role=follower)" base01

  - name: unit-test-hook-disagg-follower-table
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          # As follower, run a minimal functionality test
          unit_test_args: -v 2 --timeout 60 --hook "disagg=(role=follower,table_prefix=table)" base01

  - name: unit-test-hook-timing-stress-log
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook timing_stress_log_conn_close

  - name: unit-test-hook-tiered
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered

  - name: unit-test-hook-tiered-with-delays
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook 'tiered=(tier_storage_source=dir_store,tier_storage_source_config=(force_delay=1,delay_ms=10,verbose=1))'

  - name: unit-test-hook-tiered-s3
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} ../test/suite/run.py ${ignore_stdout} ${unit_test_args|-v 2} --hook tiered=tier_storage_source='s3_store' -j ${num_jobs} 2>&1

  - name: unit-test-hook-tiered-timestamp
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered --hook timestamp

  - name: unit-test-hook-timestamp
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook timestamp

  # A version of the tiered, timestamp hook test, scaled down for running during pull requests.
  # A small (1 out of N) random sample is run.
  - name: unit-test-hook-tiered-timestamp-quick
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered --hook timestamp --random-sample 20

  # The test_prepare_hs03.py test, when run with timestamp hooks, is run multiple times to facilitate
  # catching intermittent problems in the test.
  - name: test-prepare-hs03-hook-timestamp
    tags: ["python"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            cd cmake_build
            i=0
            limit=100
            while ((i < limit))
            do
              ((i=i+1))
              echo "Test count: $i"
              ${python_binary|python3} ../test/suite/run.py ${ignore_stdout} -v 4 test_prepare_hs03.py --hook timestamp
            done

  - name: csuite-long-running
    # Set 6 hours timeout (60 * 60 * 6 = 21600)
    exec_timeout_secs: 21600
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          ctest_extra_args: -L long_running -j ${num_jobs}

  # Break out Python unit tests into multiple buckets/tasks.  We have a fixed number of buckets,
  # and we use the -b option of the test/suite/run.py script to split up the tests.

  - name: unit-test-bucket00
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 0/12

  - name: unit-test-bucket01
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 1/12

  - name: unit-test-bucket02
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 2/12

  - name: unit-test-bucket03
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 3/12

  - name: unit-test-bucket04
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 4/12

  - name: unit-test-bucket05
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 5/12

  - name: unit-test-bucket06
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 6/12

  - name: unit-test-bucket07
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 7/12

  - name: unit-test-bucket08
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 8/12

  - name: unit-test-bucket09
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 9/12

  - name: unit-test-bucket10
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 10/12

  - name: unit-test-bucket11
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 11/12

  - name: unit-test-long-bucket00
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 0/12

  - name: unit-test-long-bucket01
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 1/12

  - name: unit-test-long-bucket02
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 2/12

  - name: unit-test-long-bucket03
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 3/12

  - name: unit-test-long-bucket04
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    run_on: ubuntu2004-medium
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 4/12

  - name: unit-test-long-bucket05
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 5/12

  - name: unit-test-long-bucket06
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 6/12

  - name: unit-test-long-bucket07
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 7/12

  - name: unit-test-long-bucket08
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 8/12

  - name: unit-test-long-bucket09
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 9/12

  - name: unit-test-long-bucket10
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 10/12

  - name: unit-test-long-bucket11
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 11/12

  # End of Python unit test tasks

  # Start of live restore tests

  - name: live-restore-short
    tags: ["pull_request"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/"
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ../test/live_restore/short_test.sh

  - name: live-restore-long
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/"
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ../test/live_restore/long_test.sh

  # End of live restore tests

  - name: s-all
    tags: ["lint"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}

            # Install Ruff as it is executed as part of s_all.
            RUFF_VERSION=$(grep required-version dist/ruff.toml | grep -o [0-9]\\.[0-9]\\.[0-9])
            virtualenv -p python3 venv
            source venv/bin/activate
            python3 -m pip install ruff==$RUFF_VERSION

            cd dist
            bash -x s_all -E 2>&1

  - name: s-outdated-fixmes
    # Detect any FIXME comments in the codebase tied to closed Jira tickets.
    # This will send a GET request to JIRA for each FIXME ticket so we don't
    # want to run it too frequently.
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${python_binary|python3} s_outdated_fixmes.py 2>&1

  - name: conf-dump-test
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} ../../../test/wtperf/test_conf_dump.py -d $(pwd) 2>&1

  - name: fops
    tags: ["pull_request"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/fops"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            if [ "Windows_NT" = "$OS" ]; then
              cmd.exe /c test_fops.exe
            else
              ./test_fops
            fi

  - name: bench-tiered-push-pull-s3
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/tiered"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./test_push_pull -PT -Po s3_store

  - name: bench-tiered-push-pull
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/tiered"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # By default the test_push_pull uses dir_store as a storage source.
            ./test_push_pull -PT

  - name: compatibility-test-dirty-restart
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -d

  - name: compatibility-test-for-newer-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -n

  - name: compatibility-test-for-older-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -o

  - name: compatibility-test-upgrade-to-latest
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -u

  - name: compatibility-test-for-patch-releases
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            test/compatibility/compatibility_test_for_releases.sh -p

  - name: compatibility-test-suite
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} test/compatibility/suite/compatibility_test.py ${compat_suite_args|-v 2} 2>&1

  - name: compatibility-test-for-wt-standalone-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -w

  - name: import-compatibility-test
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            test/compatibility/compatibility_test_for_releases.sh -i

  - name: generate-datafile-little-endian
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          times: 10
          config: ../../../test/format/CONFIG.endian
          test_format_extra_args: -h "WT_TEST.$i"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/format"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            # Archive the WT_TEST directories which include the generated wt data files. We cannot
            # use the Evergreen archive command as we need to archive multiple WT_TEST folders.
            tar -zcvf WT_TEST.tgz WT_TEST*
      - func: "upload endian format artifacts"
        vars:
          endian_format: little-endian
          local_file: wiredtiger/cmake_build/test/format/WT_TEST.tgz
          remote_file: WT_TEST-little-endian.tgz

  - name: verify-datafile-little-endian
    depends_on:
    - name: compile
    - name: generate-datafile-little-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: little-endian
          remote_file: WT_TEST-little-endian
      - func: "verify wt datafiles"

  - name: verify-datafile-from-little-endian
    depends_on:
    - name: compile
    - name: generate-datafile-little-endian
      variant: little-endian
    - name: verify-datafile-little-endian
      variant: little-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: little-endian
          remote_file: WT_TEST-little-endian
      - func: "verify wt datafiles"

  - name: generate-datafile-big-endian
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          times: 10
          config: ../../../test/format/CONFIG.endian
          test_format_extra_args: -h "WT_TEST.$i"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/format"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            # Archive the WT_TEST directories which include the generated wt data files. We cannot
            # use the Evergreen archive command as we need to archive multiple WT_TEST folders.
            tar -zcvf WT_TEST.tgz WT_TEST*
      - func: "upload endian format artifacts"
        vars:
          endian_format: big-endian
          local_file: wiredtiger/cmake_build/test/format/WT_TEST.tgz
          remote_file: WT_TEST-big-endian.tgz

  - name: verify-datafile-big-endian
    depends_on:
    - name: compile
    - name: generate-datafile-big-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: big-endian
          remote_file: WT_TEST-big-endian
      - func: "verify wt datafiles"

  - name: verify-datafile-from-big-endian
    depends_on:
    - name: compile
    - name: generate-datafile-big-endian
      variant: big-endian
    - name: verify-datafile-big-endian
      variant: big-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: big-endian
          remote_file: WT_TEST-big-endian
      - func: "verify wt datafiles"

  - name: clang-analyzer
    tags: ["lint"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}
            bash s_clang_scan 2>&1

  - name: configure-combinations
    commands:
      - func: "get project"
      - func: "generate github token"
      - func: "get automation-scripts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}
            . test/evergreen/find_cmake.sh
            cd test/evergreen
            source ensure_swig_version.sh
            CMAKE_BIN=$CMAKE ./configure_combinations.sh -g="${cmake_generator|Ninja}" -j=$(grep -c ^processor /proc/cpuinfo) 2>&1
      # Handle special build combination for running all the diagnostic tests.
      - func: "configure wiredtiger"
      - func: "make wiredtiger"
      - func: "make check all"

  - name: package
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            env CC=/opt/mongodbtoolchain/v5/bin/gcc CXX=/opt/mongodbtoolchain/v5/bin/g++ PATH=/opt/mongodbtoolchain/v5/bin:/opt/java/jdk11/bin:$PATH bash s_release `date +%Y%m%d`

  # Note that the project settings use this task name to compile the documentation during PR
  # testing, keep this in mind if you decide to change it.
  - name: doc-compile
    commands:
      - func: "get project"
      - func: "compile wiredtiger docs"

  - name: doc-update
    patchable: false
    stepback: false
    commands:
      - func: "get project"
      - func: "compile wiredtiger docs"
      - func: "update wiredtiger docs"
      - func: "upload artifact"
        vars:
          artifacts_name: wiredtiger
      - func: "upload artifact"
        vars:
          upload_filename: wiredtiger.github.com.tgz
          upload_source_dir: wiredtiger.github.com
          artifacts_name: wiredtiger.github.com
          postfix: -wiredtiger.github.com

  - name: syscall-linux
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/syscall"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            WT_BUILDDIR=$(pwd)/../../cmake_build LD_LIBRARY_PATH=$WT_BUILDDIR ${python_binary|python3} syscall.py --verbose --preserve

  - name: checkpoint-filetypes-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          # Don't use diagnostic - this test looks for timing problems that are more likely to occur without it
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t m -n 1000000 -k 5000000 -C cache_size=100MB
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t c -n 1000000 -k 5000000 -C cache_size=100MB
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t r -n 1000000 -k 5000000 -C cache_size=100MB

  - name: coverage-report-python
    # The coverage report tests are split into the two buckets, python and any other tests. The concept
    # is to reduce compute time on the overall code coverage report. This task will measure code
    # coverage across a range of python tests and run in parallel.
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - func: "run code coverage tests"
        vars:
          code_cov_extra_args: --bucket python
      - func: "code coverage analysis"
        vars:
          generate_atlas_format: true
          working_dir: "wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Remove all build directories before uploading the artifacts.
            rm -rf build_*
      - func: "upload artifact"
        vars:
          upload_source_dir: "wiredtiger/coverage_report"
          upload_filename: coverage-report-python.tgz

  - name: coverage-report-other
    # The coverage report tests are split into the two buckets, python and any other tests. The concept
    # is to reduce compute time on the overall code coverage report. This task will measure code
    # coverage tests such as wt tool and the csuite test, and run in parallel.
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - func: "run code coverage tests"
        vars:
          code_cov_extra_args: --bucket other
      - func: "code coverage analysis"
        vars:
          generate_atlas_format: true
          working_dir: "wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Remove all build directories before uploading the artifacts.
            rm -rf build_*
      - func: "upload artifact"
        vars:
          upload_source_dir: "wiredtiger/coverage_report"
          upload_filename: coverage-report-other.tgz

  - name: generate-coverage-report
    tags: ["pull_request_code_statistics"]
    depends_on:
      - name: "coverage-report-other"
      - name: "coverage-report-python"
    commands:
      - func: "get project"
      - func: "fetch artifacts"
        vars:
          dependent_task: coverage-report-python
          destination: wiredtiger/coverage_report_python
      - func: "fetch artifacts"
        vars:
          dependent_task: coverage-report-other
          destination: wiredtiger/coverage_report_other
      - func: "code coverage analysis"
        vars:
          first_coverage_file_path: coverage_report_other/full_coverage_report.json
          second_coverage_file_path: coverage_report_python/full_coverage_report.json
          combine_coverage_report: true
          generate_atlas_format: true
          working_dir: "wiredtiger"
      - func: "upload stats to atlas"
        vars:
          stats_dir: ./wiredtiger/coverage_report
          test-name: code_coverage
          collection: CodeCoverage
        # Publish the main page before the report so that it appears top of the list of files in the patch build.
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: coverage-report-per-test
    # This task will measure code coverage across a range of tests, including, but not limited to,
    # the Catch2-based unit tests. Code coverage results are captured on a per-test basis so we can determine which tests hit which code paths.
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}

            if [ ${is_patch|false} = true ]; then
              test/evergreen/code_coverage/coverage-report-per-test.sh true ${num_jobs|1}
            else
              test/evergreen/code_coverage/coverage-report-per-test.sh false ${num_jobs|1}
            fi
      - func: "code coverage publish report"

  - name: coverage-report-catch2
    # This task measures code coverage achieved by only the Catch2-based unit tests.
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - func: "run code coverage tests"
        vars:
          num_jobs: 1
          code_coverage_test_file: test/evergreen/code_coverage/code_coverage_config_catch2.json
          code_cov_extra_args: --check_errors
      - func: "code coverage analysis"
        vars:
          working_dir: "wiredtiger"
        # Publish the main page before the report so that it appears top of the list of files in the patch build.
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: code-change-report
    tags: ["pull_request_code_statistics"]
    depends_on:
      - name: "generate-coverage-report"
    commands:
      - func: "get project"
      - command: shell.exec
        vars:
          dependent_task: "generate-coverage-report"
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            echo "Code Change Report (Step 1)"
            echo "==========================="
            echo "expansions:"
            echo ". is_patch      =  ${is_patch}"
            echo ". task_id       =  ${task_id}"
            echo ". task_name     =  ${task_name}"
            echo ". build_id      =  ${build_id}"
            echo ". build_variant =  ${build_variant}"
            echo ". version_id    =  ${version_id}"
            echo ". revision      =  ${revision}"
            echo ". github_commit =  ${github_commit}"
            echo ". project       =  ${project}"
            echo ". execution     =  ${execution}"
            echo ". remote path   =  wiredtiger/${build_variant}/${revision}/${dependent_task}_${build_id}-${execution}/full_coverage_report.json"
            mkdir -p coverage_report
      - command: s3.get
        vars:
          dependent_task: "generate-coverage-report"
        params:
          aws_key: ${aws_key}
          aws_secret: ${aws_secret}
          remote_file: wiredtiger/${build_variant}/${revision}/${dependent_task}_${build_id}-${execution}/full_coverage_report.json
          bucket: build_external
          local_file: wiredtiger/coverage_report/full_coverage_report.json
      - command: github.generate_token
        params:
          expansion_name: github_token
          permissions:
            pull_requests: write
      - command: shell.exec
        vars:
          dependent_task: "generate-coverage-report"
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            echo "Code Change Report (Step 2)"
            echo "==========================="

            set -o errexit
            set -o verbose

            ${PREPARE_PATH}

            pr_args=""
            if [ ! -z "${github_pr_number}" ]; then
                echo "Detected Github PR ${github_pr_number}"
                pr_args+="--github_repo ${github_org}/${github_repo} "
                pr_args+="--github_pr_number ${github_pr_number} "
                pr_args+="--github_token ${github_token} "
            fi

            # Compose and append the code change coverage report URL.
            # An artifact URL is composed of a few components:
            # - <domain>/
            #     e.g. https://build_external.s3.amazonaws.com/
            # - <project>/<build_variant>/<revision>/
            #     e.g. wiredtiger/code-statistics/eb9b4dcafba665d0cd78af6382b47ff595d741da/
            # - <task_name>_<build_id>-<execution>/
            #     e.g. code-change-report_wiredtiger_code_statistics_patch_eb9b4dcafba665d0cd78af6382b47ff595d741da_6717f121c5311a000762e3c4_24_10_22_18_38_26-0/
            # - <file_name>
            #     e.g. code_change_report.html
            domain="https://build_external.s3.amazonaws.com"
            file_name="code_change_report.html"
            code_change_report_url=$domain/${project}/${build_variant}/${revision}/${task_name}_${build_id}-${execution}/$file_name
            pr_args+="--code_change_report_url $code_change_report_url "

            if [ ${is_patch|false} = true ]; then
              ./test/evergreen/coverage-report.sh true ${python_binary|python3} ${github_commit} "$pr_args"
            else
              ./test/evergreen/coverage-report.sh false ${python_binary|python3} ${github_commit} "$pr_args"
            fi

      - func: "code coverage publish report"

  - name: s3-tiered-storage-extensions-test
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered
  - name: s3-tiered-test-small
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered06
  - name: s3-tiered-catch2-unittest-test
    tags: ["tiered_unittest"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Run S3 extension unit testing.
            ext/storage_sources/s3_store/test/run_s3_unit_tests
  - name: azure-gcp-tiered-storage-extensions-test
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered
  - name: azure-gcp-tiered-catch2-unittest-test
    tags: ["tiered_unittest"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            # Set the Azure credentials using config variable.
            export AZURE_STORAGE_CONNECTION_STRING="${azure_sdk_ext_access_key}"

            # GCP requires a path to a credentials file for authorization. To not expose the private
            # information within the file, we use a placeholder private variable which are replaced
            # in the command line with the evergreen expansion variables and stored in a temporary
            # file.
            file=$(mktemp --suffix ".json")

            # Use '|' as the delimiter instead of default behaviour because the private key contains
            # slash characters.
            sed -e 's|gcp_project_id|${gcp_sdk_ext_project_id}|'                      \
                -e 's|gcp_private_key|'"${gcp_sdk_ext_private_key}"'|'                \
                -e 's|gcp_private_id|${gcp_sdk_ext_private_key_id}|'                  \
                -e 's|gcp_client_email|${gcp_sdk_ext_client_email}|'                  \
                -e 's|gcp_client_id|${gcp_sdk_ext_client_id}|'                        \
                -e 's|gcp_client_x509_cert_url|${gcp_sdk_ext_client_x509_cert_url}|'  ../test/evergreen/gcp_auth.json > $file
            export GOOGLE_APPLICATION_CREDENTIALS="$file"
            set -o verbose
            ${PREPARE_TEST_ENV}

            # Run Azure extension unit testing.
            ext/storage_sources/azure_store/test/run_azure_unit_tests

            # Run GCP extension unit testing.
            ext/storage_sources/gcp_store/test/run_gcp_unit_tests

  - name: azure-gcp-tiered-test-small
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered06

  - name: spinlock-gcc-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          SPINLOCK_TYPE: -DSPINLOCK_TYPE=gcc
      - func: "make check all"
      - func: "unit test"
      - func: "format test"
        vars:
          times: 3
          test_format_extra_args: runs.rows=1000000:2500000

  - name: spinlock-pthread-adaptive-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          SPINLOCK_TYPE: -DSPINLOCK_TYPE=pthread_adaptive
      - func: "make check all"
      - func: "unit test"
      - func: "format test"
        vars:
          times: 3
          test_format_extra_args: runs.rows=1000000:2500000

  - name: wtperf-test
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
        vars:
          dependent_task: compile
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # The test will generate WT_TEST directory automatically
            dir=../bench/wtperf/stress
            for file in `ls $dir`
            do
              echo "Disk usage and free space for the current drive (pre-test):"
              df -h .
              echo "===="
              echo "==== Initiating wtperf test using $dir/$file ===="
              echo "===="
              ./bench/wtperf/wtperf -O $dir/$file -o verbose=2
              echo "===="
              echo "Disk usage and free space for the current drive (post-test):"
              df -h .
              echo "Total size of WT_TEST directory prior to move:"
              du -hs WT_TEST
              mv WT_TEST WT_TEST_$file
            done

  - name: long-test
    commands:
      - func: "get project"
      - func: "generate github token"
      - func: "get automation-scripts"
      - func: "configure wiredtiger"
      - func: "make wiredtiger"

      # Run the long version of make check, that includes the full csuite tests
      - func: "make check all"

      # Many dbs test - Run with:
      # 1.  The defaults
      - func: "many dbs test"
      # 2.  Set idle flag to turn off operations.
      - func: "many dbs test"
        vars:
          many_db_args: -I
      # 3.  More dbs.
      - func: "many dbs test"
        vars:
          many_db_args: -D 40
      # 4.  With idle flag and more dbs.
      - func: "many dbs test"
        vars:
          many_db_args: -I -D 40

      # extended test/thread runs
      - func: "thread test"
        vars:
          thread_test_args: -t f
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t f
      - func: "thread test"
        vars:
          thread_test_args: -t r
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t r
      - func: "thread test"
        vars:
          thread_test_args: -t v
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t v

      # random-abort - default (random time and number of threads)
      - func: "csuite test"
        vars:
          test_name: random_abort

      # truncated-log
      - func: "csuite test"
        vars:
          test_name: truncated_log

      # random-abort - minimum time, random number of threads
      - func: "csuite test"
        vars:
          test_args: -t 10
          test_name: random_abort
      # random-abort - maximum time, random number of threads
      - func: "csuite test"
        vars:
          test_args: -t 40
          test_name: random_abort
      # random-abort - run compaction
      - func: "csuite test"
        vars:
          test_args: -c -t 60
          test_name: random_abort

      # format test
      - func: "format test"
        vars:
          test_format_extra_args: file_type=fix runs.rows=1000000:2500000
      - func: "format test"
        vars:
          test_format_extra_args: file_type=row runs.rows=1000000:2500000

      # format test for stressing compaction code path
      - func: "format test"
        vars:
          times: 3
          test_format_extra_args: file_type=row compaction=1 verify=1 runs.rows=1000000:2500000 runs.timer=3 ops.pct.delete=30

  - name: time-shift-sensitivity-test
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/csuite"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            RW_LOCK_FILE=$(pwd)/../../cmake_build/test/csuite/rwlock/test_rwlock ./time_shift_test.sh /usr/local/lib/faketime/libfaketimeMT.so.1 0-1 2>&1

  - name: format-mirror-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          config: ../../../test/format/CONFIG.mirror
          trace_args: -T all

  - name: format-msan-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test script"
        vars:
          # run for 10 minutes.
          format_test_script_args: -c ../../../test/format/CONFIG.msan -t 10 rows=10000 ops=50000

  - name: format-stress-pull-request-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test script"
        vars:
          # run for 10 minutes.
          format_test_script_args: -t 10 rows=10000 ops=50000

  - name: format-smoke-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -S
      - func: "format test"
        vars:
          test_format_extra_args: -C "verbose=(checkpoint_cleanup:1)"

  - name: format-asan-smoke-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -S
      - func: "format test"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          test_format_extra_args: -C "verbose=(checkpoint_cleanup:1)"

  - name: format-wtperf-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            cp ../../../bench/wtperf/runners/split_heavy.wtperf .
            ./wtperf -O ./split_heavy.wtperf -o verbose=2

  - name: memory-model-test
    exec_timeout_secs: 86400
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/tools/memory-model-test"
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            g++ -o memory_model_test -O2 memory_model_test.cpp -lpthread -std=c++20 -Wall -Werror
            ./memory_model_test -n 100000000

  - name: memory-model-test-mac
    exec_timeout_secs: 86400
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/tools/memory-model-test"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            sysctl -n machdep.cpu.brand_string     # Display the CPU type
            sw_vers                                # Display the macOS version
            g++ -o memory_model_test -O2 memory_model_test.cpp -lpthread -std=c++17 -Wall -Werror -DAVOID_CPP20_SEMAPHORE
            ./memory_model_test -n 100000000

  - name: tsan-playground-test
    exec_timeout_secs: 86400
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ../tools/tsan_playground/collect_warnings.sh

  - name: data-validation-disagg-stress-test-checkpoint
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s1
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 1 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s1-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 1 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s2
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 2 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s3
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 3 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s3-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 3 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s4
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 4 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s5
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 5 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s5-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 5 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s6
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 6 -C cache_size=100MB

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s7
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 7 -C cache_size=100MB

  - name: data-validation-stress-test-precise-checkpoint
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s1
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 1 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s2
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 2 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s3
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 3 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s4
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 4 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s5
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 5 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s6
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -p -s 6 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: data-validation-stress-test-precise-checkpoint-fp-hs-insert-s7
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 7 -C cache_size=1000MB,precise_checkpoint=true,preserve_prepared=true

  - name: format-failure-configs-test
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/evergreen"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./run_format_configs.sh -j ${num_jobs}

  - name: static-wt-build-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib_with_builtins
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            # -V option displays Wiredtiger library version
            ./wt -V

            if [ $? -ne 0 ]; then
              echo "Error, WT util is not generated or is not functioning"
              exit 1
            fi

            # Test if libwiredtiger is dynamically linked.
            (ldd wt | grep "libwiredtiger.so") || wt_static_build=1

            if [ $wt_static_build -ne 1 ]; then
              echo "Error, WT util is not statically linked"
              exit 1
            fi

  - name: checkpoint-stress-test
    tags: ["stress-test-1"]
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          no_of_procs: 10 # Number of processes to run in the background
          tiered: 0       # Don't enable tiered storage
          times: 1        # Number of times to run the loop

  - name: checkpoint-stress-test-tiered
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          no_of_procs: 5  # Number of processes to run in the background
          tiered: 1       # Enable tiered storage
          times: 1        # Number of times to run the loop

  - name: precise-checkpoint-stress-test
    tags: ["stress-test-1"]
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          timestamp_config: -x
          wt_config: cache_size=1000M,precise_checkpoint=true
          no_of_procs: 10 # Number of processes to run in the background
          tiered: 0       # Don't enable tiered storage
          times: 1        # Number of times to run the loop

  - name: precise-checkpoint-stress-test-tiered
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          timestamp_config: -x
          wt_config: cache_size=1000M,precise_checkpoint=true
          no_of_procs: 5  # Number of processes to run in the background
          tiered: 1       # Enable tiered storage
          times: 1        # Number of times to run the loop

  - name: skiplist-stress-test
    tags: ["stress-test-1", "stress-test-zseries-1"]
    exec_timeout_secs: 3600 # 1 hour
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/bench/workgen/runner"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            for i in $(seq 5); do
                ${python_binary|python3} skiplist_stress.py
            done

  - name: chunkcache-test
    exec_timeout_secs: 1200 # 20 minutes
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/bench/workgen/runner"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} chunkcache_simple.py

  - name: split-stress-test
    tags: ["stress-test-1", "stress-test-ppc-1", "stress-test-zseries-1"]
    # Set 5 hours timeout (60 * 60 * 5)
    exec_timeout_secs: 18000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "split stress test"

  - name: model-unit-test
    tags: ["pull_request", "model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/test"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./test_model.sh

  - name: model-test-failure-workloads
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/evergreen"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./run_model_workloads.sh

  - name: model-test-long
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/tools"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Check if we running in disagg mode.
            disagg_args=""
            if [[ "${disagg_run|false}" == "true" ]]; then
                disagg_args="-D"
            fi
            echo "./model_test $disagg_args -l 2000-3000 -t 3600"
            # Keep repeating the model test for 60 minutes
            ./model_test $disagg_args -l 2000-3000 -t 3600

  - name: model-test-long-random-config
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/tools"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Check if we running in disagg mode.
            disagg_args=""
            if [[ "${disagg_run|false}" == "true" ]]; then
                disagg_args="-D"
            fi

            # Keep repeating the model test for 60 minutes
            ./model_test $disagg_args -l 100-200 -g -t 3600

  - name: model-test-long-with-coverage
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Coverage
          CODE_COVERAGE_FLAGS: -DCODE_COVERAGE_MEASUREMENT=1 -DINLINE_FUNCTIONS_INSTEAD_OF_MACROS=1
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/tools"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}

            # Record the start time, in seconds (needed by the code coverage analysis step below)
            date +%s > ../../../../time.txt

            # Keep repeating the model test for 60 minutes
            ./model_test -l 2000-3000 -t 3600

            # Record the end time, in seconds
            date +%s >> ../../../../time.txt
      - func: "code coverage analysis"
        vars:
          coverage_filter: "src/rollback_to_stable"
          working_dir: "wiredtiger"
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: format-stress-zseries-test
    tags: ["stress-test-zseries-1"]
    # Set 2.5 hours timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          # FIXME-WT-13690: Enable ZSTD compression once failure has been resolved.
          HAVE_BUILTIN_EXTENSION_ZSTD: -DHAVE_BUILTIN_EXTENSION_ZSTD=0
      - func: "format test script"
        vars:
          #run for 2 hours ( 2 * 60 = 120 minutes), use default config
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 120

  - name: format-stress-ppc-test
    tags: ["stress-test-ppc-1"]
    # Set 2.5 hours timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          #run for 2 hours ( 2 * 60 = 120 minutes), use default config
          # Always disable mmap for PPC due to issues on variant setup.
          # See https://bugzilla.redhat.com/show_bug.cgi?id=1686261#c10 for the potential cause.
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 120 -- -C "mmap=false,mmap_all=false"

  - <<: *format-stress-test
    name: format-stress-test-1
    tags: ["stress-test-1"]
  - <<: *format-stress-test
    name: format-stress-test-2
    tags: ["stress-test-2"]
  - <<: *format-stress-test
    name: format-stress-test-3
    tags: ["stress-test-3"]
  - <<: *format-stress-test
    name: format-stress-test-4
    tags: ["stress-test-4"]
  - <<: *format-stress-asan-ppc-test
    name: format-stress-asan-ppc-test-1
    tags: ["stress-test-ppc-1"]
  - <<: *format-stress-asan-ppc-test
    name: format-stress-asan-ppc-test-2
    tags: ["stress-test-ppc-2"]
  - <<: *format-stress-asan-test
    name: format-stress-asan-test-1
    tags: ["stress-test-asan"]
  - <<: *format-stress-asan-test
    name: format-stress-asan-test-2
    tags: ["stress-test-asan"]
  - <<: *format-stress-asan-test
    name: format-stress-asan-test-3
    tags: ["stress-test-asan"]
  - <<: *format-stress-asan-test
    name: format-stress-asan-test-4
    tags: ["stress-test-asan"]
  - <<: *race-condition-stress-asan-test
    name: race-condition-stress-asan-test-1
    tags: ["stress-test-asan"]
  - <<: *race-condition-stress-asan-test
    name: race-condition-stress-asan-test-2
    tags: ["stress-test-asan"]
  - <<: *race-condition-stress-asan-test
    name: race-condition-stress-asan-test-3
    tags: ["stress-test-asan"]
  - <<: *race-condition-stress-asan-test
    name: race-condition-stress-asan-test-4
    tags: ["stress-test-asan"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-1
    tags: ["stress-test-1", "stress-test-zseries-1"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-2
    tags: ["stress-test-2", "stress-test-zseries-2"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-3
    tags: ["stress-test-3", "stress-test-zseries-3"]
   # FIXME-WT-14566: Enable disagg leader mode on all platforms once failures has been resolved.
  - <<: *format-stress-test-disagg-leader
    name: format-stress-test-disagg-leader-1
    tags: ["stress-test-disagg-fail", "stress-test-disagg-asan-fail"]
  - <<: *format-stress-test-disagg-leader
    name: format-stress-test-disagg-leader-2
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-leader
    name: format-stress-test-disagg-leader-3
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-leader
    name: format-stress-test-disagg-leader-4
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-leader
    name: format-stress-test-disagg-leader-5
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-data-validation-test-disagg-leader
    name: format-stress-data-validation-test-disagg-leader-1
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-data-validation-test-disagg-leader
    name: format-stress-data-validation-test-disagg-leader-2
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-data-validation-test-disagg-leader
    name: format-stress-data-validation-test-disagg-leader-3
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-data-validation-test-disagg-leader
    name: format-stress-data-validation-test-disagg-leader-4
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-data-validation-test-disagg-leader
    name: format-stress-data-validation-test-disagg-leader-5
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-follower
    name: format-stress-test-disagg-follower-1
    tags: ["stress-test-disagg", "stress-test-disagg-asan-fail"]
  - <<: *format-stress-test-disagg-follower
    name: format-stress-test-disagg-follower-2
    tags: ["stress-test-disagg"]
  - <<: *format-stress-test-disagg-follower
    name: format-stress-test-disagg-follower-3
    tags: ["stress-test-disagg"]
  - <<: *format-stress-test-disagg-follower
    name: format-stress-test-disagg-follower-4
    tags: ["stress-test-disagg"]
  - <<: *format-stress-test-disagg-follower
    name: format-stress-test-disagg-follower-5
    tags: ["stress-test-disagg"]
  - <<: *format-stress-data-validation-test-disagg-follower
    name: format-stress-data-validation-test-disagg-follower-1
    tags: ["stress-test-disagg"]
  - <<: *format-stress-data-validation-test-disagg-follower
    name: format-stress-data-validation-test-disagg-follower-2
    tags: ["stress-test-disagg"]
  - <<: *format-stress-data-validation-test-disagg-follower
    name: format-stress-data-validation-test-disagg-follower-3
    tags: ["stress-test-disagg"]
  - <<: *format-stress-data-validation-test-disagg-follower
    name: format-stress-data-validation-test-disagg-follower-4
    tags: ["stress-test-disagg"]
  - <<: *format-stress-data-validation-test-disagg-follower
    name: format-stress-data-validation-test-disagg-follower-5
    tags: ["stress-test-disagg"]
  # FIXME-WT-14566: Enable disagg switch mode on all platforms once failures has been resolved.
  - <<: *format-stress-test-disagg-switch
    name: format-stress-test-disagg-switch-1
    tags: ["stress-test-disagg-fail", "stress-test-disagg-asan-fail"]
  - <<: *format-stress-test-disagg-switch
    name: format-stress-test-disagg-switch-2
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-switch
    name: format-stress-test-disagg-switch-3
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-switch
    name: format-stress-test-disagg-switch-4
    tags: ["stress-test-disagg-fail"]
  - <<: *format-stress-test-disagg-switch
    name: format-stress-test-disagg-switch-5
    tags: ["stress-test-disagg-fail"]

  - name: format-stress-test-no-barrier
    tags: ["stress-test-no-barrier"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 360

  - name: format-stress-asan-test-no-barrier
    tags: ["stress-test-asan"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -t 360

  - name: race-condition-stress-asan-test-no-barrier
    tags: ["stress-test-asan"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -R -t 360

  - name: recovery-stress-test-no-barrier
    tags: ["stress-test-no-barrier"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
          CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      - func: "recovery stress test script"
        vars:
          times: 25

  - name: format-abort-recovery-stress-test
    commands:
      # Allow 30 minutes beyond test runtime because recovery under load can cause the test to
      # run longer.
      - command: timeout.update
        params:
          exec_timeout_secs: 3600
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -a -t 30

  - name: format-predictable-test
    # Set 2.5 hour timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test predictable"
        vars:
          times: 5

  - name: schema-abort-predictable-test
    # Set 20 minute timeout (60 * 20)
    exec_timeout_secs: 1200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "schema abort predictable"
        vars:
          times: 5

  - name: checkpoint-filetypes-predictable-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          # Don't use diagnostic - this test looks for timing problems that are more likely to occur without it
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      # FIXME-WT-10936: Enable once predictable replay supports column store
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -t m -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -t c -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      - func: "checkpoint test predictable"
        vars:
          checkpoint_args: -t r -n 1000000 -k 5000000 -C cache_size=100MB
          times: 5

  - name: many-collection-test
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 86400
          timeout_secs: 86400
      - func: "get project"
      - func: "fetch mongo repo"
      - func: "get engflow creds"
      - func: "import wiredtiger into mongo"
      - func: "compile mongodb"
      - func: "fetch mongo-tests repo"
      # FIXME-WT-7868: we should download a pre populated database here and remove the
      # "clean-and-populate" argument in the step below.
      - command: shell.exec
        params:
          working_dir: mongo-tests/largescale
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}
            virtualenv -p python3 venv
            source venv/bin/activate
            # Need both pymongo and pymongo[srv] as upload-results-atlas.py uses mongo+srv for the URI.
            pip3 install lorem==0.1.1 pymongo==3.12.2 "pymongo[srv]==3.12.2"
            mongod_path=$(find -L ../../mongo/bazel-bin -executable -type f -path \*/install-mongod/bin/mongod)
            echo "mongod_path = '$mongod_path'"
            # Log the actual path, using ls to display any symlinks
            ls -l $mongod_path
            ./run_many_coll.sh $mongod_path mongodb.log config/many-collection-testing many-collection clean-and-populate
      - func: "convert-to-atlas-evergreen-format"
        vars:
          input_file: ./mongo-tests/largescale/many-collection-artifacts/results/results.json
          test_name: many-collection-test
          output_path: ./mongo-tests/largescale/many-collection-artifacts/results/
      - func: "upload stats to atlas"
        vars:
          stats_dir: mongo-tests/largescale/many-collection-artifacts/results
          test-name: many-collection-test
      - func: "upload stats to evergreen"
        vars:
          stats_dir: mongo-tests/largescale/many-collection-artifacts/results
          test-name: many-collection-test
      - func: "upload artifact"
        vars:
          upload_filename: many-collection-test.tgz
          upload_source_dir: mongo-tests/largescale/many-collection-artifacts
      # Call cleanup function to avoid duplicated artifact upload in the post-task stage.
      - func: "cleanup"

  - name: cyclomatic-complexity
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            t=__wt.$$
            set -o verbose
            ${PREPARE_PATH}
            ./test/evergreen/cyclomatic-complexity.sh
      - func: "upload stats to atlas"
        vars:
          stats_dir: ./wiredtiger/code_statistics_report
          test-name: code_complexity
          collection: CodeComplexity

  - name: modularity_metrics
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o verbose
            set -o errexit
            ${PREPARE_PATH}
            ./dist/modstat -me --unmod > atlas_out_modularity_metrics.json
      - func: "upload stats to atlas"
        vars:
          stats_dir: ./wiredtiger
          test-name: modularity_metrics
          collection: CodeModularityMetrics

  - name: generate-tsan-metric
    tags: ["pull_request_code_statistics"]
    depends_on:
      - name: "unit-test-tsan"
    commands:
      - func: "get project"
      - func: "fetch artifacts"
        vars:
          dependent_task: unit-test-tsan
          destination: wiredtiger/unit-test-tsan
      - func: "tsan warning metric"
        vars:
          # Track warnings only after time 2025-08-06 06:07:50.655735+00:00
          timestamp: 1754460496

  - name: generate-tsan-metric-disagg
    tags: ["pull_request_code_statistics"]
    depends_on:
      - name: "unit-test-disagg-leader-tsan"
      - name: "unit-test-disagg-follower-tsan"
    commands:
      - func: "get project"
      - func: "fetch artifacts"
        vars:
          dependent_task: unit-test-disagg-leader-tsan
          destination: wiredtiger/unit-test-disagg-leader-tsan
      - func: "fetch artifacts"
        vars:
          dependent_task: unit-test-disagg-follower-tsan
          destination: wiredtiger/unit-test-disagg-follower-tsan
      - func: "tsan warning metric"
        vars:
          # Track warnings right before disagg merge 2025-08-02.
          timestamp: 1746576000

    ###############################
    # Performance Tests for btree #
    ###############################

  - name: perf-test-small-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: small-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: small-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: small-btree.wtperf

  - name: perf-test-small-btree-backup
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: small-btree-backup.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: small-btree-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: small-btree-backup.wtperf

  - name: perf-test-medium-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-btree.wtperf

  - name: perf-test-medium-btree-backup
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-btree-backup.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-btree-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-btree-backup.wtperf

  - name: perf-test-parallel-pop-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-btree.wtperf

  - name: perf-test-parallel-pop-btree-long
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-btree-long.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-btree-long.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-btree-long.wtperf

  - name: perf-test-short-key-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: short-key-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: short-key-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: short-key-btree.wtperf

  - name: perf-test-update-only-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-only-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-only-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-only-btree.wtperf

  - name: perf-test-update-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-btree.wtperf
          maxruns: 1
          wtarg: "-bf ../../../bench/wtperf/runners/update-btree.json"
      - func: "upload stats to atlas"
        vars:
          test-name: update-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-btree.wtperf

  - name: perf-test-update-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-large-record-btree.wtperf

  - name: perf-test-modify-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: modify-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "modify"']
      - func: "upload stats to atlas"
        vars:
          test-name: modify-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: modify-large-record-btree.wtperf

  - name: perf-test-modify-force-update-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: modify-force-update-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "modify"']
      - func: "upload stats to atlas"
        vars:
          test-name: modify-force-update-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: modify-force-update-large-record-btree.wtperf

  - name: perf-test-modify-read-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: modify-read-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "modify", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: modify-read-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: modify-read-btree.wtperf

    ###############################
    # Performance Tests for oplog #
    ###############################

  - name: perf-test-mongodb-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-oplog.wtperf

  - name: perf-test-mongodb-small-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-small-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-small-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-small-oplog.wtperf

  - name: perf-test-mongodb-large-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-large-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-large-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-large-oplog.wtperf

  - name: perf-test-mongodb-secondary-apply
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-secondary-apply.wtperf
          maxruns: 1
          wtarg: -ops ['"insert"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-secondary-apply.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-secondary-apply.wtperf

    #########################################
    # Performance Tests for perf-checkpoint #
    #########################################

  - name: perf-test-update-checkpoint-btree
    tags: ["checkpoint-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-checkpoint-btree.wtperf
          maxruns: 1
          wtarg: "-bf ../../../bench/wtperf/runners/update-checkpoint.json"
      - func: "upload stats to atlas"
        vars:
          test-name: update-checkpoint-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-checkpoint-btree.wtperf

    ###############################
    # Performance Tests for stress #
    ###############################

  - name: perf-test-overflow-10k
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: overflow-10k.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: overflow-10k.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: overflow-10k.wtperf

  - name: perf-test-overflow-130k
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: overflow-130k.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: overflow-130k.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: overflow-130k.wtperf

  - name: perf-test-parallel-pop-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-stress.wtperf

  - name: perf-test-update-grow-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-grow-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-grow-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-grow-stress.wtperf

  - name: perf-test-update-shrink-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-shrink-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-shrink-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-shrink-stress.wtperf

  - name: perf-test-update-delta-mix1
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix1.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix1.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix1.wtperf

  - name: perf-test-update-delta-mix2
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix2.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix2.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix2.wtperf

  - name: perf-test-update-delta-mix3
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix3.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix3.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix3.wtperf

  - name: perf-test-multi-btree-zipfian
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: multi-btree-zipfian-populate.wtperf
          maxruns: 1
      - func: "run-perf-test"
        vars:
          perf-test-name: multi-btree-zipfian-workload.wtperf
          maxruns: 1
          no_create: true
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: multi-btree-zipfian-workload.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: multi-btree-zipfian-workload.wtperf

  - name: perf-test-many-table-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: many-table-stress.wtperf
          maxruns: 1

  - name: perf-test-many-table-stress-backup
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: many-table-stress-backup.wtperf
          maxruns: 1

  - name: perf-test-evict-fairness
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-fairness.wtperf
          maxruns: 1
          wtarg: -args ['"-C statistics_log=(wait=10000,on_close=true,json=false,sources=[file:])", "-o reopen_connection=false"'] -ops ['"eviction_page_seen"']
      - func: "validate-expected-stats"
        vars:
          stat_file: './test_stats/evergreen_out_evict-fairness.wtperf.json'
          comparison_op: "eq"
          expected-stats: '{"Pages seen by eviction": 200}'

  - name: perf-test-evict-btree-stress-multi
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree-stress-multi.wtperf
          maxruns: 1
          wtarg: -ops ['"warnings", "top5_latencies_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree-stress-multi.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree-stress-multi.wtperf

    ##################################
    # Performance Tests for eviction #
    ##################################

  - name: perf-test-evict-btree
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree.wtperf

  - name: perf-test-evict-btree-1
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree-1.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree-1.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree-1.wtperf

  - name: perf-cache-workload-dirty-trigger
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_workload_dirty_trigger.py
          maxruns: 1
          wtarg: -ops ['"app_dirty_attempt", "app_attempt", "trigger_updates", "trigger_dirty"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_workload_dirty_trigger.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_workload_dirty_trigger.py

  - name: perf-cache-workload-update-trigger
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_workload_update_trigger.py
          maxruns: 1
          wtarg: -ops ['"app_dirty_attempt", "app_attempt", "trigger_updates", "trigger_dirty"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_workload_update_trigger.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_workload_update_trigger.py

  - name: perf-cache_dirty_trigger_read-10_write-90
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_dirty_trigger_read-10_write-90.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_dirty_trigger_read-10_write-90.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_dirty_trigger_read-10_write-90.py

  - name: perf-cache_dirty_trigger_read-25_write-75
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_dirty_trigger_read-25_write-75.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_dirty_trigger_read-25_write-75.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_dirty_trigger_read-25_write-75.py

  - name: perf-cache_dirty_trigger_read-50_write-50
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_dirty_trigger_read-50_write-50.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_dirty_trigger_read-50_write-50.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_dirty_trigger_read-50_write-50.py

  - name: perf-cache_dirty_trigger_read-75_write-25
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_dirty_trigger_read-75_write-25.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_dirty_trigger_read-75_write-25.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_dirty_trigger_read-75_write-25.py

  - name: perf-cache_dirty_trigger_read-90_write-10
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_dirty_trigger_read-90_write-10.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_dirty_trigger_read-90_write-10.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_dirty_trigger_read-90_write-10.py

  - name: perf-cache_update_trigger_read-10_write-90
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_update_trigger_read-10_write-90.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_update_trigger_read-10_write-90.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_update_trigger_read-10_write-90.py

  - name: perf-cache_update_trigger_read-25_write-75
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_update_trigger_read-25_write-75.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_update_trigger_read-25_write-75.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_update_trigger_read-25_write-75.py

  - name: perf-cache_update_trigger_read-50_write-50
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_update_trigger_read-50_write-50.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_update_trigger_read-50_write-50.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_update_trigger_read-50_write-50.py

  - name: perf-cache_update_trigger_read-75_write-25
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_update_trigger_read-75_write-25.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_update_trigger_read-75_write-25.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_update_trigger_read-75_write-25.py

  - name: perf-cache_update_trigger_read-90_write-10
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: cache_update_trigger_read-90_write-10.py
          maxruns: 1
          wtarg: -ops ['"read_ops","update_ops","read_latency_us","read_latency_ms","read_latency_sec","update_latency_us","update_latency_ms","update_latency_sec"']
      - func: "upload stats to atlas"
        vars:
          test-name: cache_update_trigger_read-90_write-10.py
      - func: "upload stats to evergreen"
        vars:
          test-name: cache_update_trigger_read-90_write-10.py

    ###########################################
    # Performance Tests for log consolidation #
    ###########################################

  - name: perf-test-log
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -ops ['"update", "min_max_update_throughput"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-small-files
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-no-checkpoints
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C checkpoint=(wait=0)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-no-prealloc
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M,prealloc=false)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-zero-fill
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M,zero_fill=true)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-many-threads
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M),session_max=256", "-o threads=((count=128,updates=1))"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

    #####################################
    # Performance Tests for chunk cache #
    #####################################

  - name: perf-test-chunk-cache
    tags: ["chunk-cache-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: chunk-cache-reads.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: chunk-cache-reads.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: chunk-cache-reads.wtperf

  - name: perf-test-chunk-cache-base
    tags: ["chunk-cache-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: chunk-cache-reads-base.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: chunk-cache-reads-base.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: chunk-cache-reads-base.wtperf

    ###########################################
    #        Performance Long Tests           #
    ###########################################

  - name: perf-test-long-500m-btree-populate
    tags: ["long-perf"]
    depends_on:
      - name: compile
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 7200
          timeout_secs: 7200
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-populate.wtperf
          maxruns: 1
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"load", "warnings", "max_latency_insert"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-populate.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-populate.wtperf
      - func: "upload artifact"
        vars:
          upload_filename: WT_TEST.tgz
          upload_source_dir: wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0/
      # Now take and upload a backup. We use this as the source in our live restore perf tests.
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} ../../../test/live_restore/take_backup.py
      - func: "upload artifact"
        vars:
          upload_filename: WT_TEST_backup.tgz
          upload_source_dir: wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0_backup/
          postfix: "_backup"
      # Call cleanup function to avoid duplicated artifact upload in the post-task stage.
      - func: "cleanup"

  - name: perf-test-long-500m-btree-50r50u
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-50r50u-backup
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u-backup.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u-backup.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-80r20u
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-80r20u.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-80r20u.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-80r20u.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-rdonly
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-rdonly.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "warnings", "max_latency_read_update", "min_max_read_throughput"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-rdonly.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-rdonly.wtperf
      - func: "cleanup"

  - name: perf-test-long-checkpoint-stress
    tags: ["long-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: checkpoint-stress.wtperf
          maxruns: 1
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"update", "checkpoint"']
      - func: "upload stats to atlas"
        vars:
          test-name: checkpoint-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: checkpoint-stress.wtperf

  - name: perf-test-live-restore
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task. This will be used
      # as the source for live restore.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/SOURCE"
          postfix: "_backup"
      # Create an empty home directory to be used as the destination for live restore.
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            mkdir WT_TEST_0_0
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u-live-restore.wtperf
          maxruns: 1
          no_create: true
          wtarg: -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u-live-restore.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u-live-restore.wtperf
      - func: "cleanup"

  - name: perf-test-live-restore-no-server
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task. This will be used
      # as the source for live restore.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/SOURCE"
          postfix: "_backup"
      # Create an empty home directory to be used as the destination for live restore.
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            mkdir WT_TEST_0_0
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u-live-restore-no-server.wtperf
          maxruns: 1
          no_create: true
          wtarg: -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u-live-restore-no-server.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u-live-restore-no-server.wtperf
      - func: "cleanup"

  - name: many-dhandle-stress
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: many-dhandle-stress.py
          maxruns: 1
          wtarg: -ops ['"max_latency_create", "max_latency_drop", "max_latency_drop_diff", "max_latency_insert_micro_sec", "max_latency_read_micro_sec", "max_latency_update_micro_sec", "warning_idle", "warning_idle_create", "warning_idle_drop", "warning_insert", "warning_operations", "warning_read", "warning_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: many-dhandle-stress.py
      - func: "upload stats to evergreen"
        vars:
          test-name: many-dhandle-stress.py
      - func: "validate-expected-stats"
        vars:
          stat_file: './test_stats/evergreen_out_many-dhandle-stress.py.json'
          comparison_op: "lt"
          expected-stats: '{"Warning Idle (drop)": 50, "Latency drop(in sec.) Max1": 500, "Latency warnings (read, insert, update)": 500}'

  - name: prefetch-off-verify
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: microbenchmark_prefetch_off_verify.py
          maxruns: 1
          wtarg: -ops ['"block_read"']
      - func: "upload stats to atlas"
        vars:
          test-name: microbenchmark_prefetch_off_verify.py
      - func: "upload stats to evergreen"
        vars:
          test-name: microbenchmark_prefetch_off_verify.py

  - name: prefetch-on-verify
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: microbenchmark_prefetch_on_verify.py
          maxruns: 1
          wtarg: -ops ['"block_read"']
      - func: "upload stats to atlas"
        vars:
          test-name: microbenchmark_prefetch_on_verify.py
      - func: "upload stats to evergreen"
        vars:
          test-name: microbenchmark_prefetch_on_verify.py

  - name: bench-wt2853-perf-test-row
    tags: ["wt2853-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "wt2853_perf test"
        vars:
          wt2853_perf_args: "-t r"
      - func: "upload test stats"
        vars:
          test_path: bench/wt2853_perf/wt2853_perf

  - name: bench-wt2853-perf-test-col
    tags: ["wt2853-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "wt2853_perf test"
        vars:
          wt2853_perf_args: "-t c"
      - func: "upload test stats"
        vars:
          test_path: bench/wt2853_perf/wt2853_perf

  - <<: *workgen-test
    name: "workgen-test-compress_ratio"

  - <<: *workgen-test
    name: "workgen-test-evict-btree-hs"

  - <<: *workgen-test
    name: "workgen-test-example_dynamic_tables"

  - <<: *workgen-test
    name: "workgen-test-example_prepare"

  - <<: *workgen-test
    name: "workgen-test-example_prepare_evict_reconcile"

  - <<: *workgen-test
    name: "workgen-test-example_simple"

  - <<: *workgen-test
    name: "workgen-test-example_txn"

  - <<: *workgen-test
    name: "workgen-test-insert_stress"

  - <<: *workgen-test
    name: "workgen-test-insert_test"

  - <<: *workgen-test
    name: "workgen-test-maintain_low_dirty_cache"

  - <<: *workgen-test
    name: "workgen-test-many-dhandle-stress"

  - <<: *workgen-test
    name: "workgen-test-multi_btree_heavy_stress"

  - <<: *workgen-test
    name: "workgen-test-multiversion"

  - <<: *workgen-test
    name: "workgen-test-prepare_stress"

  - <<: *workgen-test
    name: "workgen-test-read_write_storms"

  - <<: *workgen-test
    name: "workgen-test-read_write_sync_long"

  - <<: *workgen-test
    name: "workgen-test-read_write_sync_short"

  - <<: *workgen-test
    name: "workgen-test-skiplist_stress"

  - <<: *workgen-test
    name: "workgen-test-small_btree"

  - <<: *workgen-test
    name: "workgen-test-small_btree_reopen"

  - <<: *workgen-test
    name: "workgen-test-microbenchmark_prefetch_off_verify"

  - <<: *workgen-test
    name: "workgen-test-microbenchmark_prefetch_on_verify"

#######################################
#     Antithesis Integration          #
#######################################

  - name: debug-have-diagnostic
    tags: ["antithesis"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "build and push antithesis container"

  - name: release-with-debug-have-diagnostic
    tags: ["antithesis"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=1
      - func: "build and push antithesis container"

#######################################
#            Buildvariants            #
#######################################

buildvariants:

- name: ubuntu2004
  display_name: "! Ubuntu 20.04"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 1
    data_validation_stress_test_args: -t r -m -W 3 -D -p -n 100000 -k 100000
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` / 2" | bc)
  tasks:
    - name: ".pull_request !.pull_request_compilers"
    - name: ".lint"
    - name: ".unit_test_long"
      distros: ubuntu2004-large
    - name: compile
    - name: doc-compile
    - name: make-check-test
    - name: configure-combinations
    - name: syscall-linux
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-random-seed
    - name: unit-test-hook-disagg-follower
    - name: unit-test-hook-disagg-follower-table
    - name: unit-test-hook-tiered
    - name: unit-test-hook-tiered-with-delays
    - name: unit-test-hook-tiered-timestamp
    - name: unit-test-hook-timing-stress-log
    - name: unit-test-hook-timestamp
    - name: test-prepare-hs03-hook-timestamp
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-large
    - name: long-test
      distros: ubuntu2004-large
    - name: unit-test-extra-long
      distros: ubuntu2004-large
    - name: static-wt-build-test
    - name: format-mirror-test
    - name: format-smoke-test
    - name: format-failure-configs-test
      distros: ubuntu2004-large
    - name: ".data-validation-stress-test"
    - name: catch2-test
    - name: s3-tiered-storage-extensions-test
    # FIXME-WT-12812 - This test is unstable when run with the new tmalloc install script. Disabled while WT-12812 is under investigation.
    # - name: azure-gcp-tiered-storage-extensions-test
    - name: bench-tiered-push-pull
    - name: catch2-assertions
    - name: ".tiered_unittest"
    - name: bench-tiered-push-pull-s3
    # FIXME-WT-14962 - Added for tiered storage; may help disagg but currently a CI blocker and needs re-evaluation.
    # - name: csuite-timestamp-abort-test-s3
    - name: unit-test-hook-tiered-s3
    - name: chunkcache-test
    - name: ".workgen-test"
      batchtime: 1440 # 24 hours
    - name: model-unit-test
    - name: model-test-failure-workloads
      batchtime: 1440 # 24 hours
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-with-coverage
      batchtime: 1440 # once a day
    - name: model-test-long-random-config
      batchtime: 1440 # once a day
    - name: csuite-long-running
      batchtime: 1440 # once a day
    - name: live-restore-long

- name: ubuntu2004-asan
  display_name: "! Ubuntu 20.04 ASAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export LSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/asan_leaks.supp"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    # Exclude wt12015_backup_corruption for ASan builds. It creates false positives when it deliberately kills processes that invoke corruption.
    ctest_extra_args: -E "wt12015_backup_corruption"
  tasks:
    - name: ".pull_request !.pull_request_compilers !.python !.tiered_unittest !csuite-wt12015-backup-corruption-test"
    - name: examples-c-test
    - name: format-asan-smoke-test
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-random-config
      batchtime: 1440 # once a day
    - name: live-restore-long
    - name: ".stress-test-asan"

- name: ubuntu2004-tsan
  display_name: "! Ubuntu 20.04 TSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=TSan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export TSAN_OPTIONS="$COMMON_SAN_OPTIONS:history_size=7:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/tsan_warnings.supp"
    num_jobs: 2
  tasks:
    - name: compile
    - name: examples-c-test
    - name: unit-test-tsan
      distros: ubuntu2004-large
    - name: unit-test-disagg-leader-tsan
      distros: ubuntu2004-large
    - name: unit-test-disagg-follower-tsan
      distros: ubuntu2004-large
    - name: generate-tsan-metric
    - name: tsan-playground-test
    - name: generate-tsan-metric-disagg

# FIXME-WT-13256
# This variant runs all possible tasks under TSan but instructs TSan to not report any issues it
# encounters. This allows us to check whether a TSan change may cause unexpected failures (for example
# if adding a new field to a struct wrapped in #ifdef TSAN changes the struct layout and causes a segfault)
# while also not having to fix every TSan issue discovered. At the conclusion of the TSan work this
# variant will be deleted.
- name: ubuntu2004-tsan-no-throw
  display_name: "~ Ubuntu 20.04 TSAN (No throw)"
  patch_only: true
  run_on:
  - ubuntu2004-large
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=TSan
    ENABLE_TCMALLOC: 0
    data_validation_stress_test_args: -t r -m -W 3 -D -p -n 100000 -k 100000 -C cache_size=100MB
    additional_env_vars: |
      # We intentionally suppress all TSan warnings.
      export TSAN_OPTIONS="detect_deadlocks=0:report_bugs=0:report_atomic_races=0:allocator_may_return_null=1"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    # We don't run C++ memory sanitized testing as it creates false positives. TSan also significantly increases the runtime
    # of some csuite tests, so disable the already long tests as well as tests heavily impacted by TSan.
    ctest_extra_args: -LE "cppsuite|long_running|disagg"
    # TSan slows down python testing enough that we start printing "operation took X seconds" warnings.
    # These don't impact functionality, but the python tests fail on detection of any content in stdout so ignore it instead.
    ignore_stdout: --ignore-stdout
    # Reduce the number of test iterations in validation stressing as TSan increases runtime by a factor of roughly 10x
    validation_stress_num_runs: 20
  tasks:
    # Skip the azure/gcp/s3 tests as they don't compile under clang
    - name: ".pull_request !s3-tiered-test-small !.tiered_unittest !azure-gcp-tiered-test-small !.pull_request_compilers !.model_checking !csuite-wt12015-backup-corruption-test"
    - name: compile
    - name: make-check-test
    - name: configure-combinations
    - name: syscall-linux
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-random-seed
    - name: unit-test-hook-timestamp
    - name: test-prepare-hs03-hook-timestamp
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
    - name: static-wt-build-test
    - name: format-smoke-test
    - name: ".data-validation-stress-test"
    - name: catch2-test
    - name: bench-tiered-push-pull
    - name: catch2-assertions

# Very minimal set without any extensions
- name: ubuntu2004-minimal
  display_name: "! Ubuntu 20.04 Minimal"
  batchtime: 480 # 3 times a day
  run_on:
  - ubuntu2004-test
  expansions:
    # Compile with clang so we test that path for the catch2 unittests.
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_PYTHON=0 -DENABLE_LZ4=0 -DENABLE_SNAPPY=0 -DENABLE_ZLIB=0 -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: csuite-tests-fast
    - name: fops
    - name: catch2-test
    - name: examples-c-test

- name: ubuntu2004-msan
  display_name: "! Ubuntu 20.04 MSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=MSan
    # MemorySanitizer requires that all program code is instrumented, otherwise it may generate false reports.
    posix_configure_flags: -DENABLE_LZ4=0 -DENABLE_SNAPPY=0 -DENABLE_ZLIB=0 -DENABLE_ZSTD=0
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    # We don't run C++ memory sanitized testing as it creates false positives. MSAN also causes
    # some csuite tests to take an extended amount of time. These are excluded from the
    # make-check-test task and are covered by the csuite-long-running task.
    ctest_extra_args: -LE "cppsuite|long_running|disagg"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` / 2" | bc)
  tasks:
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-msan-test
    - name: make-check-test
      distros: ubuntu2004-large

- name: ubuntu2004-ubsan
  display_name: "! Ubuntu 20.04 UBSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=UBSan
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-stress-pull-request-test
    - name: make-check-test
      distros: ubuntu2004-large
    - name: cppsuite-default-all

- name: ubuntu2004-compilers
  display_name: "! Ubuntu 20.04 Compilers"
  run_on:
  - ubuntu2004-wt-build
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".pull_request_compilers"

- name: ubuntu2004-stress-tests
  display_name: Ubuntu 20.04 Stress tests
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
      distros: ubuntu2004-medium
    - name: format-abort-recovery-stress-test
    - name: format-predictable-test
    - name: schema-abort-predictable-test
    - name: checkpoint-filetypes-predictable-test

- name: ubuntu2004-stress-tests-arm64
  display_name: Ubuntu 20.04 Stress tests (ARM64)
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: checkpoint-stress-test-tiered
    - name: precise-checkpoint-stress-test-tiered
    - name: format-abort-recovery-stress-test

- name: cppsuite-stress-tests-ubuntu
  display_name: "Cppsuite Stress Tests Ubuntu 20.04"
  batchtime: 720 # twice a day
  run_on:
  # We run on medium as small has too small a disk.
  - ubuntu2004-medium
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".cppsuite-stress-test"

- name: cppsuite-stress-tests-arm64
  display_name: "Cppsuite Stress Tests ARM64"
  batchtime: 720 # twice a day
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".cppsuite-stress-test"

- name: package
  display_name: "~ Package"
  batchtime: 1440 # 1 day
  run_on:
  - ubuntu2004-test
  tasks:
    - name: package

- name: rhel80
  display_name: RHEL 8.0
  run_on:
  - rhel80-test
  expansions:
    ENABLE_TCMALLOC: 0
    cmake_generator: "Unix Makefiles"
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: fops
    - name: time-shift-sensitivity-test
    - name: syscall-linux
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-extra-long
      distros: rhel80-large
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
    - name: wtperf-test
    - name: long-test
    - name: configure-combinations

- name: windows
  display_name: "! Windows"
  run_on:
  - windows-2022-small
  expansions:
    ENABLE_TCMALLOC: 0
    # Remove the default configurations for the toolchain, install prefix and number of jobs.
    CMAKE_TOOLCHAIN_FILE:
    CMAKE_INSTALL_PREFIX:
    num_jobs:
    python_binary: "/cygdrive/c/python/Python311/python"
  tasks:
    - name: compile
    # Windows hosts are slow, use the designed low-perf task to avoid timeout failures.
    - name: make-check-test-low-perf
    - name: ".unit_test"
    - name: fops
    - name: catch2-test

- <<: *mac_test_template
  name: macos-14-arm64
  display_name: "macOS 14 (ARM64)"
  run_on:
    - macos-14-arm64
  batchtime: 120 # 2 hours
  expansions:
    python_binary: '/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11'
    python_config_search_string: '_Python3_EXECUTABLE'
    <<: *mac_test_template_expansions

- name: little-endian
  display_name: "~ Little-endian (x86)"
  run_on:
  - ubuntu2204-large
  batchtime: 4320 # 3 days
  expansions:
    ENABLE_TCMALLOC: 0
    # Must disable ZSTD as its not available on the big endian platform (we generate the data files used for those tests here).
    posix_configure_flags: -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: generate-datafile-little-endian
    - name: verify-datafile-little-endian
    - name: verify-datafile-from-big-endian

- name: big-endian
  display_name: "~ Big-endian (s390x/zSeries)"
  run_on:
  - rhel8-zseries-small
  batchtime: 4320 # 3 days
  expansions:
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: generate-datafile-big-endian
    - name: verify-datafile-big-endian
    - name: verify-datafile-from-little-endian

- name: rhel8-ppc
  display_name: "~ RHEL8 PPC"
  run_on:
  - rhel8-power-small
  batchtime: 120 # 2 hours
  expansions:
    format_test_setting: ulimit -c unlimited
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    # Use quarter of the vCPUs to avoid OOM kill failure and disk issues on this variant.
    num_jobs: $(echo $(grep -c ^processor /proc/cpuinfo) / 4 | bc)
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_STRICT=0
  tasks:
    - name: compile
    - name: unit-test
    - name: format-smoke-test
    - name: format-asan-smoke-test
    - name: format-wtperf-test
    - name: ".stress-test-ppc-1"
    - name: ".stress-test-ppc-2"

- name: rhel8-zseries
  display_name: "~ RHEL8 zSeries"
  run_on:
  - rhel8-zseries-small
  batchtime: 120 # 2 hours
  expansions:
    ENABLE_TCMALLOC: 0
    # Use half number of vCPU to avoid OOM kill failure
    num_jobs: $(echo $(grep -c ^processor /proc/cpuinfo) / 2 | bc)
  tasks:
    - name: compile
    - name: unit-test
    - name: format-smoke-test
    - name: ".stress-test-zseries-1"
    - name: ".stress-test-zseries-2"
    - name: ".stress-test-zseries-3"

- name: ubuntu2004-arm64
  display_name: "~ Ubuntu 20.04 ARM64"
  run_on:
  - ubuntu2004-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: ".workgen-test"
      batchtime: 1440 # 24 hours
    - name: checkpoint-filetypes-test
    - name: chunkcache-test
    - name: compile
    - name: configure-combinations
    - name: fops
    - name: format-failure-configs-test
      distros: ubuntu2004-arm64-large
    - name: format-smoke-test
    - name: format-stress-pull-request-test
    - name: long-test
      distros: ubuntu2004-arm64-large
    - name: make-check-test
    - name: model-test-failure-workloads
      batchtime: 1440 # 24 hours
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-with-coverage
      batchtime: 1440 # once a day
    - name: model-test-long-random-config
      batchtime: 1440 # once a day
    - name: model-unit-test
    - name: s3-tiered-storage-extensions-test
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-arm64-large
    - name: unit-test
    - name: unit-test-extra-long
      distros: ubuntu2004-arm64-large
    - name: unit-test-zstd
    - name: wtperf-test

- name: ubuntu2004-arm64-memory-model
  display_name: "Ubuntu 20.04 ARM64 Memory model test"
  run_on:
  - ubuntu2004-arm64-small
  batchtime: 40320 # 28 days
  expansions:
    ENABLE_TCMALLOC: 0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: memory-model-test

- name: amazon2-arm64
  display_name: "Amazon Linux 2 ARM64"
  run_on:
  - amazon2-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    cmake_generator: "Unix Makefiles"
  # Using large distro for a few tests here as more memory is required.
  tasks:
    - name: checkpoint-filetypes-test
    - name: compile
    - name: fops
    - name: format-failure-configs-test
      distros: amazon2-arm64-large
    - name: format-smoke-test
    - name: format-stress-pull-request-test
    - name: long-test
      distros: amazon2-arm64-large
    - name: make-check-test
      distros: amazon2-arm64-large
    - name: s3-tiered-storage-extensions-test
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: amazon2-arm64-large
    - name: unit-test
    - name: unit-test-extra-long
      distros: amazon2-arm64-large
    - name: unit-test-hook-timing-stress-log
    - name: unit-test-zstd
    - name: wtperf-test

# Antithesis build and push
- name: ubuntu2004-antithesis
  display_name: "~ Ubuntu 20.04 Antithesis"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_ANTITHESIS=1 -DENABLE_STRICT=0
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".antithesis"
      cron: 0 0 * * 4 # once a week (Thursday midnight UTC)
      patchable: false

- name: ubuntu2004-release
  display_name: "~ Ubuntu 20.04 (Release Build)"
  run_on:
  - ubuntu2004-small
  batchtime: 720 # 12 hours
  expansions:
    ENABLE_TCMALLOC: 0
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: format-smoke-test
    - name: fops
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-large
    - name: long-test
      distros: ubuntu2004-large
    - name: configure-combinations
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

- name: ubuntu2004-release-arm64
  display_name: "~ Ubuntu 20.04 (ARM64, Release Build)"
  run_on:
  - ubuntu2004-arm64-small
  batchtime: 720 # 12 hours
  expansions:
    ENABLE_TCMALLOC: 0
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: format-smoke-test
    - name: fops
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-arm64-large
    - name: long-test
      distros: ubuntu2004-arm64-large
    - name: configure-combinations
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

- name: ubuntu2004-release-stress-tests
  display_name: Ubuntu 20.04 Stress tests (Release Build)
  run_on:
  - ubuntu2004-test
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-disagg"
    - name: format-abort-recovery-stress-test
    - name: format-predictable-test

- name: ubuntu2004-release-stress-tests-arm64
  display_name: Ubuntu 20.04 Stress tests (ARM64, Release Build)
  run_on:
  - ubuntu2004-arm64-large
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-disagg"
    - name: format-abort-recovery-stress-test

- name: ubuntu2004-nonstandalone
  display_name: "Ubuntu 20.04 (Non-standalone)"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 1
    data_validation_stress_test_args: -t r -m -W 3 -D -p -n 100000 -k 100000
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: ".pull_request !.pull_request_compilers"
    - name: compile
    - name: make-check-test
    - name: unit-test-extra-long
      distros: ubuntu2004-large
    - name: ".data-validation-stress-test"

- name: ubuntu2004-stress-nonstandalone
  display_name: "Ubuntu 20.04 Stress tests (Non-standalone)"
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
      distros: ubuntu2004-medium
    - name: format-abort-recovery-stress-test
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours
      distros: ubuntu2004-medium # Medium is required as a larger disk is required.

- name: ubuntu2004-release-nonstandalone
  display_name: "Ubuntu 20.04 (Non-standalone, Release Build)"
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
    - name: ".stress-test-1"
      distros: ubuntu2004-test
    - name: ".stress-test-2"
      distros: ubuntu2004-test
    - name: format-abort-recovery-stress-test
      distros: ubuntu2004-test
    - name: ".stress-test-disagg"
    - name: make-check-test
    - name: unit-test

- name: ubuntu2004-arm64-nonstandalone
  display_name: Ubuntu 20.04 (ARM64, Non-standalone)
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
      distros: ubuntu2004-arm64-small
    - name: make-check-test
      distros: ubuntu2004-arm64-small
    - name: unit-test
      distros: ubuntu2004-arm64-small
    - name: unit-test-extra-long
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
    - name: format-abort-recovery-stress-test
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours

- name: ubuntu2004-arm64-release-nonstandalone
  display_name: Ubuntu 20.04 (ARM64, Release Build, Non-standalone)
  run_on:
  - ubuntu2004-arm64-small
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
    - name: ".stress-test-1"
      distros: ubuntu2004-arm64-large
    - name: ".stress-test-2"
      distros: ubuntu2004-arm64-large
    - name: format-abort-recovery-stress-test
      distros: ubuntu2004-arm64-large
    - name: ".stress-test-disagg"
    - name: make-check-test
    - name: unit-test
    - name: unit-test-extra-long
      distros: ubuntu2004-arm64-large

- name: amazon2-arm64-nonstandalone
  display_name: "Amazon Linux 2 ARM64 Non-standalone"
  run_on:
  - amazon2-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    cmake_generator: "Unix Makefiles"
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
    - name: make-check-test
      # Using large distro as more memory is required.
      distros: amazon2-arm64-large
    - name: unit-test
    - name: unit-test-extra-long
      distros: amazon2-arm64-large

- name: amazon2023-armv9-release-nonstandalone
  display_name: (ARMV9) Amazon 2023 (ARM64, Release Build, Non-standalone)
  run_on:
  - amazon2023-arm64-latest-small-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
    - name: ".stress-test-1"
      distros: amazon2023-arm64-latest-large-m8g
    - name: ".stress-test-2"
      distros: amazon2023-arm64-latest-large-m8g
    - name: format-abort-recovery-stress-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: ".stress-test-disagg"
    - name: make-check-test
    - name: unit-test
    - name: unit-test-extra-long
      distros: amazon2023-arm64-latest-large-m8g

- name: amazon2023-armv9-nonstandalone
  display_name: (ARMV9) Amazon Linux 2023 (ARM64, Non-standalone)
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
      distros: amazon2023-arm64-latest-small-m8g
    - name: make-check-test
      distros: amazon2023-arm64-latest-small-m8g
    - name: unit-test
      distros: amazon2023-arm64-latest-small-m8g
    - name: unit-test-extra-long
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
    - name: format-abort-recovery-stress-test
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours

- name: amazon2023-release-armv9
  display_name: "(ARMV9) Amazon 2023 (ARM64, Release Build)"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  batchtime: 720 # 12 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: format-smoke-test
    - name: fops
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: long-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

- name: amazon2023-armv9
  display_name: "(ARMV9) Amazon Linux 2023 ARM64"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_v5_gcc.cmake
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: ".workgen-test"
      batchtime: 1440 # 24 hours
    - name: checkpoint-filetypes-test
    - name: chunkcache-test
    - name: compile
    - name: fops
    - name: format-failure-configs-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: format-smoke-test
    - name: format-stress-pull-request-test
    - name: long-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: make-check-test
    - name: model-test-failure-workloads
      batchtime: 1440 # 24 hours
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-with-coverage
      batchtime: 1440 # once a day
    - name: model-test-long-random-config
      batchtime: 1440 # once a day
    - name: model-unit-test
    - name: s3-tiered-storage-extensions-test
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: unit-test
    - name: unit-test-extra-long
      distros: amazon2023-arm64-latest-large-m8g
    - name: unit-test-zstd
    - name: wtperf-test
    - name: checkpoint-filetypes-test
    - name: compile
    - name: fops
    - name: format-failure-configs-test
      distros: amazon2023-arm64-latest-large-m8g

- name: cppsuite-stress-tests-armv9
  display_name: "(ARMV9) Cppsuite Stress Tests ARM64"
  batchtime: 720 # twice a day
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".cppsuite-stress-test"

- name: amazon2023-stress-tests-armv9
  display_name: (ARMV9) Amazon2023 Stress tests ARM64
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
    - name: format-abort-recovery-stress-test
    - name: format-predictable-test
    - name: schema-abort-predictable-test
    - name: checkpoint-filetypes-predictable-test

- name: amazon2023-armv9-asan
  display_name: "(ARMV9) Amazon Linux 2023 ASAN"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export LSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/asan_leaks.supp"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    # Exclude wt12015_backup_corruption for ASan builds. It creates false positives when it deliberately kills processes that invoke corruption.
    ctest_extra_args: -E "wt12015_backup_corruption"
  tasks:
    - name: ".pull_request !.pull_request_compilers !.python !.tiered_unittest !csuite-wt12015-backup-corruption-test"
    - name: examples-c-test
    - name: format-asan-smoke-test
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-random-config
      batchtime: 1440 # once a day
    - name: live-restore-long
    - name: ".stress-test-asan"

- name: amazon2023-armv9-tsan
  display_name: "(ARMV9) Amazon Linux 2023 TSAN"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=TSan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export TSAN_OPTIONS="$COMMON_SAN_OPTIONS:history_size=7:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/tsan_warnings.supp"
    num_jobs: 2
  tasks:
    - name: compile
    - name: examples-c-test
    - name: unit-test-tsan
      distros: amazon2023-arm64-latest-large-m8g
    - name: generate-tsan-metric

- name: amazon2023-armv9-msan
  display_name: "(ARMV9) Amazon Linux 2023 MSAN"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=MSan
    # MemorySanitizer requires that all program code is instrumented, otherwise it may generate false reports.
    posix_configure_flags: -DENABLE_LZ4=0 -DENABLE_SNAPPY=0 -DENABLE_ZLIB=0 -DENABLE_ZSTD=0
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    # We don't run C++ memory sanitized testing as it creates false positives. MSAN also causes
    # some csuite tests to take an extended amount of time. These are excluded from the
    # make-check-test task and are covered by the csuite-long-running task.
    ctest_extra_args: -LE "cppsuite|long_running|disagg"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` / 2" | bc)
  tasks:
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-msan-test
    - name: make-check-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: csuite-long-running
      # Some of the long-running tests require a large instance to run successfully.
      distros: amazon2023-arm64-latest-large-m8g
      batchtime: 1440 # once a day

- name: amazon2023-armv9-ubsan
  display_name: "(ARMV9) Amazon Linux 2023 UBSAN"
  run_on:
  - amazon2023-arm64-latest-small-m8g
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=UBSan
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-stress-pull-request-test
    - name: make-check-test
      distros: amazon2023-arm64-latest-large-m8g
    - name: cppsuite-default-all

- name: amazon2023-stress-nonstandalone
  display_name: "(ARMV9) Amazon Linux 2023 Stress tests (Non-standalone)"
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
  tasks:
    - name: compile
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-disagg"
    - name: ".stress-test-no-barrier"
    - name: format-abort-recovery-stress-test
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours

- name: amazon2023-disagg-stress
  display_name: "[Failure Expected] (ARMV9) Amazon 2023 Disagg Stress tests"
  batchtime: 4320 # once every two days
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    disagg_run: true
  tasks:
    - name: compile
    - name: ".stress-test-disagg-fail"
    - name: data-validation-disagg-stress-test-checkpoint
      batchtime: 10080 # once every week
    - name: checkpoint-test-long-running
    - name: checkpoint-test
    - name: checkpoint-filetypes-predictable-test
    - name: checkpoint-filetypes-test
    - name: model-test-long
    - name: model-test-long-random-config

- name: amazon2023-disagg-asan-stress
  display_name: "[Failure Expected] (ARMV9) Amazon 2023 ASAN Disagg Stress tests"
  batchtime: 4320 # once every three days
  run_on:
  - amazon2023-arm64-latest-large-m8g
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export LSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/asan_leaks.supp"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    disagg_run: true
    <<: *configure_flags_asan_mongodb_stable_clang_with_builtins
    additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
  tasks:
    - name: compile
    - name: ".stress-test-disagg-asan-fail"
