#
# This file defines the tasks and platforms for WiredTiger in the
# MongoDB continuous integration system (https://evergreen.mongodb.com).
#

#######################################
#            Project Settings         #
#######################################

stepback: true
pre:
  - func: "cleanup"
  - func: "setup environment"
post:
  - func: "dump stacktraces"
  - func: "upload stacktraces"
  - func: "upload test/format config"
  - func: "upload test/model workloads"
  - func: "dump stderr/stdout"
  - func: "upload artifact"
    vars:
      postfix: -${execution}
  - func: "save wt hang analyzer core/debugger files"
  - func: "cleanup"
timeout:
  - func: "run wt hang analyzer"
exec_timeout_secs: 21600 # 6 hrs

#######################################
#            Functions                #
#######################################

functions:
  "setup environment":
    - command: expansions.update
      type: setup
      params:
        updates:
        # The expansion is used for each task that runs a WiredTiger test. The expansions are
        # created before each task and is meant to be used at the start each task. All of these
        # variables are common among the build variants, if there are any specific variables that
        # needs to be set, users can add onto the additional_env_vars in the variant.
        - key: PREPARE_TEST_ENV
          value: |
            export WT_TOPDIR=$(git rev-parse --show-toplevel)
            export WT_BUILDDIR=$WT_TOPDIR/cmake_build

            if [ "$OS" = "Windows_NT" ]; then
              export PATH=/cygdrive/c/python/Python311:/cygdrive/c/python/Python311/Scripts:$PATH
              export PYTHONPATH="($WT_TOPDIR/lang/python/wiredtiger):$(cygpath -w $WT_TOPDIR/lang/python)"
            else
              export PATH=/opt/mongodbtoolchain/v4/bin:$PATH
              export LD_LIBRARY_PATH=$WT_BUILDDIR
            fi

            # Create the common sanitizer options and export the specific sanitizer environment
            # variables.
            COMMON_SAN_OPTIONS="abort_on_error=1:disable_coredump=0"
            if [[ "${CMAKE_BUILD_TYPE|}" =~ ASan ]]; then
              export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
              export TESTUTIL_BYPASS_ASAN=1
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ MSan ]]; then
              export MSAN_OPTIONS="$COMMON_SAN_OPTIONS:verbosity=3"
              export TESTUTIL_MSAN=1
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ TSan ]]; then
              export TSAN_OPTIONS="$COMMON_SAN_OPTIONS:verbosity=3"
            elif [[ "${CMAKE_BUILD_TYPE|}" =~ UBSan ]]; then
              export UBSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_stacktrace=1"
              export TESTUTIL_UBSAN=1
            fi

            ${additional_env_vars}
        # The expansion is used for any task that requires the mongodbtoolchain binaries.
        - key: PREPARE_PATH
          value: |
            if [ "$OS" = "Windows_NT" ]; then
              export PATH=/cygdrive/c/python/Python311:/cygdrive/c/python/Python311/Scripts:$PATH
            else
              export PATH=/opt/mongodbtoolchain/v4/bin:$PATH
            fi
  # Since Bazel (currently used in SCons) uses EngFlow's remote execution system instead of icecream,
  # additional credentials need to be setup to maintain efficient compilation speed.
  "get engflow creds": &get_engflow_creds
  - command: s3.get
    display_name: "get engflow key"
    params:
      aws_key: ${engflow_key}
      aws_secret: ${engflow_secret}
      remote_file: engflow/engflow.key
      bucket: serverengflow
      local_file: "mongo/engflow.key"
  - command: s3.get
    display_name: "get engflow cert"
    params:
      aws_key: ${engflow_key}
      aws_secret: ${engflow_secret}
      remote_file: engflow/engflow.cert
      bucket: serverengflow
      local_file: "mongo/engflow.cert"
  - command: shell.exec
    params:
      display_name: "generate evergreen engflow bazelrc"
      shell: bash
      working_dir: mongo
      script: |
        set -o errexit
        set -o verbose

        # Pulled from evergreen/generate_evergreen_engflow_bazelrc.sh
        # FIXME-SERVER-86966: consider consolidating once prelude.sh is runnable in the perf project.

        source ./evergreen/bazel_RBE_supported.sh

        if bazel_rbe_supported; then

          uri="https://spruce.mongodb.com/task/${task_id}?execution=${execution}"

          echo "build --tls_client_certificate=./engflow.cert" > .bazelrc.evergreen_engflow_creds
          echo "build --tls_client_key=./engflow.key" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdPipelineName=${build_variant}" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdJobName=${task_name}" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=engflow:CiCdUri=$uri" >> .bazelrc.evergreen_engflow_creds
          echo "build --bes_keywords=evg:project=${project}" >> .bazelrc.evergreen_engflow_creds
          echo "build --workspace_status_command=./evergreen/engflow_workspace_status.sh" >> .bazelrc.evergreen_engflow_creds
        fi
  "get project":
    command: git.get_project
    type: setup
    params:
      directory: wiredtiger
  "generate github token": &gen_github_token
    command: github.generate_token
    params:
      owner: wiredtiger
      repo: automation-scripts
      expansion_name: generated_token
      permissions:
          metadata: read
          contents: read
  "get automation-scripts": &get_automation_scripts
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        if ! [ -d "./automation-scripts" ]; then
          git clone https://x-access-token:${generated_token}@github.com/wiredtiger/automation-scripts.git
        fi
  "fetch artifacts": &fetch_artifacts
    command: s3.get
    type: setup
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${dependent_task|compile}_${build_id}.tgz
      bucket: build_external
      extract_to: ${destination|wiredtiger}
  "fetch endian format artifacts" :
    - command: s3.get
      type: setup
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: wiredtiger/${endian_format}/${revision}/artifacts/${remote_file}.tgz
        bucket: build_external
        extract_to: wiredtiger/cmake_build/test/format
  "fetch mongo-tests repo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        git clone https://github.com/wiredtiger/mongo-tests
  "fetch mongo repo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        mongo_repo=https://github.com/mongodb/mongo
        branch=${branch_name}
        if [[ $branch =~ "mongodb-" ]]; then
          mongo_branch=v$(echo $branch | cut -d'-' -f 2)
        else
          mongo_branch=master
        fi
        git clone $mongo_repo -b $mongo_branch
  "import wiredtiger into mongo" :
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        cp -a wiredtiger mongo/src/third_party/
  "compile mongodb" :
    command: shell.exec
    params:
      shell: bash
      working_dir: "mongo"
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_PATH}
        virtualenv -p python3 venv
        source venv/bin/activate
        python3 -m pip install 'poetry==1.5.1'

        export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring
        # FIXME-WT-12911 - There's a dependency ordering issue in the mongo poetry.lock file. Running poetry install twice resolves it.
        set +o errexit
        python3 -m poetry install --no-root --sync
        set -o errexit
        python3 -m poetry install --no-root --sync

        ./buildscripts/scons.py --variables-files=etc/scons/mongodbtoolchain_stable_gcc.vars --link-model=dynamic --ninja generate-ninja ICECC=icecc CCACHE=ccache
        ninja -j$(nproc --all) install-mongod
  "configure wiredtiger": &configure_wiredtiger
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      include_expansions_in_env:
       - "s3_bucket_tcmalloc"
       - "s3_access_key"
       - "s3_secret_key"
      shell: bash
      script: |
        set -o errexit
        ${PREPARE_PATH}
        # Not setting verbose mode as we have sensitive keys that could be logged.

        # Define common config flags for the tasks to make it cleaner when configuring the tasks.
        # Note that the config flags are resolved prior to changing to cmake_build directory.
        DEFINED_EVERGREEN_CONFIG_FLAGS="${CMAKE_BUILD_TYPE|} \
          ${CMAKE_INSTALL_PREFIX|-DCMAKE_INSTALL_PREFIX=$(pwd)/cmake_build/LOCAL_INSTALL} \
          ${CMAKE_TOOLCHAIN_FILE|-DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_gcc.cmake} \
          ${ENABLE_LAZYFS|} \
          ${NONSTANDALONE|} \
          ${ENABLE_SHARED|} \
          ${ENABLE_STATIC|} \
          ${HAVE_BUILTIN_EXTENSION_LZ4|} \
          ${HAVE_BUILTIN_EXTENSION_SNAPPY|} \
          ${HAVE_BUILTIN_EXTENSION_ZLIB|} \
          ${HAVE_BUILTIN_EXTENSION_ZSTD|} \
          ${HAVE_UNITTEST} \
          ${CODE_COVERAGE_FLAGS} \
          ${NON_BARRIER_DIAGNOSTIC_YIELDS|} \
          ${HAVE_DIAGNOSTIC|} \
          ${GNU_C_VERSION|} \
          ${GNU_CXX_VERSION|} \
          ${CLANG_C_VERSION|} \
          ${CLANG_CXX_VERSION|} \
          ${ENABLE_AZURE|} \
          ${ENABLE_CPPSUITE|} \
          ${ENABLE_GCP|} \
          ${ENABLE_S3|} \
          ${IMPORT_AZURE_SDK|} \
          ${IMPORT_GCP_SDK|} \
          ${IMPORT_S3_SDK|} \
          ${SPINLOCK_TYPE|} \
          ${ENABLE_COLORIZE_OUTPUT|-DENABLE_COLORIZE_OUTPUT=0} \
          ${CC_OPTIMIZE_LEVEL|}"

        # The RHEL PPC platform does not have ZSTD. Strip it out.
        if [ "${build_variant|}" = "rhel8-ppc" ] && [[ "$DEFINED_EVERGREEN_CONFIG_FLAGS" =~ (\-DHAVE_BUILTIN_EXTENSION_ZSTD=1) ]]; then
          DEFINED_EVERGREEN_CONFIG_FLAGS=${DEFINED_EVERGREEN_CONFIG_FLAGS/\-DHAVE_BUILTIN_EXTENSION_ZSTD=1/}
        fi

        if [[ "${build_variant|}" =~ "macos-14" ]]; then
          # For mac builds, we want explicitly tell cmake which python to use, as
          # well as the matching library directory and header files. The find_libpython
          # module gives us the library.
          SYSPY=${python_binary}
          $SYSPY -mvenv venv
          source venv/bin/activate
          pip3 install find_libpython
          pip3 install swig==4.2.1
          SYSPYLIB=`find_libpython`
          SYSPYINCDEF=

          # We have the shared library to link to, it may be named simply 'Python3' or 'Python'.
          # If that's the case, use the associated dylib symlink found in an expected relative
          # location. Also get the location of the header files. We'll give this all to cmake.
          base=$(basename $SYSPYLIB)
          if [ "$base" = 'Python3' -o "$base" = 'Python' ]; then
             SYSPYDIR=$(dirname $SYSPYLIB)
             NSYSPYLIB=$(ls $SYSPYDIR/lib/libpython*.dylib 2>/dev/null | head -1)
             if [ -f "$NSYSPYLIB" ]; then
               SYSPYLIB=$NSYSPYLIB
             fi
             if [ -d "$SYSPYDIR/Headers" ]; then
               SYSPYINCDEF="$SYSPYDIR/Headers"
             fi
          fi

          if [ "${build_variant|}" = "macos-14-arm64" ]; then
            DEFINED_EVERGREEN_CONFIG_FLAGS="$DEFINED_EVERGREEN_CONFIG_FLAGS -DPython3_EXECUTABLE=$SYSPY -DPython3_LIBRARY=$SYSPYLIB -DPython3_INCLUDE_DIR=$SYSPYINCDEF"
          else
            DEFINED_EVERGREEN_CONFIG_FLAGS="$DEFINED_EVERGREEN_CONFIG_FLAGS -DPYTHON_EXECUTABLE:FILEPATH=$SYSPY -DPYTHON_LIBRARY=$SYSPYLIB -DPYTHON_INCLUDE_DIR=$SYSPYINCDEF"
          fi
        fi

        if [ "$OS" = "Windows_NT" ]; then
          # Use the Windows powershell script to configure the CMake build.
          # We execute it in a powershell environment as its easier to detect and source the Visual Studio
          # toolchain in a native Windows environment. We can't easily execute the build in a cygwin environment.
          echo "Using config flags $DEFINED_EVERGREEN_CONFIG_FLAGS ${windows_configure_flags|}"
          powershell.exe  -NonInteractive '.\test\evergreen\build_windows.ps1' -configure 1 $DEFINED_EVERGREEN_CONFIG_FLAGS ${windows_configure_flags|}
        else
          echo "Using config flags $DEFINED_EVERGREEN_CONFIG_FLAGS ${posix_configure_flags|}"

          if [[ ${ENABLE_TCMALLOC|0} -eq 1 ]]; then
            # Preclude use of tcmalloc with sanitizer builds.
            if [[ "$DEFINED_EVERGREEN_CONFIG_FLAGS" =~ (CMAKE_BUILD_TYPE=([AMT]|UB)San) ]]; then
              echo "tcmalloc incompatible with build variant: ${build_variant} and cmake build type ${CMAKE_BUILD_TYPE}"
              exit 1
            fi
            # Run this script in its own process as it depends on managing
            # shell options.
            /bin/bash test/evergreen/tcmalloc_install_or_build.sh ${build_variant}

            # Preload tcmalloc in the Evergreen environment: this will affect
            # ALL binaries.
            export LD_PRELOAD=$PWD/TCMALLOC_LIB/libtcmalloc.so
          fi

          # Compiling with CMake.
          echo "Find CMake"
          . test/evergreen/find_cmake.sh
          # If we've fetched the wiredtiger artifact from a previous compilation/build, it's best to remove
          # the previous build directory so we can create a fresh configuration. We can't use the the previous
          # CMake Cache configuration as its likely it will have absolute paths related to the previous build machine.
          echo "Remove the cmake_build directory, if it already exists"
          if [ -d cmake_build ]; then rm -r cmake_build; fi
          echo "Create a new cmake_build directory"
          mkdir -p cmake_build
          cd cmake_build

          echo "Call CMake"
          $CMAKE $DEFINED_EVERGREEN_CONFIG_FLAGS ${posix_configure_flags|} -G "${cmake_generator|Ninja}" ./..
          echo "Completed CMake"
        fi
  "python config check":
    command: shell.exec
    type: setup
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        # Confirm that the Python binary matches the version of that configured by CMake.
        ${python_binary|python3} ../test/evergreen/python_version_check.py -v -c ./CMakeCache.txt -s ${python_config_search_string}
  "make wiredtiger": &make_wiredtiger
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        echo "Starting 'make wiredtiger' step"
        ${PREPARE_PATH}
        if [ "$OS" = "Windows_NT" ]; then
          # Use the Windows powershell script to execute Ninja build (can't execute directly in a cygwin environment).
          powershell.exe '.\test\evergreen\build_windows.ps1 -build 1'
        else
          # Compiling with CMake generated Ninja file.
          cd cmake_build
          if [[ "${build_variant|}" =~ "macos-14-arm64" ]]; then
            source ../venv/bin/activate
          fi
          if [ "${cmake_generator|Ninja}" = "Ninja" ]; then
            ninja -j ${num_jobs} 2>&1
          else
            make -j ${num_jobs} 2>&1
          fi
        fi
        echo "Ending 'make wiredtiger' step"
  "compile wiredtiger":
    - *gen_github_token
    - *get_automation_scripts
    - *configure_wiredtiger
    - *make_wiredtiger
  "dump stacktraces": &dump_stacktraces
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${python_binary|python3} ../test/evergreen/print_stack_trace.py
  "upload stacktraces": &upload_stacktraces
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/*stacktrace.txt
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
  "upload test/format config":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/test/format/RUNDIR*/CONFIG
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
      preserve_path: true
  "upload test/model workloads":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter:
        - wiredtiger/cmake_build/test/model/tools/WT_TEST*/*.workload
      bucket: build_external
      permissions: public-read
      content_type: text/plain
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}/
      preserve_path: true
  "run data validation stress test checkpoint":
    - *fetch_artifacts
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/test/checkpoint"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${PREPARE_TEST_ENV}
          ../../../tools/run_parallel.sh 'nice ./recovery-test.sh "${data_validation_stress_test_args} ${run_test_checkpoint_args}" WT_TEST.$t test_checkpoint' 120
  "run tiered storage test":
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build"
        shell: bash
        include_expansions_in_env:
          - aws_sdk_s3_ext_access_key
          - aws_sdk_s3_ext_secret_key
        script: |
          set -o errexit
          ${PREPARE_TEST_ENV}

          # Set the Azure credentials using config variable.
          export AZURE_STORAGE_CONNECTION_STRING="${azure_sdk_ext_access_key}"

          # GCP requires a path to a credentials file for authorization. To not expose the private
          # information within the file, we use a placeholder private variable which are replaced
          # in the command line with the evergreen expansion variables and stored in a temporary
          # file.
          file=$(mktemp --suffix ".json")

          # Use '|' as the delimiter instead of default behaviour because the private key contains
          # slash characters.
          sed -e 's|gcp_project_id|${gcp_sdk_ext_project_id}|'                      \
              -e 's|gcp_private_key|'"${gcp_sdk_ext_private_key}"'|'                \
              -e 's|gcp_private_id|${gcp_sdk_ext_private_key_id}|'                  \
              -e 's|gcp_client_email|${gcp_sdk_ext_client_email}|'                  \
              -e 's|gcp_client_id|${gcp_sdk_ext_client_id}|'                        \
              -e 's|gcp_client_x509_cert_url|${gcp_sdk_ext_client_x509_cert_url}|'  ../test/evergreen/gcp_auth.json > $file
          export GOOGLE_APPLICATION_CREDENTIALS="$file"

          virtualenv -p python3 venv
          source venv/bin/activate
          pip3 install boto3
          pip3 install azure-storage-blob
          pip3 install google-cloud-storage

          # Run Python testing for all tiered tests.
          python3 ../test/suite/run.py -j $(nproc) ${tiered_storage_test_name}
  "compile wiredtiger docs":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          set -o verbose

          # Check if specific branches are provided to the function through the expansion variable
          # defined in the documentation-update build variant. If none are specified, use the
          # current branch.
          if [ -z ${doc_update_branches} ]; then
            branches=$(git rev-parse --abbrev-ref HEAD)
          else
            branches=${doc_update_branches}
          fi

          # Because of Evergreen's expansion syntax, this is used to process each branch separately.
          IFS=,
          for branch in $branches; do

            echo "Checking out branch $branch ..."
            git checkout $branch

            # Java API is removed in newer branches via WT-6675.
            if [ $branch == "mongodb-4.2" ]; then
              pushd build_posix
              bash reconf
              ../configure CFLAGS="-DMIGHT_NOT_RUN -Wno-error" --enable-java --enable-python --enable-strict
              (cd lang/python && make ../../../lang/python/wiredtiger_wrap.c)
              (cd lang/java && make ../../../lang/java/wiredtiger_wrap.c)
            elif [ $branch == "mongodb-5.0" ] || [ $branch == "mongodb-4.4" ]; then
              pushd build_posix
              bash reconf
              ../configure CFLAGS="-DMIGHT_NOT_RUN -Wno-error" --enable-python --enable-strict
              (cd lang/python && make ../../../lang/python/wiredtiger_wrap.c)
            else
              . test/evergreen/find_cmake.sh
              if [ -d cmake_build ]; then rm -r cmake_build; fi
              mkdir -p cmake_build
              pushd cmake_build
              # Adding -DENABLE_PYTHON=1 -DENABLE_STRICT=1 as 6.0 does not default these like develop.
              $CMAKE -DCMAKE_C_FLAGS="-DMIGHT_NOT_RUN -Wno-error" -DENABLE_PYTHON=1 -DENABLE_STRICT=1 ../.
              make -C lang/python -j ${num_jobs}
            fi
            # Pop to root project directory.
            popd
            # Generate WiredTiger documentation.
            (cd dist && bash s_docs && echo "The documentation for $branch was successfully generated.")
            # Save generated documentation
            mv docs docs-$branch
          done

          # Checkout the default ("develop") branch again to leave wiredtiger/ in the same state we started with
          git checkout develop

  "update wiredtiger docs":
    - command: shell.exec
      type: setup
      params:
        shell: bash
        script: |
          # Use a single function to update the documentation of each supported WiredTiger branch.
          # This is useful as not all branches have a dedicated Evergreen project. Furthermore, the
          # documentation-update task is not triggered by every commit. We rely on the activity of
          # the develop branch to update the documentation of all supported branches.
          set -o errexit
          set -o verbose

          if [[ "${branch_name}" != "develop" ]]; then
            echo "We only run the documentation update task on the WiredTiger (develop) Evergreen project."
            exit 0
          fi

          git clone https://github.com/wiredtiger/wiredtiger.github.com.git
          cd wiredtiger.github.com

          # Branches to update are defined through an expansion variable.
          branches=${doc_update_branches}

          # Go through each branch to stage the doc changes.
          IFS=,
          for branch in $branches; do

            # Synchronize the generated documentation with the current one.
            echo "Synchronizing documentation for branch $branch ..."
            rsync -aq ../wiredtiger/docs-$branch/ $branch/ --delete

            # Commit and push the changes if any.
            if [[ $(git status "$branch" --porcelain) ]]; then
              git add $branch
              git commit -m "Update auto-generated docs for $branch" \
                        --author="doc-build-bot <svc-wiredtiger-doc-build@10gen.com>"
            else
              echo "No documentation changes for $branch."
            fi

          done
    - command: shell.exec
      type: setup
      params:
        shell: bash
        silent: true
        script: |
          set -o errexit

          # We could have exited the previous command for the same reason.
          if [[ "${branch_name}" != "develop" ]]; then
            echo "We only run the documentation update task on the WiredTiger (develop) Evergreen project."
            exit 0
          fi

          # Push the above-generated commit
          ${PREPARE_PATH}
          virtualenv -p python3 venv
          source venv/bin/activate
          python -m pip install PyGithub
          export GITHUB_OWNER="wiredtiger"
          export GITHUB_REPO="wiredtiger.github.com"
          export GITHUB_APP_ID="${doc_update_github_app_id}"
          export GITHUB_APP_PRIVATE_KEY="${doc_update_github_app_private_key}"
          # Make sure the below script is called under the default ("develop") branch.
          (cd wiredtiger && git checkout develop)
          python wiredtiger/test/evergreen/doc_update.py

  "make check directory":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        . test/evergreen/find_cmake.sh
        cd cmake_build/${directory}
        $CTEST ${extra_args} --output-on-failure 2>&1

  "make check all":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        . test/evergreen/find_cmake.sh
        cd cmake_build
        echo "Using number of parallel processes '-j ${num_jobs}' for 'make check all'"
        $CTEST -L check -j ${num_jobs} --output-on-failure ${extra_args|} 2>&1

  # The following cppsuite tasks define a greater overall task.
  "cppsuite test run": &cppsuite_test_run
    command: shell.exec
    params:
      # The tests need to be executed in the cppsuite directory as some required libraries have
      # their paths defined relative to this directory.
      # The below script saves the exit code from the test to use it later in this function. By
      # doing this we can define our own custom artifact upload task without it being cancelled by
      # the test failing.
      # Additionally if the test fails perf statistics won't be uploaded as they may be invalid
      # due to the test failure.
      working_dir: "wiredtiger/cmake_build/test/cppsuite"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./run -t ${test_name} -C '${test_config}' -f ${test_config_filename} -l 2
        exit_code=$?
        echo "$exit_code" > cppsuite_exit_code
        if [ "$exit_code" != 0 ]; then
          echo "[{\"info\":{\"test_name\": \"${test_name}\"},\"metrics\": []}]" > ${test_name}.json
        fi
        exit 0

  # The following cppsuite tasks define a greater overall task.
  "cppsuite test run all": &cppsuite_test_run_all
    command: shell.exec
    params:
      # The tests need to be executed in the cppsuite directory as some required libraries have
      # their paths defined relative to this directory.
      working_dir: "wiredtiger/cmake_build/test/cppsuite"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./run -C '${test_config}' -l 2

  # Delete unnecessary data from the upload.
  "cppsuite test remove files": &cppsuite_remove_files
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        rm -rf wiredtiger/cmake_build/examples
        rm -rf wiredtiger/cmake_build/bench
        mv wiredtiger/cmake_build/test/cppsuite wiredtiger/cmake_build/
        rm -rf wiredtiger/cmake_build/test/
        mkdir wiredtiger/cmake_build/test/
        mv wiredtiger/cmake_build/cppsuite wiredtiger/cmake_build/test/cppsuite

  # Custom cppsuite archive tasks.
  "cppsuite archive": &cppsuite_archive
    command: archive.targz_pack
    type: setup
    params:
      target: archive.tgz
      source_dir: wiredtiger/cmake_build/
      include:
        - "./**"

  # Custom cppsuite s3 artifact upload task.
  "cppsuite s3 put": &cppsuite_s3_put
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_file: archive.tgz
      bucket: build_external
      permissions: public-read
      content_type: application/tar
      display_name: cppsuite-test
      remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}${postfix|}.tgz

  # FIXME-WT-8538 This task prevents us from saving the same artifacts to evergreen twice. It can be
  # removed when we implement a generalised approach in WT-8538
  "cppsuite remove dir": &cppsuite_remove_dir
    command: shell.exec
    params:
      shell: bash
      script: |
        set -o verbose
        if [ -f wiredtiger/cmake_build/test/cppsuite/cppsuite_exit_code ]; then
          exit_code=`cat wiredtiger/cmake_build/test/cppsuite/cppsuite_exit_code`
        else
          exit_code=0
        fi
        rm -rf wiredtiger
        exit "$exit_code"

  # The cppsuite test per task function. Doesn't upload perf statistics to evergreen.
  "cppsuite test":
    - *cppsuite_test_run
    # Since we later remove the WiredTiger folder, we need to check for core dumps now.
    - *dump_stacktraces
    - *upload_stacktraces
    # Cleanup tasks.
    - *cppsuite_remove_files
    - *cppsuite_archive
    - *cppsuite_s3_put
    - *cppsuite_remove_dir

  # This cppsuite test function uploads perf statistics and should only be used on perf variants.
  "cppsuite perf test":
    - *cppsuite_test_run
    # Since we later remove the WiredTiger folder, we need to check for core dumps now.
    - *dump_stacktraces
    - *upload_stacktraces
    - command: perf.send
      type: setup
      params:
        file: ./wiredtiger/cmake_build/test/cppsuite/${test_name}.json
    # Cleanup tasks.
    - *cppsuite_remove_files
    - *cppsuite_archive
    - *cppsuite_s3_put
    - *cppsuite_remove_dir

  "wt2853_perf test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/bench/wt2853_perf"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./test_wt2853_perf ${wt2853_perf_args}

  "csuite test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        $(pwd)/test/csuite/${test_name}/test_${test_name} ${test_args|} 2>&1

  "unit test":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        cd cmake_build
        threads_command=""
        if [[ -n "${num_jobs}" ]]; then
          echo "Using num_jobs '-j ${num_jobs}' for 'unit test'"
          threads_command="-j ${num_jobs}"
        fi
        if [ ${check_coverage|false} = true ]; then
            ${python_binary|python3} ../test/suite/run.py ${unit_test_args|-v 2} ${unit_test_variant_args} $threads_command 2>&1 || echo "Ignoring failed test as we are checking test coverage"
        else
            ${python_binary|python3} ../test/suite/run.py ${unit_test_args|-v 2} ${unit_test_variant_args} $threads_command 2>&1
        fi

  "code coverage analysis":
    command: shell.exec
    params:
      working_dir: ${working_dir}
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        echo "Performing code coverage analysis in ${working_dir}"
        ${PREPARE_PATH}
        virtualenv -p python3 venv
        source venv/bin/activate
        pip3 install lxml==4.8.0 Pygments==2.11.2 Jinja2==3.0.3 gcovr==5.0
        mkdir -p coverage_report
        GCOV=/opt/mongodbtoolchain/v4/bin/gcov gcovr -f ${coverage_filter|src} -j 4 --html-self-contained --html-details coverage_report/2_coverage_report.html --json-summary-pretty --json-summary coverage_report/1_coverage_report_summary.json --json coverage_report/full_coverage_report.json
        ${python_binary|python3} test/evergreen/code_coverage_analysis.py -s coverage_report/1_coverage_report_summary.json -t time.txt

        # Generate Atlas compatible format report.
        if [ ! -z ${generate_atlas_format} ]; then
            ${python_binary|python3} test/evergreen/code_coverage_analysis.py -c component_coverage -o coverage_report/atlas_out_code_coverage.json -s coverage_report/1_coverage_report_summary.json -t time.txt
        fi

  "code coverage publish report":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_files_include_filter: wiredtiger/coverage_report/*
      bucket: build_external
      permissions: public-read
      content_type: text/html
      remote_file: wiredtiger/${build_variant}/${revision}/${task_name}_${build_id}-${execution}/

  "code coverage publish main page":
    command: s3.put
    type: setup
    params:
      aws_secret: ${aws_secret}
      aws_key: ${aws_key}
      local_file: wiredtiger/coverage_report/2_coverage_report.html
      bucket: build_external
      permissions: public-read
      content_type: text/html
      display_name: "Coverage report main page"
      remote_file: wiredtiger/${build_variant}/${revision}/${task_name}_${build_id}-${execution}/1_coverage_report_main.html

  "format test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # Fail, show the configuration file.
        fail() {
          echo "======= FAILURE =========="
          [ -f RUNDIR/CONFIG ] && cat RUNDIR/CONFIG
          exit 1
        }

        for i in $(seq ${times|1}); do
          ./t -c ${config|../../../test/format/CONFIG.stress} ${trace_args|-T bulk,txn,retain=50} ${extra_args|} || fail
        done
  "format test predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        # To test predictable replay, we run test/format three times with the same data seed
        # each time, and compare the keys and values found in the WT home directories.
        # The first run is a timed one. When it's completed, we get the run's stable timestamp,
        # and do the subsequent runs up to that stable timestamp.  This, along with predictable
        # replay using the same data seed, should guarantee we have equivalent data created.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # Get a random value with leading zeroes removed, /bin/sh version.
        rando() {
          tr -cd 0-9 </dev/urandom | head -c 5 | sed -e 's/0*\(.\)/\1/'
        }
        # Fail, showing the configuration file.
        fail() {
          echo "======= FAILURE =========="
          for file; do
            if [ -f "$file" ]; then
              echo Contents of "$file":
              cat "$file"
              echo "================"
            fi
          done
          exit 1
        }
        runtime=3  # minutes
        config=../../../test/format/CONFIG.replay
        for i in $(seq ${times}); do
          echo Iteration $i/${times}
          x2=$(rando)
          x3=$(rando)
          rm -rf RUNDIR_1 RUNDIR_2 RUNDIR_3

          first_run_args="-c $config runs.timer=$runtime"
          ./t -h RUNDIR_1 $first_run_args ${extra_args} || fail RUNDIR_1/CONFIG 2>&1
          stable_hex=$(../../../tools/wt_timestamps RUNDIR_1 | sed -e '/stable=/!d' -e 's/.*=//')
          ops=$(echo $((0x$stable_hex)))

          # Do the second run up to the stable timestamp, using the same data seed,
          # but with a different extra seed.  Compare it when done.
          common_args="-c RUNDIR_1/CONFIG runs.timer=0 runs.ops=$ops"
          ./t -h RUNDIR_2 $common_args random.extra_seed=$x2 || fail RUNDIR_2/CONFIG 2>&1
          ../../../tools/wt_cmp_dir RUNDIR_1 RUNDIR_2 || fail RUNDIR_1/CONFIG RUNDIR_2/CONFIG 2>&1

          # Do the third run up to the stable timestamp, using the same data seed,
          # but with a different extra seed.  Compare it to the second run when done.
          ./t -h RUNDIR_3 $common_args random.extra_seed=$x3 || fail RUNDIR_3/CONFIG 2>&1
          ../../../tools/wt_cmp_dir RUNDIR_2 RUNDIR_3 || fail RUNDIR_2/CONFIG RUNDIR_3/CONFIG 2>&1
        done
  "format test script":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      add_expansions_to_env: true
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ${additional_san_vars}
        ${format_test_setting}
        for i in $(seq ${times|1}); do
          ./format.sh -j ${num_jobs} ${format_test_script_args|} 2>&1
        done
  "format test tiered":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/format"
      shell: bash
      script: |
        # To make sure we have plenty of flush_tier calls, we set the flush frequency high
        # and the time between checkpoints low. We specify only using tables, as that's the
        # only kind of URI that participates in tiered storage.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        format_args="tiered_storage.storage_source=dir_store tiered_storage.flush_frequency=60 checkpoint.wait=15 runs.source=table runs.timer=10 runs.in_memory=0"
        for i in $(seq ${times}); do
          echo Iteration $i/${times}
          rm -rf RUNDIR
          ./t $format_args ${extra_args}
          ./t -R $format_args ${extra_args}
        done
  "many dbs test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/manydbs"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./test_manydbs ${many_db_args|} 2>&1
  "thread test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/thread"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./t ${thread_test_args|} 2>&1
  "recovery stress test script":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/csuite"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}

        for i in $(seq ${times|1}); do
          # Run the various combinations of args. Let time and threads be random. Add a
          # timing stress to test_timestamp_abort every other run.
          if [ $(( $i % 2 )) -eq 0 ]; then
            test_timestamp_abort_args=-s
          else
            test_timestamp_abort_args=
          fi

          # Run current version with write-no-sync txns.
          ./random_abort/test_random_abort 2>&1
          ./timestamp_abort/test_timestamp_abort $test_timestamp_abort_args 2>&1

          # Current version with memory-based txns (MongoDB usage).
          ./random_abort/test_random_abort -m 2>&1
          ./timestamp_abort/test_timestamp_abort -m $test_timestamp_abort_args 2>&1

          # V1 log compatibility mode with write-no-sync txns.
          ./random_abort/test_random_abort -C 2>&1
          ./timestamp_abort/test_timestamp_abort -C $test_timestamp_abort_args 2>&1

          # V1 log compatibility mode with memory-based txns.
          ./random_abort/test_random_abort -C -m 2>&1
          ./timestamp_abort/test_timestamp_abort -C -m $test_timestamp_abort_args 2>&1

          ./truncated_log/test_truncated_log ${truncated_log_args|} 2>&1

          # Just let the system take a breath
          sleep 10s
        done
  "schema abort predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/csuite/schema_abort"
      shell: bash
      script: |
        # Get a random value with leading zeroes removed, /bin/sh version.
        rando() {
          tr -cd 0-9 </dev/urandom | head -c 5 | sed -e 's/0*\(.\)/\1/'
        }

        # Run schema_abort in a way that can test predictable replay.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        runtime=20  # seconds
        nthreads=5

        toolsdir=../../../../tools
        wtutil=../../../wt

        r=$(rando)$(rando)
        x0=$(rando)$(rando)

        rm -rf RUNDIR_0
        # The first run is for calibration only.  We just want to run for the designated
        # time and get an appropriate stop timestamp that can be used in later runs.
        calibration_run_args="-PSD$r,E$x0 -T $nthreads -t $runtime"
        ./test_schema_abort -p -h RUNDIR_0 $calibration_run_args || exit 1
        echo "Finished calibration run"
        stable_hex=$($toolsdir/wt_timestamps RUNDIR_0/WT_HOME | sed -e '/stable=/!d' -e 's/.*=//')
        op_count=$(echo $((0x$stable_hex)))

        for i in $(seq ${times}); do
          echo Iteration $i/${times}
          x1=$(rando)$(rando)
          x2=$(rando)$(rando)
          rm -rf RUNDIR_1 RUNDIR_2

          # Run with up to a slightly different timestamp for each iteration.
          ops=$(($op_count + $(rando) % 100))

          # Do two runs up to the stable timestamp, using the same data seed,
          # but with a different extra seed.  Compare it when done.
          first_run_args="-PSD$r,E$x1 -T $nthreads -s $ops"
          echo "First run with args $first_run_args"
          ./test_schema_abort -p -h RUNDIR_1 $first_run_args  || exit 1

          second_run_args="-PSD$r,E$x2 -T $nthreads -s $ops"
          echo "Second run with args $second_run_args"
          ./test_schema_abort -p -h RUNDIR_2 $second_run_args  || exit 1

          # We are ignoring the table:wt table. This table does not participate in
          # predictable replay, as it may be concurrently created, opened (regular or bulk cursor),
          # verified, upgraded and dropped by multiple threads in test_schema_abort.
          $toolsdir/wt_cmp_dir -i '^table:wt$' RUNDIR_1/WT_HOME RUNDIR_2/WT_HOME || exit 1
        done
  "upload artifact":
    - command: archive.targz_pack
      type: setup
      params:
        target: ${upload_filename|wiredtiger.tgz}
        source_dir: ${upload_source_dir|wiredtiger}
        include:
          - "./**"
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${upload_filename|wiredtiger.tgz}
        bucket: build_external
        permissions: public-read
        content_type: application/tar
        display_name: ${artifacts_name|Artifacts}
        remote_file: wiredtiger/${build_variant}/${revision}/artifacts/${task_name}_${build_id}${postfix|}.tgz
  "upload endian format artifacts":
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${local_file}
        bucket: build_external
        permissions: public-read
        content_type: application/tar
        display_name: WT_TEST
        remote_file: wiredtiger/${endian_format}/${revision}/artifacts/${remote_file}
  "cleanup":
    command: shell.exec
    type: setup
    params:
      shell: bash
      script: |
        rm -rf "wiredtiger"
        rm -rf "wiredtiger.tgz"

  "run wt hang analyzer":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o verbose
        ${PREPARE_PATH}

        # Dump core (-c) and debugger outputs (-o)
        wt_hang_analyzer_option="-c -o file -o stdout"

        echo "Calling the wt hang analyzer ..."
        ${python_binary|python3} ../test/wt_hang_analyzer/wt_hang_analyzer.py $wt_hang_analyzer_option

  "save wt hang analyzer core/debugger files":
    - command: archive.targz_pack
      type: setup
      params:
        target: "wt-hang-analyzer.tgz"
        source_dir: "wiredtiger/cmake_build"
        include:
          - "./*core*"
          - "./debugger*.*"
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: wt-hang-analyzer.tgz
        bucket: build_external
        optional: true
        permissions: public-read
        content_type: application/tar
        display_name: WT Hang Analyzer Output - Execution ${execution}
        remote_file: wiredtiger/${build_variant}/${revision}/wt_hang_analyzer/wt-hang-analyzer_${task_name}_${build_id}${postfix|}.tgz

  "dump stderr/stdout":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build"
      shell: bash
      script: |
        set -o errexit
        set -o verbose

        if [ -d "WT_TEST" ]; then
          # Dump stderr/stdout contents generated by the C libraries onto console for Python tests
          find "WT_TEST" -name "std*.txt" ! -empty -exec bash -c "echo 'Contents from {}:'; cat '{}'" \;
        fi

  "checkpoint test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        ./test_checkpoint ${checkpoint_args} 2>&1

  "checkpoint test predictable":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        # Get a random value with leading zeroes removed, /bin/sh version.
        rando() {
          tr -cd 0-9 </dev/urandom | head -c 5 | sed -e 's/0*\(.\)/\1/'
        }

        # Run test/checkpoint in a way that can test predictable replay.
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}

        toolsdir=../../../tools
        wtutil=../../wt

        r=$(rando)$(rando)
        x0=$(rando)$(rando)

        # Always run with timestamps and in the predictable mode
        base_args="-x -R"

        rm -rf RUNDIR_0
        # The first run is for calibration only.  We just want to run for the designated
        # time and get an approriate stop timestamp that can be used in later runs.
        calibration_run_args="-PSD$r,E$x0"
        ./test_checkpoint -h RUNDIR_0 $base_args ${checkpoint_args} $calibration_run_args || exit 1
        echo "Finished calibration run"
        stable_hex=$($toolsdir/wt_timestamps RUNDIR_0 | sed -e '/stable=/!d' -e 's/.*=//')
        stop_ts=$(echo $((0x$stable_hex)))
        for i in $(seq ${times}); do
          echo Iteration $i/${times}
          x1=$(rando)$(rando)
          x2=$(rando)$(rando)
          rm -rf RUNDIR_1 RUNDIR_2
          # Do two runs up to the stable timestamp, using the same data seed,
          # but with a different extra seed.  Compare it when done.
          first_run_args="-PSD$r,E$x1 -S $stop_ts"
          echo "First run with args $base_args ${checkpoint_args} $first_run_args"
          ./test_checkpoint -h RUNDIR_1 $base_args ${checkpoint_args} $first_run_args || exit 1
          second_run_args="-PSD$r,E$x2 -S $stop_ts"
          echo "Second run with args $base_args ${checkpoint_args} $second_run_args"
          ./test_checkpoint -h RUNDIR_2 $base_args ${checkpoint_args} $second_run_args || exit 1
          # Compare the runs.
          $toolsdir/wt_cmp_dir RUNDIR_1 RUNDIR_2 || exit 1
        done

  "checkpoint stress test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/cmake_build/test/checkpoint"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        export WIREDTIGER_CONFIG='checkpoint_sync=0,transaction_sync=(method=none)'
        CMD='./test_checkpoint -h WT_TEST.$i.$t -t r -r 2 -W 3 -n 1000000 -k 1000000 -C "cache_size=100MB"'
        if [ ${tiered|0} -eq 1 ]; then
            CMD="$CMD -PT"
        fi

        for i in $(seq ${times|1}); do
          for t in $(seq ${no_of_procs|1}); do
            eval nohup $CMD > nohup.out.$i.$t 2>&1 &
          done

          failure=0
          for t in $(seq ${no_of_procs|1}); do
            ret=0
            wait -n || ret=$?
            if [ $ret -ne 0 ]; then
              # Skip the below lines from nohup output file because they are very verbose and
              # print only the errors to evergreen log file.
              grep -v "Finished verifying" nohup.out.* | grep -v "Finished a checkpoint" | grep -v "thread starting"
              failure=1
              fail_ret=$ret
            fi
          done
          if [ $failure -eq 1 ]; then
            exit $fail_ret
          fi
        done

  "compatibility test":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        script: |
          set -o errexit
          set -o verbose
          test/compatibility/compatibility_test_for_releases.sh ${compat_test_args}

  "run-perf-test":
    # Run a performance test
    # Parameterised using the 'perf-test-name' and 'maxruns' variables
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/bench/wtperf"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${PREPARE_TEST_ENV}

          if [ ${no_create|false} = false ]; then
            rm -rf WT_TEST*
          fi
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          pip3 install psutil==5.9.4
          ${python_binary|python3} ../../../bench/perf_run_py/perf_run.py --${test_type|wtperf} -e ${exec_path|./wtperf} -t ${perf-test-path|../../../bench/wtperf/runners}/${perf-test-name} -ho WT_TEST -m ${maxruns} -v -b -o test_stats/evergreen_out_${perf-test-name}.json ${wtarg}
          ${python_binary|python3} ../../../bench/perf_run_py/perf_run.py --${test_type|wtperf} -e ${exec_path|./wtperf} -t ${perf-test-path|../../../bench/wtperf/runners}/${perf-test-name} -ho WT_TEST -m ${maxruns} -v -re -o test_stats/atlas_out_${perf-test-name}.json ${wtarg}

  "csuite smoke test":
    command: shell.exec
    params:
      working_dir: "wiredtiger"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        test/csuite/${test_binary}/smoke.sh ${test_args|} 2>&1

  "upload test stats":
    - command: perf.send
      type: setup
      params:
        file: ./wiredtiger/cmake_build/${test_path}.json

  "convert-to-atlas-evergreen-format":
    - command: shell.exec
      params:
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ${python_binary|python3} wiredtiger/bench/perf_run_py/perf_json_converter_for_atlas_evergreen.py -i ${input_file} -n ${test_name} -o ${output_path}

  "upload stats to atlas":
    - *gen_github_token
    - *get_automation_scripts
    - command: shell.exec
      params:
        shell: bash
        silent: true
        script: |
          set -o errexit
          ${PREPARE_PATH}
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          pip3 install pymongo[srv]==3.12.2 pygit2==1.10.1
          EVERGREEN_TASK_INFO='{ "evergreen_task_info": { "is_patch": "'${is_patch}'", "task_id": "'${task_id}'", "distro_id": "'${distro_id}'", "execution": "'${execution}'", "task_name": "'${task_name}'", "version_id": "'${version_id}'", "branch_name": "'${branch_name}'" } }'
          echo "EVERGREEN_TASK_INFO: $EVERGREEN_TASK_INFO"
          ${python_binary|python3} automation-scripts/evergreen/upload_stats_atlas.py -u ${atlas_perf_test_username} -p ${atlas_perf_test_password} -c ${collection|} -d ${database|} -f ${stats_dir|./wiredtiger/cmake_build/bench/wtperf/test_stats}/atlas_out_${test-name}.json -t ${created_at} -i "$EVERGREEN_TASK_INFO" -g "./wiredtiger"

  "upload stats to evergreen":
    - command: perf.send
      type: setup
      params:
        file: ${stats_dir|./wiredtiger/cmake_build/bench/wtperf/test_stats}/evergreen_out_${test-name}.json
    # Push the json results to the 'Files' tab of the task in Evergreen
    # Parameterised using the 'test-name' variable
    - command: s3.put
      type: setup
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        local_file: ${stats_dir|wiredtiger/cmake_build/bench/wtperf/test_stats}/atlas_out_${test-name}.json
        bucket: build_external
        permissions: public-read
        content_type: text/html
        remote_file: wiredtiger/${build_variant}/${revision}/${task_name}-${build_id}-${execution}/

  "validate-expected-stats":
    - command: shell.exec
      params:
        working_dir: "wiredtiger/cmake_build/bench/wtperf"
        shell: bash
        script: |
          set -o errexit
          ${PREPARE_PATH}
          virtualenv -p ${python_binary|python3} venv
          source venv/bin/activate
          ${python_binary|python3} ../../../bench/perf_run_py/validate_expected_stats.py '${stat_file}' ${comparison_op} '${expected-stats}'

  "verify wt datafiles":
    - command: shell.exec
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          set -o verbose
          ./test/evergreen/verify_wt_datafiles.sh 2>&1

  "install gcp dependencies":
    - command: shell.exec
      type: setup
      params:
        working_dir: "wiredtiger"
        shell: bash
        script: |
          set -o errexit
          . test/evergreen/find_cmake.sh
          . test/evergreen/install_gcp_dependencies.sh $CMAKE

  "split stress test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/bench/workgen/runner"
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        for i in $(seq ${times|15}); do
            ${python_binary|python3} split_stress.py
        done

  "build and push antithesis container":
    command: subprocess.exec
    type: setup
    params:
      working_dir: wiredtiger/tools/antithesis
      binary: bash
      add_expansions_to_env: true
      args:
      - "./build_and_push_containers.sh"

  "run workgen test":
    command: shell.exec
    params:
      working_dir: "wiredtiger/bench/workgen/runner"
      include_expansions_in_env:
        - task_name
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${PREPARE_TEST_ENV}
        # The task name gives us the test name to run (workgen-test-<filename>)
        FILENAME=`echo ${task_name}| cut -d'-' -f 3-`
        echo "Running $FILENAME.py"
        ${python_binary|python3} $FILENAME.py

#######################################
#               Variables             #
#######################################

variables:
  # Configure flags for builtins.
  - &configure_flags_with_builtins
    HAVE_BUILTIN_EXTENSION_LZ4: -DHAVE_BUILTIN_EXTENSION_LZ4=1
    HAVE_BUILTIN_EXTENSION_SNAPPY: -DHAVE_BUILTIN_EXTENSION_SNAPPY=1
    HAVE_BUILTIN_EXTENSION_ZLIB: -DHAVE_BUILTIN_EXTENSION_ZLIB=1
    HAVE_BUILTIN_EXTENSION_ZSTD: -DHAVE_BUILTIN_EXTENSION_ZSTD=1

  # Configure flags static library (default in cmake is dynamic).
  - &configure_flags_static_lib
    ENABLE_SHARED: -DENABLE_SHARED=0
    ENABLE_STATIC: -DENABLE_STATIC=1

  # Configure flags static library (default in cmake is dynamic) with builtins.
  - &configure_flags_static_lib_with_builtins
    <<: *configure_flags_with_builtins
    ENABLE_SHARED: -DENABLE_SHARED=0
    ENABLE_STATIC: -DENABLE_STATIC=1

  # Configure flags for tiered storage Azure extension.
  - &configure_flags_tiered_storage_azure
    ENABLE_AZURE: -DENABLE_AZURE=1
    IMPORT_AZURE_SDK: -DIMPORT_AZURE_SDK=external

  # Configure flags for tiered storage GCP extension.
  - &configure_flags_tiered_storage_gcp
    ENABLE_GCP: -DENABLE_GCP=1
    IMPORT_GCP_SDK: -DIMPORT_GCP_SDK=external

  # Configure flags for tiered storage S3 extension.
  - &configure_flags_tiered_storage_s3
    ENABLE_S3: -DENABLE_S3=1
    IMPORT_S3_SDK: -DIMPORT_S3_SDK=external

  # Configure flags for address sanitizer for stable mongodb toolchain clang (include builtins).
  - &configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
    <<: *configure_flags_with_builtins
    ENABLE_TCMALLOC: 0
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0

  # Template for Mac tests
  - &mac_test_template
    expansions: &mac_test_template_expansions
      # The cmake toolchain file is set to the mongodb toolchain gcc by default.
      # Remove that configuration here and let MacOS use the default Xcode toolchain instead.
      # We'll explicitly use the python3 in /usr/bin, we use the same in configuring cmake.
      CMAKE_TOOLCHAIN_FILE:
      CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      num_jobs: $(echo $(sysctl -n hw.logicalcpu) / 2 | bc)
      cmake_generator: "Unix Makefiles"
      additional_env_vars: |
        export LD_LIBRARY_PATH=""
        export DYLD_LIBRARY_PATH=$WT_BUILDDIR
      # Must disable TCMALLOC as it may be picked up locally and its not on all hosts.
      ENABLE_TCMALLOC: 0
    tasks:
      - name: compile
      - name: make-check-test
        # Use a special version of unit-test for macOS that checks for Python version consistency.
      - name: unit-test-macos
      - name: fops
      - name: memory-model-test-mac
        batchtime: 40320 # 28 days

#########################################################################################
# The following stress tests are configured to run for six hours via the "-t 360"
# argument to format.sh: format-stress-test, format-stress-sanitizer-test, and
# race-condition-stress-sanitizer-test. The recovery tests run in a loop, with
# the number of runs adjusted to provide aproximately six hours of testing.
#########################################################################################

  - &format-stress-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 360

  - &format-stress-sanitizer-ppc-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          # Always disable mmap for PPC due to issues on variant setup.
          # See https://bugzilla.redhat.com/show_bug.cgi?id=1686261#c10 for the potential cause.
          format_test_script_args: -t 360 -- -C "mmap=false,mmap_all=false"

  - &format-stress-sanitizer-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
      - func: "format test script"
        vars:
          format_test_script_args: -t 360

  - &race-condition-stress-sanitizer-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -R -t 360

  - &recovery-stress-test
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      - func: "recovery stress test script"
        vars:
          times: 25

  - &workgen-test
    tags: ["workgen-test"]
    exec_timeout_secs: 21600
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "run workgen test"

#######################################
#               Tasks                 #
#######################################

tasks:
  # Check the python configuration
  # Base compile task on posix flavours
  - name: compile
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "upload artifact"
      - func: "cleanup"

  # production build with --disable-shared
  - name: compile-production-disable-shared
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib
      - func: "upload artifact"
      - func: "cleanup"

  # production build with --disable-static
  - name: compile-production-disable-static
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "upload artifact"
      - func: "cleanup"

  - name: compile-gcc
    tags: ["pull_request", "pull_request_compilers"]
    commands:
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Release
      - func: "compile wiredtiger"
        vars:
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Debug
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=7
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=8
          GNU_CXX_VERSION: -DGNU_CXX_VERSION=8
      - func: "compile wiredtiger"
        vars:
          GNU_C_VERSION: -DGNU_C_VERSION=9
          GNU_CXX_VERSION: -DGNU_CXX_VERSION=9
      # Check that WiredTiger will compile for code coverage, as this may switch to alternative
      # implementations of some code which may generate different compiler errors to a 'normal' build
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Coverage
          CODE_COVERAGE_FLAGS: -DCODE_COVERAGE_MEASUREMENT=1 -DINLINE_FUNCTIONS_INSTEAD_OF_MACROS=1

  - name: compile-clang
    tags: ["pull_request", "pull_request_compilers"]
    commands:
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Release
      - func: "compile wiredtiger"
        vars:
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Debug
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=7
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=7
      - func: "compile wiredtiger"
        vars:
          CLANG_C_VERSION: -DCLANG_C_VERSION=8
          CLANG_CXX_VERSION: -DCLANG_CXX_VERSION=8

  - name: make-check-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check all"

  # Start of normal make check test tasks

  - name: lang-python-test
    tags: ["pull_request", "python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: lang/python

  - name: examples-c-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: examples-c-tsan
    # There's a lot of TSan warnings to resolve and TSan is a runtime tool so they aren't always reliably detected.
    # For now only run TSan tasks in patches until we're confident the tests are stable.
    # tags: ["pull_request"]
    patch_only: true
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: examples/c
          extra_args: -E "(ex_all|ex_backup|ex_backup_block|ex_file_system)" -j ${num_jobs}

  - name: examples-c-production-disable-shared-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: examples-c-production-disable-static-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "make check directory"
        vars:
          directory: examples/c

  - name: bloom-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/bloom

  - name: checkpoint-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/checkpoint

  - name: cursor-order-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/cursor_order

  - name: fops-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/fops

  - name: format-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/format

  - name: huge-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/huge

  - name: manydbs-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/manydbs

  - name: packing-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/packing

  - name: readonly-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/readonly

  - name: salvage-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/salvage

  - name: thread-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/thread

  - name: bench-wtperf-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: bench/wtperf

  - name: catch2-unittest-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/unittest/catch2-unittests

  - name: catch2-unittest-assertions
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          HAVE_UNITTEST: -DHAVE_UNITTEST=1 -DHAVE_UNITTEST_ASSERTS=1 -DHAVE_DIAGNOSTIC=0
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/unittest/catch2-unittests

  # End of normal make check test tasks

  # Start of cppsuite test tasks.
  # All cppsuite pull request tasks must supply the relative path to the config file as we are in
  # the cmake build working directory and the LD_LIBRARY_PATH is .libs.

  - name: cppsuite-background-compact-default
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config: debug_mode=(background_compact,cursor_copy=true)
          test_config_filename: configs/background_compact_default.txt
          test_name: background_compact

  - name: cppsuite-default-all
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test run all"
        vars:
          test_config: debug_mode=(cursor_copy=true)

  - name: cppsuite-background-long
    tags: ["cppsuite-stress-test"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/background_compact_long.txt
          test_name: background_compact

  - name: cppsuite-operations-test-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/operations_test_stress.txt
          test_name: operations_test

  - name: cppsuite-hs-cleanup-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/hs_cleanup_stress.txt
          test_name: hs_cleanup

  - name: cppsuite-burst-inserts-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/burst_inserts_stress.txt
          test_name: burst_inserts

  - name: cppsuite-bounded-cursor-stress-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_stress_stress.txt
          test_name: bounded_cursor_stress

  - name: cppsuite-bounded-cursor-stress-reverse-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_stress_reverse_stress.txt
          test_name: bounded_cursor_stress

  - name: cppsuite-bounded-cursor-prefix-stat-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_stat_stress.txt
          test_name: bounded_cursor_prefix_stat

  - name: cppsuite-bounded-cursor-prefix-search-near-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_search_near_stress.txt
          test_name: bounded_cursor_prefix_search_near

  - name: cppsuite-bounded-cursor-prefix-indices-stress
    depends_on:
      - name: compile
    tags: ["cppsuite-stress-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/bounded_cursor_prefix_indices_stress.txt
          test_name: bounded_cursor_prefix_indices

  - name: cppsuite-reverse-split-stress
    tags: ["cppsuite-stress-test"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite test"
        vars:
          test_config_filename: configs/reverse_split_stress.txt
          test_name: reverse_split

# Cppsuite perf tests
  - name: cppsuite-hs-cleanup-default-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/hs_cleanup_default.txt
          test_name: hs_cleanup

  - name: cppsuite-hs-cleanup-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/hs_cleanup_stress.txt
          test_name: hs_cleanup

  - name: cppsuite-operations-test-default-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/operations_test_default.txt
          test_name: operations_test

  - name: cppsuite-operations-test-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/operations_test_stress.txt
          test_name: operations_test
  # This is a perf test and as such doesn't run under the stress test tag. This name seems excessive
  # but in order to have a consistent naming system is required, the issue here is that the test
  # itself has the word "perf" in it.
  - name: cppsuite-bounded-cursor-perf-stress-perf
    depends_on:
      - name: compile
    tags: ["cppsuite-perf-test"]
    commands:
      - func: "fetch artifacts"
      - func: "cppsuite perf test"
        vars:
          test_config_filename: configs/bounded_cursor_perf_stress.txt
          test_name: bounded_cursor_perf

  # End of cppsuite test tasks.
  # Start of csuite test tasks
  - name: csuite-tests-fast
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          directory: test/csuite
          extra_args: -LE "long_running" ${extra_args} -j ${num_jobs}

  - name: csuite-timestamp-abort-test-s3
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/timestamp_abort"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose

            ./test_timestamp_abort -PT -Po s3_store

  - name: csuite-random-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/random_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-schema-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/schema_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-timestamp-abort-lazyfs
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          ENABLE_LAZYFS: -DENABLE_LAZYFS=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/csuite/timestamp_abort"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            ./smoke_lazyfs.sh

  - name: csuite-wt12015-backup-corruption-test
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "csuite test"
        vars:
          test_name: wt12015_backup_corruption

  - name: csuite-wt11126-compile-config-test
    tags: ["pull_request"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "csuite test"
        vars:
          test_name: wt11126_compile_config

  # End of csuite test tasks

  # Start of Python unit test tasks

  - name: unit-test
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"

  - name: unit-test-macos
    tags: ["python"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "python config check"
      - func: "unit test"

  - name: unit-test-zstd
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --zstd

  - name: unit-test-extra-long
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --extra-long

  # Run the tests that uses suite_random with a random starting seed
  - name: unit-test-random-seed
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -R cursor13 join02 join07 schema03 timestamp22

  - name: unit-test-hook-tiered
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered

  - name: unit-test-hook-tiered-with-delays
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook 'tiered=(tier_storage_source=dir_store,tier_storage_source_config=(force_delay=1,delay_ms=10,verbose=1))'

  - name: unit-test-hook-tiered-s3
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} ../test/suite/run.py ${unit_test_args|-v 2} --hook tiered=tier_storage_source='s3_store' -j ${num_jobs} 2>&1

  - name: unit-test-hook-tiered-timestamp
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered --hook timestamp

  - name: unit-test-hook-timestamp
    tags: ["python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook timestamp

  # A version of the tiered, timestamp hook test, scaled down for running during pull requests.
  # A small (1 out of N) random sample is run.
  - name: unit-test-hook-tiered-timestamp-quick
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --hook tiered --hook timestamp --random-sample 20

  # The test_prepare_hs03.py test, when run with timestamp hooks, is run multiple times to facilitate
  # catching intermittent problems in the test.
  - name: test-prepare-hs03-hook-timestamp
    tags: ["python"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            cd cmake_build
            i=0
            limit=100
            while ((i < limit))
            do
              ((i=i+1))
              echo "Test count: $i"
              ${python_binary|python3} ../test/suite/run.py -v 4 test_prepare_hs03.py --hook timestamp
            done

  - name: csuite-long-running
    # Set 5 hours timeout (60 * 60 * 5)
    exec_timeout_secs: 18000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "make check directory"
        vars:
          extra_args: -L long_running -j ${num_jobs}

  # Break out Python unit tests into multiple buckets/tasks.  We have a fixed number of buckets,
  # and we use the -b option of the test/suite/run.py script to split up the tests.

  - name: unit-test-bucket00
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 0/12

  - name: unit-test-bucket01
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 1/12

  - name: unit-test-bucket02
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 2/12

  - name: unit-test-bucket03
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 3/12

  - name: unit-test-bucket04
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 4/12

  - name: unit-test-bucket05
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 5/12

  - name: unit-test-bucket06
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 6/12

  - name: unit-test-bucket07
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 7/12

  - name: unit-test-bucket08
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 8/12

  - name: unit-test-bucket09
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 9/12

  - name: unit-test-bucket10
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 10/12

  - name: unit-test-bucket11
    tags: ["pull_request", "python", "unit_test"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 -b 11/12

  - name: unit-test-long-bucket00
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 0/12

  - name: unit-test-long-bucket01
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 1/12

  - name: unit-test-long-bucket02
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 2/12

  - name: unit-test-long-bucket03
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 3/12

  - name: unit-test-long-bucket04
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    run_on: ubuntu2004-medium
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 4/12

  - name: unit-test-long-bucket05
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 5/12

  - name: unit-test-long-bucket06
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 6/12

  - name: unit-test-long-bucket07
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 7/12

  - name: unit-test-long-bucket08
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 8/12

  - name: unit-test-long-bucket09
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 9/12

  - name: unit-test-long-bucket10
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 10/12

  - name: unit-test-long-bucket11
    tags: ["python", "unit_test_long"]
    patch_only: true
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "unit test"
        vars:
          unit_test_args: -v 2 --long -b 11/12

  # End of Python unit test tasks

  - name: s-all
    tags: ["pull_request"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            bash -x s_all -E 2>&1
            # Run s_string with the "-r" option to remove no-longer-needed words from s_string.ok
            # This is run separately from s_all as it can be too proactive when run as part of
            # developers local workflow, but needs to be run in PR builds before the code is merged.
            bash s_string -r 2>&1

  - name: s-outdated-fixmes
    # Detect any FIXME comments in the codebase tied to closed Jira tickets.
    # This will send a GET request to JIRA for each FIXME ticket so we don't
    # want to run it too frequently.
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${python_binary|python3} s_outdated_fixmes.py 2>&1

  - name: conf-dump-test
    tags: ["pull_request", "python"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} ../../../test/wtperf/test_conf_dump.py -d $(pwd) 2>&1

  - name: fops
    tags: ["pull_request"]
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/fops"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            if [ "Windows_NT" = "$OS" ]; then
              cmd.exe /c test_fops.exe
            else
              ./test_fops
            fi

  - name: bench-tiered-push-pull-s3
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/tiered"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./test_push_pull -PT -Po s3_store

  - name: bench-tiered-push-pull
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/tiered"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # By default the test_push_pull uses dir_store as a storage source.
            ./test_push_pull -PT

  - name: compatibility-test-for-newer-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -n

  - name: compatibility-test-for-older-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -o

  - name: compatibility-test-upgrade-to-latest
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -u

  - name: compatibility-test-for-patch-releases
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            test/compatibility/compatibility_test_for_releases.sh -p

  - name: compatibility-test-suite
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} test/compatibility/suite/compatibility_test.py ${compat_suite_args|-v 2} 2>&1

  - name: compatibility-test-for-wt-standalone-releases
    commands:
      - func: "get project"
      - func: "compatibility test"
        vars:
          compat_test_args: -w

  - name: import-compatibility-test
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            test/compatibility/compatibility_test_for_releases.sh -i

  - name: generate-datafile-little-endian
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          times: 10
          config: ../../../test/format/CONFIG.endian
          extra_args: -h "WT_TEST.$i"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/format"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            # Archive the WT_TEST directories which include the generated wt data files. We cannot
            # use the Evergreen archive command as we need to archive multiple WT_TEST folders.
            tar -zcvf WT_TEST.tgz WT_TEST*
      - func: "upload endian format artifacts"
        vars:
          endian_format: little-endian
          local_file: wiredtiger/cmake_build/test/format/WT_TEST.tgz
          remote_file: WT_TEST-little-endian.tgz

  - name: verify-datafile-little-endian
    depends_on:
    - name: compile
    - name: generate-datafile-little-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: little-endian
          remote_file: WT_TEST-little-endian
      - func: "verify wt datafiles"

  - name: verify-datafile-from-little-endian
    depends_on:
    - name: compile
    - name: generate-datafile-little-endian
      variant: little-endian
    - name: verify-datafile-little-endian
      variant: little-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: little-endian
          remote_file: WT_TEST-little-endian
      - func: "verify wt datafiles"

  - name: generate-datafile-big-endian
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          times: 10
          config: ../../../test/format/CONFIG.endian
          extra_args: -h "WT_TEST.$i"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/format"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            # Archive the WT_TEST directories which include the generated wt data files. We cannot
            # use the Evergreen archive command as we need to archive multiple WT_TEST folders.
            tar -zcvf WT_TEST.tgz WT_TEST*
      - func: "upload endian format artifacts"
        vars:
          endian_format: big-endian
          local_file: wiredtiger/cmake_build/test/format/WT_TEST.tgz
          remote_file: WT_TEST-big-endian.tgz

  - name: verify-datafile-big-endian
    depends_on:
    - name: compile
    - name: generate-datafile-big-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: big-endian
          remote_file: WT_TEST-big-endian
      - func: "verify wt datafiles"

  - name: verify-datafile-from-big-endian
    depends_on:
    - name: compile
    - name: generate-datafile-big-endian
      variant: big-endian
    - name: verify-datafile-big-endian
      variant: big-endian
    commands:
      - func: "fetch artifacts"
      - func: "fetch endian format artifacts"
        vars:
          endian_format: big-endian
          remote_file: WT_TEST-big-endian
      - func: "verify wt datafiles"

  - name: clang-analyzer
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}
            bash s_clang_scan 2>&1

  - name: configure-combinations
    commands:
      - func: "get project"
      - func: "generate github token"
      - func: "get automation-scripts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            . test/evergreen/find_cmake.sh
            cd test/evergreen
            CMAKE_BIN=$CMAKE ./configure_combinations.sh -g="${cmake_generator|Ninja}" -j=$(grep -c ^processor /proc/cpuinfo) 2>&1
      # Handle special build combination for running all the diagnostic tests.
      - func: "configure wiredtiger"
      - func: "make wiredtiger"
      - func: "make check all"

  # Use format.sh to run tests in parallel for just under two hours (the
  # default Evergreen timeout) on the higher spec build distros. This allows
  # us to perform multiple test runs while ensuring a long-running config does
  # not result in an Evergreen test timeout failure. This test is run on a
  # higher spec distro due to historical issues with timeout failures. Consider
  # reducing the parallelism if frequent timeouts occur.
  - name: linux-directio
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test script"
        vars:
          format_test_script_args: -t 110 direct_io=1

  - name: package
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/dist"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            env CC=/opt/mongodbtoolchain/v4/bin/gcc CXX=/opt/mongodbtoolchain/v4/bin/g++ PATH=/opt/mongodbtoolchain/v4/bin:/opt/java/jdk11/bin:$PATH bash s_release `date +%Y%m%d`

  # Note that the project settings use this task name to compile the documentation during PR
  # testing, keep this in mind if you decide to change it.
  - name: doc-compile
    commands:
      - func: "get project"
      - func: "compile wiredtiger docs"

  - name: doc-update
    patchable: false
    stepback: false
    commands:
      - func: "get project"
      - func: "compile wiredtiger docs"
      - func: "update wiredtiger docs"
      - func: "upload artifact"
        vars:
          artifacts_name: wiredtiger
      - func: "upload artifact"
        vars:
          upload_filename: wiredtiger.github.com.tgz
          upload_source_dir: wiredtiger.github.com
          artifacts_name: wiredtiger.github.com
          postfix: -wiredtiger.github.com

  - name: syscall-linux
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/syscall"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            WT_BUILDDIR=$(pwd)/../../cmake_build LD_LIBRARY_PATH=$WT_BUILDDIR ${python_binary|python3} syscall.py --verbose --preserve

  - name: checkpoint-filetypes-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          # Don't use diagnostic - this test looks for timing problems that are more likely to occur without it
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t m -n 1000000 -k 5000000 -C cache_size=100MB
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t c -n 1000000 -k 5000000 -C cache_size=100MB
      - func: "checkpoint test"
        vars:
          checkpoint_args: -t r -n 1000000 -k 5000000 -C cache_size=100MB

  - name: coverage-report
    # This task will measure code coverage across a range of tests, including, but not limited to,
    # the Catch2-based unit tests. The tests are run in parallel.
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/evergreen/find_cmake.sh

            # Create directories for 8 builds
            mkdir build_0 build_1 build_2 build_3 build_4 build_5 build_6 build_7

            # Record the start time, in seconds
            date +%s > time.txt

            ${python_binary|python3} test/evergreen/code_coverage/parallel_code_coverage.py -v -c test/evergreen/code_coverage/code_coverage_config.json -b $(pwd)/build_ -j ${num_jobs} -s

            # Record the end time, in seconds
            date +%s >> time.txt
      - func: "code coverage analysis"
        vars:
          generate_atlas_format: true
          working_dir: "wiredtiger"
      - func: "upload stats to atlas"
        vars:
          stats_dir: ./wiredtiger/coverage_report
          test-name: code_coverage
          collection: CodeCoverage
        # Publish the main page before the report so that it appears top of the list of files in the patch build.
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: coverage-report-catch2
    # This task measures code coverage achieved by only the Catch2-based unit tests.
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            test/evergreen/find_cmake.sh

            # Create directories for 1 builds, as only one test will be run at once
            mkdir build_0

            # Record the start time, in seconds
            date +%s > time.txt

            ${python_binary|python3} test/evergreen/code_coverage/parallel_code_coverage.py -v -c test/evergreen/code_coverage/code_coverage_config_catch2.json -b $(pwd)/build_ -j 1 -s

            # Record the end time, in seconds
            date +%s >> time.txt
      - func: "code coverage analysis"
        vars:
          working_dir: "wiredtiger"
        # Publish the main page before the report so that it appears top of the list of files in the patch build.
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: code-change-report
    tags: ["pull_request_code_statistics"]
    depends_on:
      - name: "coverage-report"
    commands:
      - func: "get project"
      - command: shell.exec
        vars:
          dependent_task: coverage-report
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            echo "Code Change Report (Step 1)"
            echo "==========================="
            echo "expansions:"
            echo ". is_patch      =  ${is_patch}"
            echo ". task_id       =  ${task_id}"
            echo ". task_name     =  ${task_name}"
            echo ". build_id      =  ${build_id}"
            echo ". version_id    =  ${version_id}"
            echo ". revision      =  ${revision}"
            echo ". github_commit =  ${github_commit}"
            echo ". project       =  ${project}"
            echo ". execution     =  ${execution}"
            echo ". remote path   =  wiredtiger/${build_variant}/${revision}/${dependent_task}_${build_id}-${execution}/full_coverage_report.json"
            mkdir -p coverage_report
      - command: s3.get
        vars:
          dependent_task: coverage-report
        params:
          aws_key: ${aws_key}
          aws_secret: ${aws_secret}
          remote_file: wiredtiger/${build_variant}/${revision}/${dependent_task}_${build_id}-${execution}/full_coverage_report.json
          bucket: build_external
          local_file: wiredtiger/coverage_report/full_coverage_report.json
      - command: github.generate_token
        params:
          expansion_name: github_token
          permissions:
            pull_requests: write
      - command: shell.exec
        vars:
          dependent_task: coverage-report
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            echo "Code Change Report (Step 2)"
            echo "==========================="

            set -o errexit
            set -o verbose

            ${PREPARE_PATH}
            virtualenv -p ${python_binary|python3} venv
            source venv/bin/activate
            pip3 install pygit2==1.10.1 requests

            EXTRA_CODE_CHANGE_PARAMETERS=''

            if [ ${is_patch|false} = true ]; then
              echo "This is a patch build"
              # Obtain the diff for the changes in this patch, excluding newly added 0-length files.
              python3 test/evergreen/code_change_report/git_diff_tool.py -g . -d coverage_report/diff.txt -v
              EXTRA_CODE_CHANGE_PARAMETERS='-d coverage_report/diff.txt'
              # Generate an HTML friendly version of the diff for
              sed 's/$/<br>/' coverage_report/diff.txt > coverage_report/diff.html
              # Logging for debugging
              ls -l coverage_report
              cat coverage_report/diff.txt
            fi

            pr_args=""
            if [ ! -z "${github_pr_number}" ]; then
              echo "Detected Github PR ${github_pr_number}"
              pr_args+="--github_repo ${github_org}/${github_repo} "
              pr_args+="--github_pr_number ${github_pr_number} "
              pr_args+="--github_token ${github_token} "
            fi

            ######################################################
            # Obtain the complexity metrics for the 'current' code
            ######################################################
            # Install Metrix++, ensuring it is outside the 'src' directory
            source test/evergreen/download_metrixpp.sh

            # We only want complexity measures for the 'src' directory
            cd src
            python3 "../metrixplusplus/metrix++.py" collect --std.code.lines.code --std.code.complexity.cyclomatic
            python3 "../metrixplusplus/metrix++.py" export --db-file=metrixpp.db > ../coverage_report/metrixpp.csv
            cd ..

            #######################################################
            # Obtain the complexity metrics for the 'previous' code
            #######################################################
            git worktree add --detach wiredtiger_previous "${github_commit}"
            cd wiredtiger_previous
            if [ ${is_patch|false} = true ]; then
              # Checkout the point at which this patch/branch diverged from develop
              git checkout `python3 dist/common_functions.py last_commit_from_dev`
            else
              # Checkout the previous commit
              git checkout HEAD~
            fi

            # Log the current git status of the 'previous' code
            git status

            cd src
            python3 "../../metrixplusplus/metrix++.py" collect --std.code.lines.code --std.code.complexity.cyclomatic
            python3 "../../metrixplusplus/metrix++.py" export --db-file=metrixpp.db > ../../coverage_report/metrixpp_prev.csv
            cd ../..

            #########################################
            # Generate the change info and the report
            #########################################

            ${python_binary|python3} test/evergreen/code_change_report/code_change_info.py -v -c coverage_report/full_coverage_report.json -g . $EXTRA_CODE_CHANGE_PARAMETERS -m coverage_report/metrixpp.csv -p coverage_report/metrixpp_prev.csv -o coverage_report/code_change_info.json

            # Log the contents of the change info file
            cat coverage_report/code_change_info.json

            # Generate the Code Change Report
            ${python_binary|python3} test/evergreen/code_change_report/code_change_report.py -v -c coverage_report/code_change_info.json -o coverage_report/code_change_report.html $pr_args

      - func: "code coverage publish report"

  - name: s3-tiered-storage-extensions-test
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered
  - name: s3-tiered-test-small
    tags: ["pull_request", "python"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered06
  - name: s3-tiered-catch2-unittest-test
    tags: ["pull_request", "tiered_unittest"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_tiered_storage_s3
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          include_expansions_in_env:
            - aws_sdk_s3_ext_access_key
            - aws_sdk_s3_ext_secret_key
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Run S3 extension unit testing.
            ext/storage_sources/s3_store/test/run_s3_unit_tests
  - name: azure-gcp-tiered-storage-extensions-test
    tags: ["python"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered
  - name: azure-gcp-tiered-catch2-unittest-test
    tags: ["pull_request", "tiered_unittest"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
          HAVE_UNITTEST: -DHAVE_UNITTEST=1
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            # Set the Azure credentials using config variable.
            export AZURE_STORAGE_CONNECTION_STRING="${azure_sdk_ext_access_key}"

            # GCP requires a path to a credentials file for authorization. To not expose the private
            # information within the file, we use a placeholder private variable which are replaced
            # in the command line with the evergreen expansion variables and stored in a temporary
            # file.
            file=$(mktemp --suffix ".json")

            # Use '|' as the delimiter instead of default behaviour because the private key contains
            # slash characters.
            sed -e 's|gcp_project_id|${gcp_sdk_ext_project_id}|'                      \
                -e 's|gcp_private_key|'"${gcp_sdk_ext_private_key}"'|'                \
                -e 's|gcp_private_id|${gcp_sdk_ext_private_key_id}|'                  \
                -e 's|gcp_client_email|${gcp_sdk_ext_client_email}|'                  \
                -e 's|gcp_client_id|${gcp_sdk_ext_client_id}|'                        \
                -e 's|gcp_client_x509_cert_url|${gcp_sdk_ext_client_x509_cert_url}|'  ../test/evergreen/gcp_auth.json > $file
            export GOOGLE_APPLICATION_CREDENTIALS="$file"
            set -o verbose
            ${PREPARE_TEST_ENV}

            # Run Azure extension unit testing.
            ext/storage_sources/azure_store/test/run_azure_unit_tests

            # Run GCP extension unit testing.
            ext/storage_sources/gcp_store/test/run_gcp_unit_tests

  - name: azure-gcp-tiered-test-small
    tags: ["pull_request", "python"]
    commands:
      - func: "get project"
      - func: "install gcp dependencies"
      - func: "compile wiredtiger"
        vars:
          <<: [*configure_flags_tiered_storage_azure, *configure_flags_tiered_storage_gcp]
      - func: "run tiered storage test"
        vars:
          # Set this in case of a core to get the correct python binary.
          python_binary: $(pwd)/venv/bin/python3
          tiered_storage_test_name: tiered06

  - name: spinlock-gcc-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          SPINLOCK_TYPE: -DSPINLOCK_TYPE=gcc
      - func: "make check all"
      - func: "unit test"
      - func: "format test"
        vars:
          times: 3
          extra_args: runs.rows=1000000:2500000

  - name: spinlock-pthread-adaptive-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          SPINLOCK_TYPE: -DSPINLOCK_TYPE=pthread_adaptive
      - func: "make check all"
      - func: "unit test"
      - func: "format test"
        vars:
          times: 3
          extra_args: runs.rows=1000000:2500000

  - name: wtperf-test
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
        vars:
          dependent_task: compile
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # The test will generate WT_TEST directory automatically
            dir=../bench/wtperf/stress
            for file in `ls $dir`
            do
              echo "Disk usage and free space for the current drive (pre-test):"
              df -h .
              echo "===="
              echo "==== Initiating wtperf test using $dir/$file ===="
              echo "===="
              ./bench/wtperf/wtperf -O $dir/$file -o verbose=2
              echo "===="
              echo "Disk usage and free space for the current drive (post-test):"
              df -h .
              echo "Total size of WT_TEST directory prior to move:"
              du -hs WT_TEST
              mv WT_TEST WT_TEST_$file
            done

  - name: long-test
    commands:
      - func: "get project"
      - func: "generate github token"
      - func: "get automation-scripts"
      - func: "configure wiredtiger"
      - func: "make wiredtiger"

      # Run the long version of make check, that includes the full csuite tests
      - func: "make check all"

      # Many dbs test - Run with:
      # 1.  The defaults
      - func: "many dbs test"
      # 2.  Set idle flag to turn off operations.
      - func: "many dbs test"
        vars:
          many_db_args: -I
      # 3.  More dbs.
      - func: "many dbs test"
        vars:
          many_db_args: -D 40
      # 4.  With idle flag and more dbs.
      - func: "many dbs test"
        vars:
          many_db_args: -I -D 40

      # extended test/thread runs
      - func: "thread test"
        vars:
          thread_test_args: -t f
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t f
      - func: "thread test"
        vars:
          thread_test_args: -t r
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t r
      - func: "thread test"
        vars:
          thread_test_args: -t v
      - func: "thread test"
        vars:
          thread_test_args: -S -F -n 100000 -t v

      # random-abort - default (random time and number of threads)
      - func: "csuite test"
        vars:
          test_name: random_abort

      # truncated-log
      - func: "csuite test"
        vars:
          test_name: truncated_log

      # random-abort - minimum time, random number of threads
      - func: "csuite test"
        vars:
          test_args: -t 10
          test_name: random_abort
      # random-abort - maximum time, random number of threads
      - func: "csuite test"
        vars:
          test_args: -t 40
          test_name: random_abort
      # random-abort - run compaction
      - func: "csuite test"
        vars:
          test_args: -c -t 60
          test_name: random_abort

      # format test
      - func: "format test"
        vars:
          extra_args: file_type=fix runs.rows=1000000:2500000
      - func: "format test"
        vars:
          extra_args: file_type=row runs.rows=1000000:2500000

      # format test for stressing compaction code path
      - func: "format test"
        vars:
          times: 3
          extra_args: file_type=row compaction=1 verify=1 runs.rows=1000000:2500000 runs.timer=3 ops.pct.delete=30

  - name: time-shift-sensitivity-test
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/csuite"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            RW_LOCK_FILE=$(pwd)/../../cmake_build/test/csuite/rwlock/test_rwlock ./time_shift_test.sh /usr/local/lib/faketime/libfaketimeMT.so.1 0-1 2>&1

  - name: format-mirror-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test"
        vars:
          config: ../../../test/format/CONFIG.mirror
          trace_args: -T all

  - name: format-stress-pull-request-test
    tags: ["pull_request"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "format test script"
        vars:
          # run for 10 minutes.
          format_test_script_args: -t 10 rows=10000 ops=50000

  - name: format-smoke-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -S
      - func: "format test"
        vars:
          extra_args: -C "verbose=(checkpoint_cleanup:1)"

  - name: format-asan-smoke-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -S
      - func: "format test"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          extra_args: -C "verbose=(checkpoint_cleanup:1)"

  - name: format-wtperf-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/bench/wtperf"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            cp ../../../bench/wtperf/runners/split_heavy.wtperf .
            ./wtperf -O ./split_heavy.wtperf -o verbose=2

  - name: memory-model-test
    exec_timeout_secs: 86400
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/tools/memory-model-test"
          script: |
            set -o errexit
            set -o verbose
            export "PATH=/opt/mongodbtoolchain/v4/bin:$PATH"
            g++ -o memory_model_test -O2 memory_model_test.cpp -lpthread -std=c++20 -Wall -Werror
            ./memory_model_test -n 100000000

  - name: memory-model-test-mac
    exec_timeout_secs: 86400
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/tools/memory-model-test"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            sysctl -n machdep.cpu.brand_string     # Display the CPU type
            sw_vers                                # Display the macOS version
            g++ -o memory_model_test -O2 memory_model_test.cpp -lpthread -std=c++17 -Wall -Werror -DAVOID_CPP20_SEMAPHORE
            ./memory_model_test -n 100000000

  - name: data-validation-stress-test-checkpoint
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x

  - name: data-validation-stress-test-checkpoint-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s1
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 1

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s1-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 1

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s2
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 2

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s3
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 3

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s3-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 3

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s4
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 4

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s5
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 5

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s5-no-timestamp
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -s 5

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s6
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 6

  - name: data-validation-stress-test-checkpoint-fp-hs-insert-s7
    tags: ["data-validation-stress-test"]
    depends_on:
    - name: compile
    commands:
      - func: "run data validation stress test checkpoint"
        vars:
          run_test_checkpoint_args: -x -s 7

  - name: format-failure-configs-test
    depends_on:
    - name: compile
    commands:
      - func: "fetch artifacts"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/evergreen"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./run_format_configs.sh -j ${num_jobs}

  - name: static-wt-build-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_static_lib_with_builtins
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build"
          shell: bash
          script: |
            set -o errexit
            set -o verbose

            # -V option displays Wiredtiger library version
            ./wt -V

            if [ $? -ne 0 ]; then
              echo "Error, WT util is not generated or is not functioning"
              exit 1
            fi

            # Test if libwiredtiger is dynamically linked.
            (ldd wt | grep "libwiredtiger.so") || wt_static_build=1

            if [ $wt_static_build -ne 1 ]; then
              echo "Error, WT util is not statically linked"
              exit 1
            fi

  - name: format-stress-sanitizer-lsm-test
    # FIXME-WT-6258: Re-enable the test once the outstanding issues with LSM are resolved.
    # tags: ["stress-test-sanitizer"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          # Run for 30 mins, and explicitly set data_source to LSM with a large cache
          format_test_script_args: -t 30 data_source=lsm cache_minimum=5000

  - name: checkpoint-stress-test
    tags: ["stress-test-1"]
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          no_of_procs: 10 # Number of processes to run in the background
          tiered: 0       # Don't enable tiered storage
          times: 1        # Number of times to run the loop

  - name: checkpoint-stress-test-tiered
    exec_timeout_secs: 86400
    commands:
      - command: timeout.update
        params:
          timeout_secs: 86400
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "checkpoint stress test"
        vars:
          no_of_procs: 5  # Number of processes to run in the background
          tiered: 1       # Enable tiered storage
          times: 1        # Number of times to run the loop

  - name: skiplist-stress-test
    tags: ["stress-test-1", "stress-test-zseries-1"]
    exec_timeout_secs: 3600 # 1 hour
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/bench/workgen/runner"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            for i in $(seq 5); do
                ${python_binary|python3} skiplist_stress.py
            done

  - name: chunkcache-test
    exec_timeout_secs: 1200 # 20 minutes
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/bench/workgen/runner"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ${python_binary|python3} chunkcache_simple.py

  - name: split-stress-test
    tags: ["stress-test-1", "stress-test-ppc-1", "stress-test-zseries-1"]
    # Set 5 hours timeout (60 * 60 * 5)
    exec_timeout_secs: 18000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "split stress test"

  - name: model-unit-test
    tags: ["pull_request", "model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/test"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./test_model.sh

  - name: model-test-failure-workloads
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/test/evergreen"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            ./run_model_workloads.sh

  - name: model-test-long
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/tools"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}
            # Keep repeating the model test for 60 minutes
            ./model_test -l 2000-3000 -t 3600

  - name: model-test-long-with-coverage
    tags: ["model_checking"]
    commands:
      - func: "get project"
      - command: expansions.update
        params:
          updates:
          - key: CMAKE_TOOLCHAIN_FILE
            value: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=Coverage
          CODE_COVERAGE_FLAGS: -DCODE_COVERAGE_MEASUREMENT=1 -DINLINE_FUNCTIONS_INSTEAD_OF_MACROS=1
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      - command: shell.exec
        params:
          working_dir: "wiredtiger/cmake_build/test/model/tools"
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_TEST_ENV}

            # Record the start time, in seconds (needed by the code coverage analysis step below)
            date +%s > ../../../../time.txt

            # Keep repeating the model test for 60 minutes
            ./model_test -l 2000-3000 -t 3600

            # Record the end time, in seconds
            date +%s >> ../../../../time.txt
      - func: "code coverage analysis"
        vars:
          coverage_filter: "src/rollback_to_stable"
          working_dir: "wiredtiger"
      - func: "code coverage publish main page"
      - func: "code coverage publish report"

  - name: format-stress-zseries-test
    tags: ["stress-test-zseries-1"]
    # Set 2.5 hours timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          # FIXME-WT-13690: Enable ZSTD compression once failure has been resolved.
          HAVE_BUILTIN_EXTENSION_ZSTD: -DHAVE_BUILTIN_EXTENSION_ZSTD=0
      - func: "format test script"
        vars:
          #run for 2 hours ( 2 * 60 = 120 minutes), use default config
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 120

  - name: format-stress-ppc-test
    tags: ["stress-test-ppc-1"]
    # Set 2.5 hours timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          #run for 2 hours ( 2 * 60 = 120 minutes), use default config
          # Always disable mmap for PPC due to issues on variant setup.
          # See https://bugzilla.redhat.com/show_bug.cgi?id=1686261#c10 for the potential cause.
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 120 -- -C "mmap=false,mmap_all=false"

  - <<: *format-stress-test
    name: format-stress-test-1
    tags: ["stress-test-1"]
  - <<: *format-stress-test
    name: format-stress-test-2
    tags: ["stress-test-2"]
  - <<: *format-stress-test
    name: format-stress-test-3
    tags: ["stress-test-3"]
  - <<: *format-stress-test
    name: format-stress-test-4
    tags: ["stress-test-4"]
  - <<: *format-stress-sanitizer-ppc-test
    name: format-stress-sanitizer-ppc-test-1
    tags: ["stress-test-ppc-1"]
  - <<: *format-stress-sanitizer-ppc-test
    name: format-stress-sanitizer-ppc-test-2
    tags: ["stress-test-ppc-2"]
  - <<: *format-stress-sanitizer-test
    name: format-stress-sanitizer-test-1
    tags: ["stress-test-sanitizer"]
  - <<: *format-stress-sanitizer-test
    name: format-stress-sanitizer-test-2
    tags: ["stress-test-sanitizer"]
  - <<: *format-stress-sanitizer-test
    name: format-stress-sanitizer-test-3
    tags: ["stress-test-sanitizer"]
  - <<: *format-stress-sanitizer-test
    name: format-stress-sanitizer-test-4
    tags: ["stress-test-sanitizer"]
  - <<: *race-condition-stress-sanitizer-test
    name: race-condition-stress-sanitizer-test-1
    tags: ["stress-test-sanitizer"]
  - <<: *race-condition-stress-sanitizer-test
    name: race-condition-stress-sanitizer-test-2
    tags: ["stress-test-sanitizer"]
  - <<: *race-condition-stress-sanitizer-test
    name: race-condition-stress-sanitizer-test-3
    tags: ["stress-test-sanitizer"]
  - <<: *race-condition-stress-sanitizer-test
    name: race-condition-stress-sanitizer-test-4
    tags: ["stress-test-sanitizer"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-1
    tags: ["stress-test-1", "stress-test-zseries-1"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-2
    tags: ["stress-test-2", "stress-test-zseries-2"]
  - <<: *recovery-stress-test
    name: recovery-stress-test-3
    tags: ["stress-test-3", "stress-test-zseries-3"]

  - name: format-stress-test-no-barrier
    tags: ["stress-test-no-barrier"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          format_test_script_args: -e "SEGFAULT_SIGNALS=all" -b "catchsegv ./t" -t 360

  - name: format-stress-sanitizer-test-no-barrier
    tags: ["stress-test-sanitizer"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -t 360

  - name: race-condition-stress-sanitizer-test-no-barrier
    tags: ["stress-test-sanitizer"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_address_sanitizer_mongodb_stable_clang_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
      - func: "format test script"
        vars:
          additional_san_vars: export ASAN_OPTIONS="$COMMON_SAN_OPTIONS:unmap_shadow_on_exit=1"
          format_test_script_args: -R -t 360

  - name: recovery-stress-test-no-barrier
    tags: ["stress-test-no-barrier"]
    exec_timeout_secs: 25200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
          NON_BARRIER_DIAGNOSTIC_YIELDS: -DNON_BARRIER_DIAGNOSTIC_YIELDS=1
          CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
      - func: "recovery stress test script"
        vars:
          times: 25

  - name: format-abort-recovery-stress-test
    commands:
      # Allow 30 minutes beyond test runtime because recovery under load can cause the test to
      # run longer.
      - command: timeout.update
        params:
          exec_timeout_secs: 3600
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test script"
        vars:
          format_test_script_args: -a -t 30

  - name: format-predictable-test
    # Set 2.5 hour timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test predictable"
        vars:
          times: 5

  - name: format-tiered-test
    # Set 2.5 hour timeout (60 * 60 * 2.5)
    exec_timeout_secs: 9000
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "format test tiered"
        vars:
          times: 10

  - name: schema-abort-predictable-test
    # Set 20 minute timeout (60 * 20)
    exec_timeout_secs: 1200
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          <<: *configure_flags_with_builtins
      - func: "schema abort predictable"
        vars:
          times: 5

  - name: checkpoint-filetypes-predictable-test
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          # Don't use diagnostic - this test looks for timing problems that are more likely to occur without it
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=0
      # FIXME-WT-10936: Enable once predictable replay supports column store
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -t m -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -t c -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      #- func: "checkpoint test predictable"
      #  vars:
      #    checkpoint_args: -n 1000000 -k 5000000 -C cache_size=100MB
      #    times: 5
      - func: "checkpoint test predictable"
        vars:
          checkpoint_args: -t r -n 1000000 -k 5000000 -C cache_size=100MB
          times: 5

  - name: many-collection-test
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 86400
          timeout_secs: 86400
      - func: "fetch mongo repo"
      - func: "get project"
      - func: "get engflow creds"
      - func: "import wiredtiger into mongo"
      - func: "compile mongodb"
      - func: "fetch mongo-tests repo"
      # FIXME-WT-7868: we should download a pre populated database here and remove the
      # "clean-and-populate" argument in the step below.
      - command: shell.exec
        params:
          working_dir: mongo-tests/largescale
          shell: bash
          script: |
            set -o errexit
            set -o verbose
            ${PREPARE_PATH}
            virtualenv -p python3 venv
            source venv/bin/activate
            # Need both pymongo and pymongo[srv] as upload-results-atlas.py uses mongo+srv for the URI.
            pip3 install lorem pymongo==3.12.2 "pymongo[srv]==3.12.2"
            mongod_path=$(find ../../mongo/build -executable -type f -path \*/bin/mongod)
            ./run_many_coll.sh $mongod_path mongodb.log config/many-collection-testing many-collection clean-and-populate
      - func: "convert-to-atlas-evergreen-format"
        vars:
          input_file: ./mongo-tests/largescale/many-collection-artifacts/results/results.json
          test_name: many-collection-test
          output_path: ./mongo-tests/largescale/many-collection-artifacts/results/
      - func: "upload stats to atlas"
        vars:
          stats_dir: mongo-tests/largescale/many-collection-artifacts/results
          test-name: many-collection-test
      - func: "upload stats to evergreen"
        vars:
          stats_dir: mongo-tests/largescale/many-collection-artifacts/results
          test-name: many-collection-test
      - func: "upload artifact"
        vars:
          upload_filename: many-collection-test.tgz
          upload_source_dir: mongo-tests/largescale/many-collection-artifacts
      # Call cleanup function to avoid duplicated artifact upload in the post-task stage.
      - func: "cleanup"

  - name: cyclomatic-complexity
    tags: ["pull_request_code_statistics"]
    commands:
      - func: "get project"
      - command: shell.exec
        params:
          working_dir: "wiredtiger"
          shell: bash
          script: |
            t=__wt.$$
            set -o verbose

            # Install Metrix++, ensuring it is outside the 'src' directory
            source test/evergreen/download_metrixpp.sh

            mkdir -p code_statistics_report

            # We only want complexity measures for the 'src' directory
            cd src

            python3 "../metrixplusplus/metrix++.py" collect --std.code.lines.code --std.code.complexity.cyclomatic
            python3 "../metrixplusplus/metrix++.py" view

            # Set the cyclomatic complexity limit to 20
            python3 "../metrixplusplus/metrix++.py" limit --max-limit=std.code.complexity:cyclomatic:20

            # Fail if there are functions with cyclomatic complexity larger than 98
            python "../metrixplusplus/metrix++.py" limit --max-limit=std.code.complexity:cyclomatic:98 > $t
            if grep -q 'exceeds' $t; then
                echo "[ERROR]:complexity:cyclomatic: Complexity limit exceeded."
                cat $t
                echo "[ERROR]:complexity:cyclomatic: Finished " && rm $t && exit 1
            else
                cat $t && rm $t
            fi

            python3 "../metrixplusplus/metrix++.py" view --format=python > ../code_statistics_report/code_complexity_summary.json
            python3 "../metrixplusplus/metrix++.py" export --db-file=metrixpp.db > ../code_statistics_report/metrixpp.csv

            # Generate the code complexity statistics that is compatible with Atlas.
            virtualenv -p python3 venv
            source venv/bin/activate
            pip3 install pandas

            python3 ../test/evergreen/code_complexity_analysis.py -s ../code_statistics_report/code_complexity_summary.json -d ../code_statistics_report/metrixpp.csv -o ../code_statistics_report/atlas_out_code_complexity.json
      - func: "upload stats to atlas"
        vars:
          stats_dir: ./wiredtiger/code_statistics_report
          test-name: code_complexity
          collection: CodeComplexity

    #############################
    # Performance Tests for lsm #
    #############################

  - name: perf-test-small-lsm
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: small-lsm.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: small-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: small-lsm.wtperf

  - name: perf-test-medium-lsm
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-lsm.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-lsm.wtperf

  - name: perf-test-medium-lsm-compact
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-lsm-compact.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-lsm-compact.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-lsm-compact.wtperf

  - name: perf-test-medium-multi-lsm
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-multi-lsm.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-multi-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-multi-lsm.wtperf

  - name: perf-test-parallel-pop-lsm
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-lsm.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-lsm.wtperf

  - name: perf-test-update-lsm
    tags: ["lsm-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-lsm.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update", "insert"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-lsm.wtperf

    ###############################
    # Performance Tests for btree #
    ###############################

  - name: perf-test-small-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: small-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: small-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: small-btree.wtperf

  - name: perf-test-small-btree-backup
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: small-btree-backup.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: small-btree-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: small-btree-backup.wtperf

  - name: perf-test-medium-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-btree.wtperf

  - name: perf-test-medium-btree-backup
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: medium-btree-backup.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: medium-btree-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: medium-btree-backup.wtperf

  - name: perf-test-parallel-pop-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-btree.wtperf

  - name: perf-test-parallel-pop-btree-long
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-btree-long.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-btree-long.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-btree-long.wtperf

  - name: perf-test-short-key-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: short-key-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: short-key-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: short-key-btree.wtperf

  - name: perf-test-update-only-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-only-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-only-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-only-btree.wtperf

  - name: perf-test-update-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-btree.wtperf
          maxruns: 1
          wtarg: "-bf ../../../bench/wtperf/runners/update-btree.json"
      - func: "upload stats to atlas"
        vars:
          test-name: update-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-btree.wtperf

  - name: perf-test-update-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-large-record-btree.wtperf

  - name: perf-test-modify-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: modify-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "modify"']
      - func: "upload stats to atlas"
        vars:
          test-name: modify-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: modify-large-record-btree.wtperf

  - name: perf-test-modify-force-update-large-record-btree
    tags: ["btree-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: modify-force-update-large-record-btree.wtperf
          maxruns: 3
          wtarg: -ops ['"load", "modify"']
      - func: "upload stats to atlas"
        vars:
          test-name: modify-force-update-large-record-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: modify-force-update-large-record-btree.wtperf

    ###############################
    # Performance Tests for oplog #
    ###############################

  - name: perf-test-mongodb-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-oplog.wtperf

  - name: perf-test-mongodb-small-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-small-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-small-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-small-oplog.wtperf

  - name: perf-test-mongodb-large-oplog
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-large-oplog.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "insert", "truncate", "database_size"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-large-oplog.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-large-oplog.wtperf

  - name: perf-test-mongodb-secondary-apply
    tags: ["oplog-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: mongodb-secondary-apply.wtperf
          maxruns: 1
          wtarg: -ops ['"insert"']
      - func: "upload stats to atlas"
        vars:
          test-name: mongodb-secondary-apply.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: mongodb-secondary-apply.wtperf

    #########################################
    # Performance Tests for perf-checkpoint #
    #########################################

  - name: perf-test-update-checkpoint-btree
    tags: ["checkpoint-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-checkpoint-btree.wtperf
          maxruns: 1
          wtarg: "-bf ../../../bench/wtperf/runners/update-checkpoint.json"
      - func: "upload stats to atlas"
        vars:
          test-name: update-checkpoint-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-checkpoint-btree.wtperf

  - name: perf-test-update-checkpoint-lsm
    # FIXME-WT-8867 Disable/Un-tag the LSM perf test till the support for LSM is restored.
    # tags: ["checkpoint-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-checkpoint-lsm.wtperf
          maxruns: 1
          wtarg: "-bf ../../../bench/wtperf/runners/update-checkpoint.json"
      - func: "upload stats to atlas"
        vars:
          test-name: update-checkpoint-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-checkpoint-lsm.wtperf

    ###############################
    # Performance Tests for stress #
    ###############################

  - name: perf-test-overflow-10k
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: overflow-10k.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: overflow-10k.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: overflow-10k.wtperf

  - name: perf-test-overflow-130k
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: overflow-130k.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read", "update"']
      - func: "upload stats to atlas"
        vars:
          test-name: overflow-130k.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: overflow-130k.wtperf

  - name: perf-test-parallel-pop-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: parallel-pop-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"load"']
      - func: "upload stats to atlas"
        vars:
          test-name: parallel-pop-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: parallel-pop-stress.wtperf

  - name: perf-test-update-grow-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-grow-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-grow-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-grow-stress.wtperf

  - name: perf-test-update-shrink-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-shrink-stress.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-shrink-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-shrink-stress.wtperf

  - name: perf-test-update-delta-mix1
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix1.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix1.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix1.wtperf

  - name: perf-test-update-delta-mix2
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix2.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix2.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix2.wtperf

  - name: perf-test-update-delta-mix3
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: update-delta-mix3.wtperf
          maxruns: 1
          wtarg: -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: update-delta-mix3.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: update-delta-mix3.wtperf

  - name: perf-test-multi-btree-zipfian
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: multi-btree-zipfian-populate.wtperf
          maxruns: 1
      - func: "run-perf-test"
        vars:
          perf-test-name: multi-btree-zipfian-workload.wtperf
          maxruns: 1
          no_create: true
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: multi-btree-zipfian-workload.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: multi-btree-zipfian-workload.wtperf

  - name: perf-test-many-table-stress
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: many-table-stress.wtperf
          maxruns: 1

  - name: perf-test-many-table-stress-backup
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: many-table-stress-backup.wtperf
          maxruns: 1

  - name: perf-test-evict-fairness
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-fairness.wtperf
          maxruns: 1
          wtarg: -args ['"-C statistics_log=(wait=10000,on_close=true,json=false,sources=[file:])", "-o reopen_connection=false"'] -ops ['"eviction_page_seen"']
      - func: "validate-expected-stats"
        vars:
          stat_file: './test_stats/evergreen_out_evict-fairness.wtperf.json'
          comparison_op: "eq"
          expected-stats: '{"Pages seen by eviction": 200}'

  - name: perf-test-evict-btree-stress-multi
    tags: ["stress-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree-stress-multi.wtperf
          maxruns: 1
          wtarg: -ops ['"warnings", "top5_latencies_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree-stress-multi.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree-stress-multi.wtperf

    ##################################
    # Performance Tests for eviction #
    ##################################

  - name: perf-test-evict-btree
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree.wtperf

  - name: perf-test-evict-btree-1
    tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-btree-1.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-btree-1.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-btree-1.wtperf

  - name: perf-test-evict-lsm
    # FIXME-WT-8867 Disable/Un-tag the LSM perf test till the support for LSM is restored.
    # tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-lsm.wtperf
          maxruns: 1
          wtarg: -ops ['"load", "read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-lsm.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-lsm.wtperf

  - name: perf-test-evict-lsm-1
    # FIXME-WT-8867 Disable/Un-tag the LSM perf test till the support for LSM is restored.
    # tags: ["evict-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: evict-lsm-1.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: evict-lsm-1.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: evict-lsm-1.wtperf

    ###########################################
    # Performance Tests for log consolidation #
    ###########################################

  - name: perf-test-log
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -ops ['"update", "min_max_update_throughput"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-small-files
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-no-checkpoints
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C checkpoint=(wait=0)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-no-prealloc
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M,prealloc=false)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-zero-fill
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M,zero_fill=true)"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

  - name: perf-test-log-many-threads
    tags: ["log-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: log.wtperf
          maxruns: 1
          wtarg: -args ['"-C log=(enabled,file_max=1M),session_max=256", "-o threads=((count=128,updates=1))"'] -ops ['"update"']
      - func: "upload stats to atlas"
        vars:
          test-name: log.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: log.wtperf

    #####################################
    # Performance Tests for chunk cache #
    #####################################

  - name: perf-test-chunk-cache
    tags: ["chunk-cache-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: chunk-cache-reads.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: chunk-cache-reads.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: chunk-cache-reads.wtperf

  - name: perf-test-chunk-cache-base
    tags: ["chunk-cache-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: chunk-cache-reads-base.wtperf
          maxruns: 1
          wtarg: -ops ['"read"']
      - func: "upload stats to atlas"
        vars:
          test-name: chunk-cache-reads-base.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: chunk-cache-reads-base.wtperf

    ###########################################
    #        Performance Long Tests           #
    ###########################################

  - name: perf-test-long-500m-btree-populate
    tags: ["long-perf"]
    depends_on:
      - name: compile
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 7200
          timeout_secs: 7200
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-populate.wtperf
          maxruns: 1
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"load", "warnings", "max_latency_insert"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-populate.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-populate.wtperf
      - func: "upload artifact"
        vars:
          upload_filename: WT_TEST.tgz
          upload_source_dir: wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0/
      # Call cleanup function to avoid duplicated artifact upload in the post-task stage.
      - func: "cleanup"

  - name: perf-test-long-500m-btree-50r50u
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-50r50u-backup
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-50r50u-backup.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-50r50u-backup.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-50r50u-backup.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-80r20u
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-80r20u.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "update", "warnings", "max_latency_read_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-80r20u.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-80r20u.wtperf
      - func: "cleanup"

  - name: perf-test-long-500m-btree-rdonly
    tags: ["long-perf"]
    depends_on:
      - name: perf-test-long-500m-btree-populate
    commands:
      - command: timeout.update
        params:
          exec_timeout_secs: 10800
          timeout_secs: 10800
      # Fetch the compile artifacts.
      - func: "fetch artifacts"
      # Fetch the database created by perf-test-long-500m-btree-populate task.
      - func: "fetch artifacts"
        vars:
          dependent_task: perf-test-long-500m-btree-populate
          destination: "wiredtiger/cmake_build/bench/wtperf/WT_TEST_0_0"
      - func: "run-perf-test"
        vars:
          perf-test-name: 500m-btree-rdonly.wtperf
          maxruns: 1
          no_create: true
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"read", "warnings", "max_latency_read_update", "min_max_read_throughput"']
      - func: "upload stats to atlas"
        vars:
          test-name: 500m-btree-rdonly.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: 500m-btree-rdonly.wtperf
      - func: "cleanup"

  - name: perf-test-long-checkpoint-stress
    tags: ["long-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          perf-test-name: checkpoint-stress.wtperf
          maxruns: 1
          wtarg: -args ['"-C create,statistics=(fast),statistics_log=(json,wait=1,sources=[file:])"'] -ops ['"update", "checkpoint"']
      - func: "upload stats to atlas"
        vars:
          test-name: checkpoint-stress.wtperf
      - func: "upload stats to evergreen"
        vars:
          test-name: checkpoint-stress.wtperf

  - name: many-dhandle-stress
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "run-perf-test"
        vars:
          test_type: workgen
          exec_path: ${python_binary|python3}
          perf-test-path: ../../../bench/workgen/runner
          perf-test-name: many-dhandle-stress.py
          maxruns: 1
          wtarg: -ops ['"max_latency_create", "max_latency_drop", "max_latency_drop_diff", "max_latency_insert_micro_sec", "max_latency_read_micro_sec", "max_latency_update_micro_sec", "warning_idle", "warning_idle_create", "warning_idle_drop", "warning_insert", "warning_operations", "warning_read", "warning_update"']
      - func: "upload stats to atlas"
        vars:
          test-name: many-dhandle-stress.py
      - func: "upload stats to evergreen"
        vars:
          test-name: many-dhandle-stress.py
      - func: "validate-expected-stats"
        vars:
          stat_file: './test_stats/evergreen_out_many-dhandle-stress.py.json'
          comparison_op: "lt"
          expected-stats: '{"Warning Idle (drop)": 50, "Latency drop(in sec.) Max1": 500, "Latency warnings (read, insert, update)": 500}'

  - name: bench-wt2853-perf-test-row
    tags: ["wt2853-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "wt2853_perf test"
        vars:
          wt2853_perf_args: "-t r"
      - func: "upload test stats"
        vars:
          test_path: bench/wt2853_perf/wt2853_perf

  - name: bench-wt2853-perf-test-col
    tags: ["wt2853-perf"]
    depends_on:
      - name: compile
    commands:
      - func: "fetch artifacts"
      - func: "wt2853_perf test"
        vars:
          wt2853_perf_args: "-t c"
      - func: "upload test stats"
        vars:
          test_path: bench/wt2853_perf/wt2853_perf

  - <<: *workgen-test
    name: "workgen-test-compress_ratio"

  - <<: *workgen-test
    name: "workgen-test-evict-btree-hs"

  - <<: *workgen-test
    name: "workgen-test-example_dynamic_tables"

  - <<: *workgen-test
    name: "workgen-test-example_prepare"

  - <<: *workgen-test
    name: "workgen-test-example_prepare_evict_reconcile"

  - <<: *workgen-test
    name: "workgen-test-example_simple"

  - <<: *workgen-test
    name: "workgen-test-example_txn"

  - <<: *workgen-test
    name: "workgen-test-insert_stress"

  - <<: *workgen-test
    name: "workgen-test-insert_test"

  - <<: *workgen-test
    name: "workgen-test-maintain_low_dirty_cache"

  - <<: *workgen-test
    name: "workgen-test-many-dhandle-stress"

  - <<: *workgen-test
    name: "workgen-test-multi_btree_heavy_stress"

  - <<: *workgen-test
    name: "workgen-test-multiversion"

  - <<: *workgen-test
    name: "workgen-test-prepare_stress"

  - <<: *workgen-test
    name: "workgen-test-read_write_storms"

  - <<: *workgen-test
    name: "workgen-test-read_write_sync_long"

  - <<: *workgen-test
    name: "workgen-test-read_write_sync_short"

  - <<: *workgen-test
    name: "workgen-test-skiplist_stress"

  - <<: *workgen-test
    name: "workgen-test-small_btree"

  - <<: *workgen-test
    name: "workgen-test-small_btree_reopen"

#######################################
#     Antithesis Integration          #
#######################################

  - name: debug-have-diagnostic
    tags: ["antithesis"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
      - func: "build and push antithesis container"

  - name: release-with-debug-have-diagnostic
    tags: ["antithesis"]
    commands:
      - func: "get project"
      - func: "compile wiredtiger"
        vars:
          CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
          HAVE_DIAGNOSTIC: -DHAVE_DIAGNOSTIC=1
      - func: "build and push antithesis container"

#######################################
#            Buildvariants            #
#######################################

buildvariants:

- name: ubuntu2004
  display_name: "! Ubuntu 20.04"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 1
    data_validation_stress_test_args: -t r -m -W 3 -D -p -n 100000 -k 100000 -C cache_size=100MB
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: ".pull_request !.pull_request_compilers"
    - name: ".unit_test_long"
      distros: ubuntu2004-large
    - name: compile
    - name: doc-compile
    - name: make-check-test
    - name: configure-combinations
    - name: syscall-linux
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-random-seed
    - name: unit-test-hook-tiered
    - name: unit-test-hook-tiered-with-delays
    - name: unit-test-hook-tiered-timestamp
    - name: unit-test-hook-timestamp
    - name: test-prepare-hs03-hook-timestamp
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-large
    - name: long-test
      distros: ubuntu2004-large
    - name: unit-test-extra-long
      distros: ubuntu2004-large
    - name: static-wt-build-test
    - name: linux-directio
      distros: ubuntu2004-build
    - name: format-mirror-test
    - name: format-smoke-test
    - name: format-failure-configs-test
      distros: ubuntu2004-large
    - name: ".data-validation-stress-test"
    - name: catch2-unittest-test
    - name: s3-tiered-storage-extensions-test
    # FIXME-WT-12812 - This test is unstable when run with the new tmalloc install script. Disabled while WT-12812 is under investigation.
    # - name: azure-gcp-tiered-storage-extensions-test
    - name: bench-tiered-push-pull
    - name: catch2-unittest-assertions
    - name: ".tiered_unittest"
    - name: bench-tiered-push-pull-s3
    - name: csuite-timestamp-abort-test-s3
    - name: unit-test-hook-tiered-s3
    - name: chunkcache-test
    - name: ".workgen-test"
      batchtime: 1440 # 24 hours
    - name: model-unit-test
    - name: model-test-failure-workloads
      batchtime: 1440 # 24 hours
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-with-coverage
      batchtime: 1440 # once a day
    - name: csuite-long-running
      batchtime: 1440 # once a day

- name: ubuntu2004-asan
  display_name: "! Ubuntu 20.04 ASAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=ASan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export LSAN_OPTIONS="$COMMON_SAN_OPTIONS:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/asan_leaks.supp"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    # Exclude this test for ASan builds, as it results in a false positives owing
    # to deliberate killing of processes to invoke corruption.
    extra_args: -E "wt12015_backup_corruption"
  tasks:
    - name: ".pull_request !.pull_request_compilers !.model_checking !.python !.tiered_unittest !csuite-wt12015-backup-corruption-test"
    - name: examples-c-test
    - name: format-asan-smoke-test

- name: ubuntu2004-tsan
  display_name: "! Ubuntu 20.04 TSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=TSan
    ENABLE_TCMALLOC: 0
    additional_env_vars: |
      export TSAN_OPTIONS="$COMMON_SAN_OPTIONS:history_size=7:print_suppressions=0:suppressions=$(git rev-parse --show-toplevel)/test/evergreen/tsan_warnings.supp"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: examples-c-tsan

# Very minimal set without any extensions
- name: ubuntu2004-minimal
  display_name: "! Ubuntu 20.04 Minimal"
  batchtime: 480 # 3 times a day
  run_on:
  - ubuntu2004-test
  expansions:
    # Compile with clang so we test that path for the catch2 unittests.
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_PYTHON=0 -DENABLE_LZ4=0 -DENABLE_SNAPPY=0 -DENABLE_ZLIB=0 -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: csuite-tests-fast
    - name: fops
    - name: catch2-unittest-test
    - name: examples-c-test

- name: ubuntu2004-msan
  display_name: "! Ubuntu 20.04 MSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=MSan
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    # We don't run C++ memory sanitized testing as it creates false positives. MSAN also causes
    # some csuite tests to take an extended amount of time. These are excluded from the
    # make-check-test task and are covered by the csuite-long-running task.
    extra_args: -LE "cppsuite|long_running"
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` / 2" | bc)
  tasks:
    - name: clang-analyzer
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-stress-pull-request-test
    - name: make-check-test
      distros: ubuntu2004-large
    - name: csuite-long-running
      # Some of the long-running tests require a large instance to run successfully.
      distros: ubuntu2004-large
      batchtime: 1440 # once a day

- name: ubuntu2004-ubsan
  display_name: "! Ubuntu 20.04 UBSAN"
  run_on:
  - ubuntu2004-test
  expansions:
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/clang.cmake
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=UBSan
    # Use optimisation level -O0 since Clang treats -Og (our default optimisation for non-release
    # builds) as -O1, making test binaries more difficult to debug.
    CC_OPTIMIZE_LEVEL: -DCC_OPTIMIZE_LEVEL=-O0
    ENABLE_TCMALLOC: 0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: clang-analyzer
    - name: compile
    - name: compile-production-disable-shared
    - name: compile-production-disable-static
    - name: examples-c-production-disable-shared-test
    - name: examples-c-production-disable-static-test
    - name: format-stress-pull-request-test
    - name: make-check-test
      distros: ubuntu2004-large
    - name: cppsuite-default-all

- name: ubuntu2004-compilers
  display_name: "! Ubuntu 20.04 Compilers"
  run_on:
  - ubuntu2004-wt-build
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".pull_request_compilers"

- name: ubuntu2004-stress-tests
  display_name: Ubuntu 20.04 Stress tests
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-no-barrier"
      distros: ubuntu2004-medium
    - name: ".stress-test-sanitizer"
      run_on:
      - ubuntu2004-medium
    - name: format-abort-recovery-stress-test
    - name: format-predictable-test
    # FIXME-WT-10822
    # - name: format-tiered-test
    - name: schema-abort-predictable-test
    - name: checkpoint-filetypes-predictable-test

- name: ubuntu2004-stress-tests-arm64
  display_name: Ubuntu 20.04 Stress tests (ARM64)
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-no-barrier"
    - name: checkpoint-stress-test-tiered
    - name: format-abort-recovery-stress-test

- name: cppsuite-stress-tests-ubuntu
  display_name: "Cppsuite Stress Tests Ubuntu 20.04"
  batchtime: 720 # twice a day
  run_on:
  # We run on medium as small has too small a disk.
  - ubuntu2004-medium
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".cppsuite-stress-test"

- name: cppsuite-stress-tests-arm64
  display_name: "Cppsuite Stress Tests ARM64"
  batchtime: 720 # twice a day
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: ".cppsuite-stress-test"

- name: package
  display_name: "~ Package"
  batchtime: 1440 # 1 day
  run_on:
  - ubuntu2004-test
  tasks:
    - name: package

- name: rhel80
  display_name: RHEL 8.0
  run_on:
  - rhel80-test
  expansions:
    ENABLE_TCMALLOC: 0
    cmake_generator: "Unix Makefiles"
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/gcc.cmake
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: fops
    - name: time-shift-sensitivity-test
    - name: linux-directio
      distros: rhel80-build
    - name: syscall-linux
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-extra-long
      distros: rhel80-large
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
    - name: wtperf-test
    - name: long-test
    - name: configure-combinations

- name: windows
  display_name: "! Windows"
  run_on:
  - windows-2022-small
  expansions:
    ENABLE_TCMALLOC: 0
    # Remove the default configurations for the toolchain, install prefix and number of jobs.
    CMAKE_TOOLCHAIN_FILE:
    CMAKE_INSTALL_PREFIX:
    num_jobs:
    python_binary: "/cygdrive/c/python/Python311/python"
  tasks:
    - name: compile
    - name: make-check-test
    - name: ".unit_test"
    - name: fops
    - name: catch2-unittest-test

- <<: *mac_test_template
  name: macos-14-arm64
  display_name: "macOS 14 (ARM64)"
  run_on:
    - macos-14-arm64
  batchtime: 120 # 2 hours
  expansions:
    python_binary: '/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11'
    python_config_search_string: '_Python3_EXECUTABLE'
    <<: *mac_test_template_expansions

- name: little-endian
  display_name: "~ Little-endian (x86)"
  run_on:
  - ubuntu2204-large
  batchtime: 4320 # 3 days
  expansions:
    ENABLE_TCMALLOC: 0
    # Must disable ZSTD as its not available on the big endian platform (we generate the data files used for those tests here).
    posix_configure_flags: -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: generate-datafile-little-endian
    - name: verify-datafile-little-endian
    - name: verify-datafile-from-big-endian

- name: big-endian
  display_name: "~ Big-endian (s390x/zSeries)"
  run_on:
  - rhel8-zseries-small
  batchtime: 4320 # 3 days
  expansions:
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_ZSTD=0
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: compile
    - name: generate-datafile-big-endian
    - name: verify-datafile-big-endian
    - name: verify-datafile-from-little-endian

- name: rhel8-ppc
  display_name: "~ RHEL8 PPC"
  run_on:
  - rhel81-power8-small
  batchtime: 120 # 2 hours
  expansions:
    format_test_setting: ulimit -c unlimited
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    # Use quarter of the vCPUs to avoid OOM kill failure and disk issues on this variant.
    num_jobs: $(echo $(grep -c ^processor /proc/cpuinfo) / 4 | bc)
    ENABLE_CPPSUITE: -DENABLE_CPPSUITE=0
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_STRICT=0
  tasks:
    - name: compile
    - name: unit-test
    - name: format-smoke-test
    - name: format-asan-smoke-test
    - name: format-wtperf-test
    - name: ".stress-test-ppc-1"
    - name: ".stress-test-ppc-2"

- name: rhel8-zseries
  display_name: "~ RHEL8 zSeries"
  run_on:
  - rhel8-zseries-small
  batchtime: 120 # 2 hours
  expansions:
    ENABLE_TCMALLOC: 0
    # Use half number of vCPU to avoid OOM kill failure
    num_jobs: $(echo $(grep -c ^processor /proc/cpuinfo) / 2 | bc)
  tasks:
    - name: compile
    - name: unit-test
    - name: format-smoke-test
    - name: ".stress-test-zseries-1"
    - name: ".stress-test-zseries-2"
    - name: ".stress-test-zseries-3"

- name: ubuntu2004-arm64
  display_name: "~ Ubuntu 20.04 ARM64"
  run_on:
  - ubuntu2004-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: fops
    - name: format-failure-configs-test
      distros: ubuntu2004-arm64-large
    - name: linux-directio
      distros: ubuntu2004-arm64-large
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-extra-long
      distros: ubuntu2004-arm64-large
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-arm64-large
    - name: wtperf-test
    - name: long-test
      distros: ubuntu2004-arm64-large
    - name: configure-combinations
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test
    - name: chunkcache-test
    - name: memory-model-test
      batchtime: 40320 # 28 days
    - name: ".workgen-test"
      batchtime: 1440 # 24 hours
    - name: model-unit-test
    - name: model-test-failure-workloads
      batchtime: 1440 # 24 hours
    - name: model-test-long
      batchtime: 1440 # once a day
    - name: model-test-long-with-coverage
      batchtime: 1440 # once a day

- name: amazon2-arm64
  display_name: "Amazon Linux 2 ARM64"
  run_on:
  - amazon2-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    cmake_generator: "Unix Makefiles"
  # Using large distro for a few tests here as more memory is required.
  tasks:
    - name: compile
    - name: make-check-test
      distros: amazon2-arm64-large
    - name: unit-test
    - name: fops
    - name: format-failure-configs-test
      distros: amazon2-arm64-large
    - name: linux-directio
      distros: amazon2-arm64-large
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: unit-test-extra-long
      distros: amazon2-arm64-large
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: amazon2-arm64-large
    - name: wtperf-test
    - name: long-test
      distros: amazon2-arm64-large
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

# Antithesis build and push
- name: ubuntu2004-antithesis
  display_name: "~ Ubuntu 20.04 Antithesis"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 0
    posix_configure_flags: -DENABLE_ANTITHESIS=1 -DENABLE_STRICT=0
    CMAKE_TOOLCHAIN_FILE: -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/mongodbtoolchain_stable_clang.cmake
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".antithesis"
      cron: 0 0 * * 4 # once a week (Thursday midnight UTC)
      patchable: false

- name: ubuntu2004-release
  display_name: "~ Ubuntu 20.04 (Release Build)"
  run_on:
  - ubuntu2004-small
  batchtime: 720 # 12 hours
  expansions:
    ENABLE_TCMALLOC: 0
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: format-smoke-test
    - name: fops
    - name: linux-directio
      distros: ubuntu2004-large
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-large
    - name: long-test
      distros: ubuntu2004-large
    - name: configure-combinations
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

- name: ubuntu2004-release-arm64
  display_name: "~ Ubuntu 20.04 (ARM64, Release Build)"
  run_on:
  - ubuntu2004-arm64-small
  batchtime: 720 # 12 hours
  expansions:
    ENABLE_TCMALLOC: 0
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
  tasks:
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: format-smoke-test
    - name: fops
    - name: linux-directio
      distros: ubuntu2004-arm64-large
    - name: checkpoint-filetypes-test
    - name: unit-test-zstd
    - name: spinlock-gcc-test
    - name: spinlock-pthread-adaptive-test
      distros: ubuntu2004-arm64-large
    - name: long-test
      distros: ubuntu2004-arm64-large
    - name: configure-combinations
    - name: format-smoke-test
    - name: s3-tiered-storage-extensions-test

- name: ubuntu2004-release-stress-tests
  display_name: Ubuntu 20.04 Stress tests (Release Build)
  run_on:
  - ubuntu2004-test
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: format-abort-recovery-stress-test
    - name: format-predictable-test

- name: ubuntu2004-release-stress-tests-arm64
  display_name: Ubuntu 20.04 Stress tests (ARM64, Release Build)
  run_on:
  - ubuntu2004-arm64-large
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
  tasks:
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: format-abort-recovery-stress-test

- name: ubuntu2004-nonstandalone
  display_name: "Ubuntu 20.04 (Non-standalone)"
  run_on:
  - ubuntu2004-test
  expansions:
    ENABLE_TCMALLOC: 1
    data_validation_stress_test_args: -t r -m -W 3 -D -p -n 100000 -k 100000 -C cache_size=100MB
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: ".pull_request !.pull_request_compilers"
    - name: compile
    - name: make-check-test
    - name: unit-test-extra-long
      distros: ubuntu2004-large
    - name: ".data-validation-stress-test"

- name: ubuntu2004-stress-nonstandalone
  display_name: "Ubuntu 20.04 Stress tests (Non-standalone)"
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
  tasks:
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-no-barrier"
      distros: ubuntu2004-medium
    - name: format-abort-recovery-stress-test
    - name: compile
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours
      distros: ubuntu2004-medium # Medium is required as a larger disk is required.

- name: ubuntu2004-release-nonstandalone
  display_name: "Ubuntu 20.04 (Non-standalone, Release Build)"
  run_on:
  - ubuntu2004-small
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: ".stress-test-1"
      distros: ubuntu2004-test
    - name: ".stress-test-2"
      distros: ubuntu2004-test
    - name: format-abort-recovery-stress-test
      distros: ubuntu2004-test
    - name: compile
    - name: make-check-test
    - name: unit-test

- name: ubuntu2004-arm64-nonstandalone
  display_name: Ubuntu 20.04 (ARM64, Non-standalone)
  run_on:
  - ubuntu2004-arm64-large
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
      distros: ubuntu2004-arm64-small
    - name: make-check-test
      distros: ubuntu2004-arm64-small
    - name: unit-test
      distros: ubuntu2004-arm64-small
    - name: unit-test-extra-long
    - name: ".stress-test-1"
    - name: ".stress-test-2"
    - name: ".stress-test-3"
    - name: ".stress-test-4"
    - name: ".stress-test-no-barrier"
    - name: format-abort-recovery-stress-test
    - name: ".cppsuite-stress-test"
      batchtime: 720 # 12 hours

- name: ubuntu2004-arm64-release-nonstandalone
  display_name: Ubuntu 20.04 (ARM64, Release Build, Non-standalone)
  run_on:
  - ubuntu2004-arm64-small
  expansions:
    ENABLE_TCMALLOC: 1
    CMAKE_BUILD_TYPE: -DCMAKE_BUILD_TYPE=RelWithDebInfo
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo`" | bc)
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: ".stress-test-1"
      distros: ubuntu2004-arm64-large
    - name: ".stress-test-2"
      distros: ubuntu2004-arm64-large
    - name: format-abort-recovery-stress-test
      distros: ubuntu2004-arm64-large
    - name: compile
    - name: make-check-test
    - name: unit-test
    - name: unit-test-extra-long
      distros: ubuntu2004-arm64-large

- name: amazon2-arm64-nonstandalone
  display_name: "Amazon Linux 2 ARM64 Non-standalone"
  run_on:
  - amazon2-arm64-small
  batchtime: 1440 # 24 hours
  expansions:
    ENABLE_TCMALLOC: 1
    num_jobs: $(echo "`grep -c ^processor /proc/cpuinfo` * 2" | bc)
    cmake_generator: "Unix Makefiles"
    NONSTANDALONE: -DWT_STANDALONE_BUILD=0
    unit_test_variant_args: "--hook nonstandalone"
  tasks:
    - name: compile
    - name: make-check-test
      # Using large distro as more memory is required.
      distros: amazon2-arm64-large
    - name: unit-test
    - name: unit-test-extra-long
      distros: amazon2-arm64-large
